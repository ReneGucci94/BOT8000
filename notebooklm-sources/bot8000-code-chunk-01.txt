# BOT8000 Trading Bot - Source Code Chunk 1
# Generated for Notebook LM ingestion
# Total files in this chunk: 80
================================================================================


================================================================================
# FILE: .agent/auditor.py
================================================================================

import sys
from src.database import get_db_session
from src.database.models import Strategy, Pattern, Trade
from sqlalchemy import func

def verify_results():
    print("--- üîç RESULTS VERIFICATION AUDIT ---")
    
    with get_db_session() as db:
        # 1. Audit Approved Strategies
        strategies = db.query(Strategy).filter(Strategy.status == 'APPROVED').all()
        print(f"\n[1] APPROVED STRATEGIES: {len(strategies)}")
        if not strategies:
            print("‚ùå No approved strategies found (despite dashboard saying 8?)")
        
        for s in strategies:
            print(f"   ‚ñ∫ {s.name}")
            print(f"     PF: {s.profit_factor} | WR: {s.win_rate}%")
            print(f"     Params: {s.parameters}")
            print(f"     Filters: {s.filters}")
            
            # Sanity Check: Is PF realistic?
            if s.profit_factor is not None and float(s.profit_factor) > 10.0:
                print("     ‚ö†Ô∏è WARNING: Suspiciously high Profit Factor (Look-ahead bias?)")
            
        # 2. Audit Patterns
        patterns = db.query(Pattern).filter(Pattern.is_active == True).all()
        print(f"\n[2] DETECTED FAILURE PATTERNS: {len(patterns)}")
        for p in patterns:
            print(f"   ‚ñ∫ Type: {p.pattern_type}")
            print(f"     WR in context: {p.win_rate}% (Samples: {p.sample_size})")
            print(f"     Conditions: {p.conditions}")

        # 3. Audit Trade Integrity (Sample)
        trades_count = db.query(func.count(Trade.id)).scalar()
        print(f"\n[3] TOTAL TRADES RECORDED: {trades_count}")
        
        # Check for Look-ahead bias in a sample trade
        # Entry time should be >= Candle timestamp (which usually denotes Open time or Close time? need to verify)
        # Binance klines uses Open Time. So Close Time is Timestamp + Timeframe_ms - 1.
        # If we trade at Close of candle T, our trade timestamp should be T + 4h.
        
        sample_trade = db.query(Trade).order_by(Trade.timestamp.desc()).first()
        if sample_trade:
            print(f"\n   ‚ñ∫ LATEST TRADE AUDIT:")
            print(f"     Pair: {sample_trade.pair}")
            print(f"     Timestamp (Signal): {sample_trade.timestamp}")
            print(f"     Entry: {sample_trade.entry_price}")
            print(f"     Result: {sample_trade.result}")
            print(f"     Market State Keys: {list(sample_trade.market_state.keys()) if sample_trade.market_state else 'None'}")
            
    print("\n--- ‚úÖ AUDIT COMPLETE ---")

if __name__ == "__main__":
    verify_results()


================================================================================
# FILE: consolidate_for_notebooklm.py
================================================================================

#!/usr/bin/env python3
"""
Consolidador de c√≥digo para Notebook LM
Convierte 1,462 archivos Python en ~30 fuentes de texto
"""

import os
import glob
from pathlib import Path

# Configuraci√≥n
SOURCE_DIR = "/Users/feux/Desktop/BOT8000"
OUTPUT_DIR = "/Users/feux/Desktop/BOT8000/notebooklm-sources"
MAX_WORDS_PER_FILE = 450000  # Dejar margen al l√≠mite de 500K

# Exclusiones
EXCLUDE_PATTERNS = [
    "**/__pycache__/**",
    "**/.git/**",
    "**/venv/**",
    "**/.venv/**",
    "**/node_modules/**",
    "**/.pytest_cache/**",
    "**/tests/**",
    "**/test_*.py",
    "**/*_test.py",
]

def count_words(text):
    """Cuenta palabras aproximadas"""
    return len(text.split())

def should_include(filepath):
    """Verifica si el archivo debe incluirse"""
    path = Path(filepath)
    
    # Solo archivos Python
    if not path.suffix == '.py':
        return False
    
    # Verificar exclusiones
    for pattern in EXCLUDE_PATTERNS:
        if path.match(pattern):
            return False
    
    return True

def consolidate_files():
    """Consolida archivos Python en chunks"""
    
    # Crear directorio de salida
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Encontrar todos los archivos Python
    all_py_files = []
    for root, dirs, files in os.walk(SOURCE_DIR):
        # Filtrar directorios excluidos
        dirs[:] = [d for d in dirs if d not in ['__pycache__', '.git', 'venv', '.venv', 'node_modules', '.pytest_cache', 'tests']]
        
        for file in files:
            if file.endswith('.py'):
                filepath = os.path.join(root, file)
                if should_include(filepath):
                    all_py_files.append(filepath)
    
    print(f"üìÅ Encontrados {len(all_py_files)} archivos Python")
    
    # Ordenar por directorio para mantener estructura l√≥gica
    all_py_files.sort()
    
    # Consolidar en chunks
    current_chunk = []
    current_words = 0
    chunk_number = 1
    
    for filepath in all_py_files:
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Crear header con info del archivo
            rel_path = os.path.relpath(filepath, SOURCE_DIR)
            header = f"\n{'='*80}\n# FILE: {rel_path}\n{'='*80}\n\n"
            
            file_content = header + content
            file_words = count_words(file_content)
            
            # Si a√±adir este archivo excede el l√≠mite, guardar chunk actual
            if current_words + file_words > MAX_WORDS_PER_FILE and current_chunk:
                save_chunk(current_chunk, chunk_number)
                chunk_number += 1
                current_chunk = []
                current_words = 0
            
            current_chunk.append(file_content)
            current_words += file_words
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error leyendo {filepath}: {e}")
    
    # Guardar √∫ltimo chunk
    if current_chunk:
        save_chunk(current_chunk, chunk_number)
    
    print(f"‚úÖ Consolidaci√≥n completa: {chunk_number} archivos en {OUTPUT_DIR}/")
    
    # Crear √≠ndice
    create_index(all_py_files, chunk_number)

def save_chunk(contents, number):
    """Guarda un chunk de archivos"""
    output_file = os.path.join(OUTPUT_DIR, f"bot8000-code-chunk-{number:02d}.txt")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# BOT8000 Trading Bot - Source Code Chunk {number}\n")
        f.write(f"# Generated for Notebook LM ingestion\n")
        f.write(f"# Total files in this chunk: {len(contents)}\n")
        f.write("="*80 + "\n\n")
        f.write("\n".join(contents))
    
    word_count = count_words("\n".join(contents))
    print(f"  üíæ Chunk {number}: {len(contents)} archivos, ~{word_count:,} palabras")

def create_index(all_files, total_chunks):
    """Crea archivo √≠ndice"""
    index_file = os.path.join(OUTPUT_DIR, "00_INDEX.txt")
    
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write("# BOT8000 Trading Bot - Notebook LM Source Index\n")
        f.write("="*80 + "\n\n")
        f.write(f"Total Python files: {len(all_files)}\n")
        f.write(f"Consolidated into: {total_chunks} chunks\n")
        f.write(f"Max words per chunk: {MAX_WORDS_PER_FILE:,}\n\n")
        
        f.write("## Directory Structure:\n\n")
        
        # Agrupar por directorio
        dirs = {}
        for filepath in all_files:
            rel_path = os.path.relpath(filepath, SOURCE_DIR)
            dir_name = os.path.dirname(rel_path) or "root"
            if dir_name not in dirs:
                dirs[dir_name] = []
            dirs[dir_name].append(os.path.basename(filepath))
        
        for dir_name in sorted(dirs.keys()):
            f.write(f"\n### {dir_name}/\n")
            for filename in sorted(dirs[dir_name])[:10]:  # Limitar a 10 por directorio
                f.write(f"  - {filename}\n")
            if len(dirs[dir_name]) > 10:
                f.write(f"  ... and {len(dirs[dir_name]) - 10} more files\n")
        
        f.write("\n\n## Key Components:\n\n")
        f.write("- src/agents/ - Trading agents (Trend Hunter, Mean Reversion, etc.)\n")
        f.write("- src/core/ - Core logic (Orchestrator, Market State)\n")
        f.write("- src/execution/ - Order execution and risk management\n")
        f.write("- src/alphas/ - Alpha signal generators\n")
        f.write("- src/strategy/ - Trading strategies\n")
        f.write("- src/database/ - Database models and connection\n")
        f.write("- src/optimization/ - Genetic Algorithm and WFO\n")
        f.write("- src/api/ - FastAPI REST API\n")
        f.write("- src/ml/ - Machine learning components\n")
    
    print(f"üìë √çndice creado: {index_file}")

if __name__ == "__main__":
    consolidate_files()


================================================================================
# FILE: debug_pipeline.py
================================================================================

import sys
import logging
from src.agents.worker import OptimizerWorker
from src.database import init_db

# Configure logging to stdout
logging.basicConfig(level=logging.DEBUG)

def debug_run():
    init_db()
    
    # Config similar to what the pipeline uses
    config = {
        'pair': 'BTCUSDT',
        'timeframe': '4h',
        'year': 2024,
        'months': [1, 2],
        'stop_loss': 2000,
        'take_profit_multiplier': 2.0,
        'fee_rate': 0.001,
        'initial_balance': 10000,
        'risk_per_trade_pct': 1.0,
        'backtest_run_id': '00000000-0000-0000-0000-000000000000', # Dummy UUID
        'use_ml_model': False # Disable ML for basic logic test
    }
    
    print("Initializing Worker...")
    worker = OptimizerWorker("DEBUG_WORKER")
    
    print("Running Execute...")
    try:
        result = worker.execute(config)
        print("RESULT:", result)
    except Exception as e:
        print("EXCEPTION:", e)
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_run()


================================================================================
# FILE: find_w1.py
================================================================================

import subprocess
import re

def main():
    # Use command_status output if I could... but I can only run commands.
    # I can't capture the background process output from a new command easily without a temporary file.
    # But wait, I have the background command ID. 
    # The system doesn't provide a way to 'cat' the background proc's stdout from a shell command.
    pass

if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/analyze_top_3.py
================================================================================


import sys
import os

# Add project root to path
sys.path.append(os.getcwd())

from src.database.connection import get_db_session
from src.database.models import Strategy, Trade
from sqlalchemy import desc, func

def calculate_max_drawdown(trades):
    if not trades:
        return 0.0
    
    cumulative_profit = 0
    peak = 0
    max_dd = 0
    
    for t in trades:
        pl = float(t.profit_loss) if t.profit_loss is not None else 0.0
        cumulative_profit += pl
        
        if cumulative_profit > peak:
            peak = cumulative_profit
        
        dd = peak - cumulative_profit
        if dd > max_dd:
            max_dd = dd
            
    return max_dd

def analyze_top_strategies():
    with get_db_session() as db:
        strategies = db.query(Strategy)\
            .filter(Strategy.status == 'APPROVED')\
            .order_by(desc(Strategy.profit_factor))\
            .limit(3)\
            .all()

        print("DEBUG: Trade Counts by Strategy Version:")
        trade_counts = db.query(Trade.strategy_version, func.count(Trade.id))\
            .group_by(Trade.strategy_version).all()
        for tv, count in trade_counts:
            print(f"  {tv}: {count} trades")
        print("\n")

        print(f"Found {len(strategies)} approved strategies.\n")

        for i, s in enumerate(strategies, 1):
            # Fetch trades for this strategy
            # Try matching by name first
            trades = db.query(Trade).filter(Trade.strategy_version == s.name).order_by(Trade.timestamp).all()
            
            # If no trades found, try matching by ID if applicable (though model says String(50))
            if not trades:
                 trades = db.query(Trade).filter(Trade.strategy_version == str(s.strategy_id)).order_by(Trade.timestamp).all()

            total_trades = len(trades)
            
            # Infer info from trades if missing in params
            pair = "N/A"
            timeframe = "N/A"
            if trades:
                pair = trades[0].pair
                timeframe = trades[0].timeframe
            
            # Calculate metrics
            max_dd = calculate_max_drawdown(trades)
            
            print(f"=== STRATEGY #{i}: {s.name} ===")
            print(f"Base Strategy: {s.base_strategy}")
            print(f"Status: {s.status}")
            
            # Performance
            print("\n--- Performance (Calculated from DB) ---")
            print(f"Total Trades: {total_trades}")
            print(f"Win Rate: {float(s.win_rate) if s.win_rate else 0.0:.1f}%")
            print(f"Profit Factor: {float(s.profit_factor) if s.profit_factor else 0.0:.2f}")
            print(f"Max Drawdown: {max_dd:.2f}")
            print(f"Sharpe Ratio: {float(s.sharpe_ratio) if s.sharpe_ratio else 'N/A'}")

            # Parameters
            params = s.parameters or {}
            print("\n--- Parameters ---")
            print(f"Pair: {pair}")
            print(f"Timeframe: {timeframe}")
            print(f"Stop Loss: {params.get('stop_loss', 'N/A')} pips (or multiplier)")
            print(f"Take Profit: {params.get('take_profit_multiplier', 'N/A')} (multiplier)")
            if 'risk_reward' in params:
                print(f"Risk Reward: {params['risk_reward']}")
            
            # Filters (ML)
            filters = s.filters or {}
            print("\n--- ML Filters ---")
            if filters and 'ml_avoidance' in filters:
                avoidance = filters['ml_avoidance']
                print(f"Description: {avoidance.get('description', 'N/A')}")
                print("Conditions:")
                for cond in avoidance.get('conditions', []):
                    print(f"  - {cond}")
            elif filters:
                 for k, v in filters.items():
                    print(f"{k}: {v}")
            else:
                print("None")
            
            print("\n" + "="*30 + "\n")

if __name__ == "__main__":
    analyze_top_strategies()


================================================================================
# FILE: scripts/dashboard_wfo.py
================================================================================

"""
WFO Dashboard - Visualizaci√≥n en tiempo real del progreso WFO.
"""
import argparse
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path

LOG_FILE = Path("results/wfo/wfo_execution.log")
STATUS_FILE = Path("results/wfo/status.json")


def clear_screen():
    """Clear terminal screen."""
    os.system('cls' if os.name == 'nt' else 'clear')


def parse_log_file() -> dict:
    """Parse log file for detailed status."""
    data = {
        "windows": [],
        "current_window": 0,
        "current_gen": 0,
        "total_gens": 8,
        "evaluations": 0,
        "last_fitness": None,
        "best_fitness": None,
        "start_time": None,
        "current_status": "Unknown",
        "errors": 0,
        "trades_subtrain": 0,
        "trades_valtrain": 0,
        "last_lines": []
    }
    
    if not LOG_FILE.exists():
        data["current_status"] = "Waiting for log file..."
        return data
    
    try:
        with open(LOG_FILE, 'r') as f:
            content = f.read()
            lines = content.split('\n')
            data["last_lines"] = lines[-20:]
        
        # Errors
        data["errors"] = content.count("ERROR")
        
        # Window progress
        window_matches = re.findall(r'WINDOW (\d+)/8: ([\w\-]+)', content)
        if window_matches:
            data["current_window"] = int(window_matches[-1][0])
            for match in window_matches:
                data["windows"].append({
                    "id": int(match[0]),
                    "label": match[1]
                })
        
        # Generation progress
        gen_matches = re.findall(r'Generation (\d+)', content)
        if gen_matches:
            data["current_gen"] = int(gen_matches[-1])
        
        # Fitness values
        fitness_matches = re.findall(r'Best fitness.*?: ([0-9.\-]+)', content)
        if fitness_matches:
            values = [float(x) for x in fitness_matches]
            data["last_fitness"] = values[-1]
            data["best_fitness"] = max(values)
        
        # Evaluation count (approximate from backtest logs)
        backtest_count = content.count("Backtest finished")
        data["evaluations"] = backtest_count // 2  # SubTrain + ValTrain per eval
        
        # Current status
        if "WFO COMPLETE" in content:
            data["current_status"] = "‚úÖ COMPLETE"
        elif "GA completed" in content.split("WINDOW")[-1] if "WINDOW" in content else False:
            data["current_status"] = "Running OOS backtest..."
        elif "Running GA" in content:
            data["current_status"] = f"GA Generation {data['current_gen']}"
        elif "Creating fitness" in content:
            data["current_status"] = "Setting up fitness function..."
        elif "Loading candles" in content:
            data["current_status"] = "Loading data..."
        else:
            data["current_status"] = "Processing..."
        
        # Win rates from backtests
        winrate_matches = re.findall(r'WinRate: ([0-9.]+)%', content)
        if winrate_matches:
            data["last_winrate"] = float(winrate_matches[-1])
        
    except Exception as e:
        data["current_status"] = f"Error parsing log: {e}"
    
    return data


def format_time_elapsed(start: datetime) -> str:
    """Format elapsed time."""
    delta = datetime.now() - start
    hours, remainder = divmod(int(delta.total_seconds()), 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


def draw_progress_bar(current: int, total: int, width: int = 40) -> str:
    """Draw ASCII progress bar."""
    if total == 0:
        return "[" + "-" * width + "]"
    
    filled = int(width * current / total)
    bar = "‚ñà" * filled + "‚ñë" * (width - filled)
    pct = 100 * current / total
    return f"[{bar}] {pct:.1f}%"


def render_dashboard(data: dict):
    """Render dashboard to terminal."""
    clear_screen()
    
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    print("‚ïî" + "‚ïê"*68 + "‚ïó")
    print("‚ïë" + "  ü§ñ BOT8000 WFO DASHBOARD  ".center(68) + "‚ïë")
    print("‚ï†" + "‚ïê"*68 + "‚ï£")
    print(f"‚ïë  Last Update: {now}".ljust(69) + "‚ïë")
    print("‚ï†" + "‚ïê"*68 + "‚ï£")
    
    # Window Progress
    print("‚ïë  WINDOW PROGRESS".ljust(69) + "‚ïë")
    window_bar = draw_progress_bar(data["current_window"], 8)
    print(f"‚ïë  {window_bar} Window {data['current_window']}/8".ljust(69) + "‚ïë")
    
    # Generation Progress (within current window)
    print("‚ïë".ljust(69) + "‚ïë")
    print("‚ïë  GA GENERATIONS".ljust(69) + "‚ïë")
    gen_bar = draw_progress_bar(data["current_gen"], data["total_gens"])
    print(f"‚ïë  {gen_bar} Gen {data['current_gen']}/{data['total_gens']}".ljust(69) + "‚ïë")
    
    print("‚ï†" + "‚ïê"*68 + "‚ï£")
    
    # Stats
    print("‚ïë  METRICS".ljust(69) + "‚ïë")
    fitness_str = f"{data['last_fitness']:.4f}" if data['last_fitness'] else "N/A"
    best_str = f"{data['best_fitness']:.4f}" if data['best_fitness'] else "N/A"
    winrate_str = f"{data.get('last_winrate', 0):.1f}%" if data.get('last_winrate') else "N/A"
    
    print(f"‚ïë  Last Fitness: {fitness_str}  |  Best: {best_str}  |  WinRate: {winrate_str}".ljust(69) + "‚ïë")
    print(f"‚ïë  Evaluations: ~{data['evaluations']}  |  Errors: {data['errors']}".ljust(69) + "‚ïë")
    
    print("‚ï†" + "‚ïê"*68 + "‚ï£")
    
    # Status
    status_color = "üü¢" if "COMPLETE" in data["current_status"] else "üü°"
    print(f"‚ïë  STATUS: {status_color} {data['current_status']}".ljust(69) + "‚ïë")
    
    print("‚ï†" + "‚ïê"*68 + "‚ï£")
    
    # Recent log lines
    print("‚ïë  RECENT ACTIVITY".ljust(69) + "‚ïë")
    print("‚ïë" + "-"*68 + "‚ïë")
    
    for line in data["last_lines"][-5:]:
        # Truncate long lines
        if len(line) > 65:
            line = line[:62] + "..."
        # Clean up line
        line = line.replace("\n", "").strip()
        if line:
            print(f"‚ïë  {line}".ljust(69) + "‚ïë")
    
    print("‚ïö" + "‚ïê"*68 + "‚ïù")
    print()
    print("Press Ctrl+C to exit")


def main():
    parser = argparse.ArgumentParser(description="WFO Dashboard")
    parser.add_argument("--refresh", type=int, default=10, help="Refresh interval in seconds")
    
    args = parser.parse_args()
    
    print("Starting WFO Dashboard...")
    print(f"Refresh interval: {args.refresh}s")
    print()
    
    try:
        while True:
            data = parse_log_file()
            render_dashboard(data)
            
            if "COMPLETE" in data["current_status"]:
                print("\n‚úÖ WFO Completed! Dashboard will stop.")
                break
            
            time.sleep(args.refresh)
            
    except KeyboardInterrupt:
        print("\nDashboard stopped.")


if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/download_may_data.py
================================================================================


import os
import sys
import zipfile
import io
import requests

# Set target directory
DATA_DIR = os.path.join(os.getcwd(), 'data', 'raw')
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

BASE_URL = "https://data.binance.vision/data/spot/monthly/klines/BTCUSDT"

def download_and_extract(timeframe, month):
    filename = f"BTCUSDT-{timeframe}-2024-{month:02d}.zip"
    url = f"{BASE_URL}/{timeframe}/{filename}"
    print(f"Downloading {url}...")
    
    try:
        r = requests.get(url)
        if r.status_code == 200:
            z = zipfile.ZipFile(io.BytesIO(r.content))
            z.extractall(DATA_DIR)
            print(f"Extracted to {DATA_DIR}")
            
            # Rename extracted file to match project convention (usually extracted as BTCUSDT-1h-2024-05.csv)
            extracted_name = f"BTCUSDT-{timeframe}-2024-{month:02d}.csv"
            # Binance zip contains a csv with same name usually
            if os.path.exists(os.path.join(DATA_DIR, extracted_name)):
                print(f"Verified {extracted_name} exists.")
            else:
                print(f"Warning: Expected {extracted_name} not found after extraction.")
        else:
            print(f"Failed to download {url}: Status {r.status_code}")
    except Exception as e:
        print(f"Error downloading {filename}: {e}")

if __name__ == "__main__":
    # Download May 2024 (Month 5) for 4h (strategy timeframe) and 15m/1h/5m if needed
    # Top strategies use 4h.
    import argparse
    
    download_and_extract("4h", 5)
    download_and_extract("1h", 5)
    download_and_extract("15m", 5)
    download_and_extract("5m", 5)


================================================================================
# FILE: scripts/get_telegram_chatid.py
================================================================================

"""
Helper para obtener chat_id de Telegram.
"""
import requests
import json

TOKEN = "8413964929:AAEcrozVHC6mkmNIwr65oTTs2Sjb-k2a_mQ"

import time

print("="*70)
print("OBTENIENDO CHAT_ID DE TELEGRAM (MODO AUTOM√ÅTICO)")
print("="*70)
print()
print("INSTRUCCIONES:")
print("1. Abre Telegram en tu celular")
print("2. Busca tu bot (@BotFather te dio el link)")
print("3. Env√≠a el mensaje: /start")
print()
print("Esperando 120 segundos a que env√≠es el mensaje...")

chat_id = None
start_time = time.time()
found = False

while time.time() - start_time < 120:
    try:
        url = f"https://api.telegram.org/bot{TOKEN}/getUpdates"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            if data.get("result"):
                # Buscar chat_id en los updates
                for update in data["result"]:
                    if "message" in update:
                        chat_id = update["message"]["chat"]["id"]
                        found = True
                        break
        
        if found:
            break
            
    except Exception:
        pass
    
    time.sleep(2)
    elapsed = int(time.time() - start_time)
    if elapsed % 10 == 0:
        print(f"Buscando... ({elapsed}s/120s)")

if not found:
    print("\n‚ùå Tiempo agotado. No se encontr√≥ chat_id.")
    print("Aseg√∫rate de enviar /start al bot.")
    exit(1)

print(f"\n‚úÖ Chat ID encontrado: {chat_id}")

try:
    url = f"https://api.telegram.org/bot{TOKEN}/getUpdates"
    response = requests.get(url, timeout=10)
    
    if response.status_code != 200:
        print(f"‚ùå Error: {response.text}")
        exit(1)
    
    data = response.json()
    
    if not data.get("result"):
        print("‚ùå No se encontraron mensajes")
        print("Aseg√∫rate de haber enviado /start al bot")
        exit(1)
    
    # Buscar chat_id en los updates
    chat_ids = set()
    for update in data["result"]:
        if "message" in update:
            chat_id = update["message"]["chat"]["id"]
            chat_ids.add(chat_id)
    
    if not chat_ids:
        print("‚ùå No se encontr√≥ chat_id")
        exit(1)
    
    if len(chat_ids) == 1:
        chat_id = list(chat_ids)[0]
        print(f"‚úÖ Chat ID encontrado: {chat_id}")
        
        # Guardar configuraci√≥n autom√°ticamente
        from pathlib import Path
        
        config = {
            "bot_token": TOKEN,
            "chat_id": str(chat_id)
        }
        
        config_path = Path("results/wfo/telegram_config.json")
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"‚úÖ Configuraci√≥n guardada en: {config_path}")
        
        # Enviar mensaje de prueba
        print()
        print("Enviando mensaje de prueba...")
        
        test_url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        test_data = {
            "chat_id": chat_id,
            "text": "ü§ñ *BOT8000 WFO*\n\n‚úÖ Telegram configurado correctamente!\n\nRecibir√°s alertas aqu√≠ cuando:\n‚Ä¢ El proceso se detenga\n‚Ä¢ Haya problemas detectados\n‚Ä¢ El WFO se complete",
            "parse_mode": "Markdown"
        }
        
        test_response = requests.post(test_url, data=test_data, timeout=10)
        
        if test_response.status_code == 200:
            print("‚úÖ Mensaje de prueba enviado!")
            print("Revisa tu Telegram")
        else:
            print(f"‚ö†Ô∏è No se pudo enviar mensaje: {test_response.text}")
        
    else:
        print(f"‚ö†Ô∏è Se encontraron m√∫ltiples chat_ids: {chat_ids}")
        print("Por favor selecciona el correcto manualmente")
        
except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

print()
print("="*70)


================================================================================
# FILE: scripts/migrate_add_symbol.py
================================================================================

# scripts/migrate_add_symbol.py
from sqlalchemy import text
from src.database import get_db_session

def migrate():
    with get_db_session() as db:
        # 0. Count existing trades for validation
        initial_count = db.execute(text("SELECT COUNT(*) FROM trades")).scalar()
        print(f"Found {initial_count} existing trades")
        
        # 1. Add nullable column
        print("Adding symbol column (nullable)...")
        db.execute(text("ALTER TABLE trades ADD COLUMN IF NOT EXISTS symbol VARCHAR(20)"))
        db.execute(text("ALTER TABLE strategies ADD COLUMN IF NOT EXISTS symbol VARCHAR(20)"))
        
        # 2. Backfill legacy data
        print("Backfilling legacy data with 'BTCUSDT'...")
        db.execute(text("UPDATE trades SET symbol = 'BTCUSDT' WHERE symbol IS NULL"))
        db.execute(text("UPDATE strategies SET symbol = 'BTCUSDT' WHERE symbol IS NULL"))
        
        # 3. Set NOT NULL
        print("Setting NOT NULL constraint...")
        db.execute(text("ALTER TABLE trades ALTER COLUMN symbol SET NOT NULL"))
        db.execute(text("ALTER TABLE strategies ALTER COLUMN symbol SET NOT NULL"))
        
        # 4. Create indexes
        print("Creating indexes...")
        db.execute(text("CREATE INDEX IF NOT EXISTS idx_trades_symbol ON trades(symbol)"))
        db.execute(text("CREATE INDEX IF NOT EXISTS idx_strategies_symbol ON strategies(symbol)"))
        
        db.commit()
        
        # 5. Verify migration
        migrated_count = db.execute(text("SELECT COUNT(*) FROM trades WHERE symbol = 'BTCUSDT'")).scalar()
        assert migrated_count == initial_count, f"Expected {initial_count} trades, got {migrated_count}"
        print(f"‚úì Verified {migrated_count} legacy trades migrated correctly")
        print("‚úì Migration complete")

if __name__ == "__main__":
    migrate()


================================================================================
# FILE: scripts/monitor_wfo.py
================================================================================

"""
WFO Monitor - Monitorea el proceso WFO y env√≠a alertas via Telegram.
"""
import argparse
import json
import os
import re
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# Telegram config
TELEGRAM_CONFIG_PATH = Path("results/wfo/telegram_config.json")
STATUS_FILE = Path("results/wfo/status.json")
ALERTS_LOG = Path("results/wfo/alerts.log")
LOG_FILE = Path("results/wfo/wfo_execution.log")


def load_telegram_config():
    """Load Telegram configuration."""
    if TELEGRAM_CONFIG_PATH.exists():
        with open(TELEGRAM_CONFIG_PATH) as f:
            return json.load(f)
    return None


def send_telegram_alert(message: str, config: dict = None):
    """Send alert via Telegram."""
    if config is None:
        config = load_telegram_config()
    
    if not config:
        print(f"[ALERT] (No Telegram) {message}")
        return False
    
    try:
        import requests
        url = f"https://api.telegram.org/bot{config['bot_token']}/sendMessage"
        data = {
            "chat_id": config['chat_id'],
            "text": message,
            "parse_mode": "Markdown"
        }
        response = requests.post(url, data=data, timeout=10)
        return response.status_code == 200
    except Exception as e:
        print(f"[ERROR] Telegram send failed: {e}")
        return False


def log_alert(message: str):
    """Log alert to file."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(ALERTS_LOG, 'a') as f:
        f.write(f"[{timestamp}] {message}\n")


def is_process_running(pid: int) -> bool:
    """Check if process is running."""
    try:
        os.kill(pid, 0)
        return True
    except OSError:
        return False


def get_log_stats() -> dict:
    """Get statistics from log file."""
    stats = {
        "size": 0,
        "lines": 0,
        "current_window": 0,
        "last_fitness": None,
        "last_update": None,
        "errors": 0
    }
    
    if not LOG_FILE.exists():
        return stats
    
    stats["size"] = LOG_FILE.stat().st_size
    stats["last_update"] = datetime.fromtimestamp(LOG_FILE.stat().st_mtime)
    
    try:
        with open(LOG_FILE, 'r') as f:
            content = f.read()
            lines = content.split('\n')
            stats["lines"] = len(lines)
            
            # Find current window
            window_matches = re.findall(r'WINDOW (\d+)/8', content)
            if window_matches:
                stats["current_window"] = int(window_matches[-1])
            
            # Find last fitness
            fitness_matches = re.findall(r'Best fitness.*?: ([0-9.\-]+)', content)
            if fitness_matches:
                stats["last_fitness"] = float(fitness_matches[-1])
            
            # Count errors
            stats["errors"] = content.count("ERROR")
            
    except Exception as e:
        print(f"[WARN] Error reading log: {e}")
    
    return stats


def save_status(status: dict):
    """Save current status to file."""
    STATUS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(STATUS_FILE, 'w') as f:
        json.dump(status, f, indent=2, default=str)


def monitor_wfo(pid: int, interval: int = 300):
    """Main monitoring loop."""
    print("="*70)
    print("WFO MONITOR INICIADO")
    print("="*70)
    print(f"PID: {pid}")
    print(f"Intervalo: {interval}s")
    print(f"Log: {LOG_FILE}")
    print()
    
    telegram_config = load_telegram_config()
    if telegram_config:
        print("‚úÖ Telegram configurado - alertas activadas")
        send_telegram_alert("üöÄ *WFO Monitor Iniciado*\n\nMonitoreando proceso...", telegram_config)
    else:
        print("‚ö†Ô∏è Telegram no configurado - solo alertas locales")
    
    print()
    print("Monitoreando... (Ctrl+C para detener)")
    print("-"*70)
    
    last_log_size = 0
    stuck_count = 0
    
    try:
        while True:
            # Check process
            if not is_process_running(pid):
                message = "üî¥ *ALERTA: Proceso WFO terminado*\n\nEl proceso ya no est√° corriendo."
                print(f"\n[ALERT] {message}")
                log_alert(message)
                send_telegram_alert(message, telegram_config)
                
                # Check if completed successfully
                stats = get_log_stats()
                if stats["current_window"] == 8:
                    complete_msg = "‚úÖ *WFO COMPLETADO*\n\nEl proceso termin√≥ exitosamente."
                    send_telegram_alert(complete_msg, telegram_config)
                
                break
            
            # Get stats
            stats = get_log_stats()
            
            # Check if stuck
            if stats["size"] == last_log_size and last_log_size > 0:
                stuck_count += 1
                if stuck_count >= 3:  # 3 intervals sin cambios
                    message = f"‚ö†Ô∏è *ALERTA: Proceso posiblemente atascado*\n\nLog sin cambios por {stuck_count * interval}s"
                    print(f"\n[WARN] {message}")
                    log_alert(message)
                    send_telegram_alert(message, telegram_config)
                    stuck_count = 0
            else:
                stuck_count = 0
                last_log_size = stats["size"]
            
            # Check for bad fitness
            if stats["last_fitness"] is not None and stats["last_fitness"] < -100:
                message = f"‚ö†Ô∏è *ALERTA: Fitness muy bajo*\n\nFitness: {stats['last_fitness']:.2f}"
                log_alert(message)
                # Don't spam Telegram for this
            
            # Update status
            status = {
                "timestamp": datetime.now().isoformat(),
                "pid": pid,
                "running": True,
                "window": stats["current_window"],
                "log_size_mb": stats["size"] / (1024*1024),
                "log_lines": stats["lines"],
                "last_fitness": stats["last_fitness"],
                "errors": stats["errors"],
                "last_update": stats["last_update"].isoformat() if stats["last_update"] else None
            }
            save_status(status)
            
            # Print status
            now = datetime.now().strftime("%H:%M:%S")
            print(f"[{now}] Window {stats['current_window']}/8 | "
                  f"Log: {stats['size']/(1024*1024):.1f}MB | "
                  f"Lines: {stats['lines']} | "
                  f"Fitness: {stats['last_fitness'] if stats['last_fitness'] else 'N/A'}")
            
            time.sleep(interval)
            
    except KeyboardInterrupt:
        print("\n\nMonitor detenido por usuario")
        send_telegram_alert("‚è∏Ô∏è *WFO Monitor Detenido*\n\nEl monitor fue detenido manualmente.", telegram_config)


def main():
    parser = argparse.ArgumentParser(description="WFO Monitor")
    parser.add_argument("--pid", type=int, required=True, help="PID del proceso WFO")
    parser.add_argument("--interval", type=int, default=300, help="Intervalo de monitoreo en segundos")
    
    args = parser.parse_args()
    monitor_wfo(args.pid, args.interval)


if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/optimizer.py
================================================================================

import argparse
import sys
import yaml
import os
from pathlib import Path
from decimal import Decimal

# Ensure src is in path
sys.path.append(os.path.abspath("src"))

from optimization.types import OptimizerConfig
from optimization.engine import OptimizerEngine
from optimization.analyzer import ResultAnalyzer
from data.downloader import download_binance_data

def load_config(config_path: str) -> OptimizerConfig:
    with open(config_path, 'r') as f:
        data = yaml.safe_load(f)
        
    return OptimizerConfig(
        timeframes=data['timeframes'],
        pairs=data['pairs'],
        stop_losses=[Decimal(str(x)) for x in data['stop_losses']],
        take_profit_multiples=[float(x) for x in data['take_profit_multiples']],
        fee_rates=[Decimal(str(x)) for x in data['fee_rates']],
        initial_balance=Decimal(str(data['initial_balance'])),
        risk_percent=Decimal(str(data['risk_percent'])),
        data_path=data['data_path'],
        download_years=data.get('download_years', [2024]),
        download_months=data.get('download_months', list(range(1, 13))),
        parallel=data.get('parallel', True),
        checkpoint_interval=data.get('checkpoint_interval', 10)
    )

def main():
    parser = argparse.ArgumentParser(description="TJR Bot Massive Optimizer")
    parser.add_argument("--config", type=str, default="optimizer_config.yaml", help="Path to config file")
    parser.add_argument("--analyze", action="store_true", help="Run analysis only")
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    if not args.analyze:
        # Automatic Data Download
        download_binance_data(
            pairs=config.pairs,
            timeframes=config.timeframes,
            years=config.download_years,
            months=config.download_months,
            data_dir=config.data_path
        )
        
        print(f"--- Starting Optimization ---")
        print(f"Timeframes: {config.timeframes}")
        print(f"Pairs: {config.pairs}")
        
        engine = OptimizerEngine(config)
        engine.run()
        print("Optimization Complete.")
        
    # Always run analysis after
    results_file = Path("results/optimizer_results.csv")
    if results_file.exists():
        analyzer = ResultAnalyzer(results_file)
        analyzer.load()
        analyzer.save_summary(Path("results/summary.txt"))
        print("\nSummary Report generated in results/summary.txt")
    else:
        print("No results to analyze.")

if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/run_backtest.py
================================================================================

import sys
import os
from decimal import Decimal
from typing import List

# Ensure src is in path
sys.path.append(os.path.abspath("src"))

from core.timeframe import Timeframe
from core.market import MarketState
from utils.data_loader import load_binance_csv
from simulation.backtest import Backtester
from strategy.engine import TJRStrategy
from execution.executor import TradeExecutor
from execution.risk import RiskManager, RiskConfig
from execution.broker import Broker, OrderRequest, OrderResult, Position, OrderSide

from simulation.broker import InMemoryBroker

def main():
    print("--- TJR Trading Bot v2: Backtest Console (1H TIMEFRAME - PIVOT) ---")
    data_path = "data/raw/BTCUSDT-1h-2024-12.csv"
    
    if not os.path.exists(data_path):
        print(f"Error: Data file not found at {data_path}")
        return

    print(f"Loading data from {data_path}...")
    candles = load_binance_csv(data_path, Timeframe.H1)
    print(f"Loaded {len(candles)} candles.")

    # Setup
    broker = InMemoryBroker(balance=Decimal("10000"))
    risk = RiskManager(RiskConfig(Decimal("0.01"))) # 1% Risk
    executor = TradeExecutor(broker, risk)
    strategy = TJRStrategy()
    backtester = Backtester()

    print("Running Backtest...")
    report = backtester.run(candles, broker, strategy, executor, Timeframe.H1)

    print("\n--- RESULTS (1H) ---")
    print(f"Initial Balance: ${report.initial_balance}")
    print(f"Final Balance:   ${report.final_balance:.2f}")
    print(f"Net Profit:      ${report.net_profit:.2f}")
    print(f"Total Fees Paid: ${broker.total_fees_paid:.2f}")
    print(f"Total Trades:    {report.total_trades}")
    print(f"Win Rate:        {report.win_rate:.2f}%")
    print(f"Max Drawdown:    ${report.max_drawdown:.2f}")
    print("----------------")


if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/run_multi_month_backtest.py
================================================================================

import sys
import os
import glob
from decimal import Decimal
from typing import List

# Ensure src is in path
sys.path.append(os.path.abspath("src"))

from core.timeframe import Timeframe
from utils.data_loader import load_binance_csv
from simulation.backtest import Backtester
from strategy.engine import TJRStrategy
from execution.executor import TradeExecutor
from execution.risk import RiskManager, RiskConfig
from simulation.broker import InMemoryBroker

def run_backtest_for_file(file_path: str, timeframe: Timeframe):
    # Setup
    broker = InMemoryBroker(balance=Decimal("10000"))
    risk = RiskManager(RiskConfig(Decimal("0.01"))) # 1% Risk
    executor = TradeExecutor(broker, risk)
    strategy = TJRStrategy()
    backtester = Backtester()

    # Load data
    candles = load_binance_csv(file_path, timeframe)
    
    # Run
    report = backtester.run(candles, broker, strategy, executor, timeframe)
    
    return report, broker.total_fees_paid

def main():
    # SETTINGS
    TARGET_TIMEFRAME = Timeframe.H4
    GLO_PATTERN = "*-4h-*.csv"
    
    print(f"--- TJR Trading Bot v2: Multi-Month Backtest ({TARGET_TIMEFRAME.value.upper()}) ---")
    data_dir = "data/raw/"
    csv_files = sorted(glob.glob(os.path.join(data_dir, GLO_PATTERN)))
    
    if not csv_files:
        print(f"No {TARGET_TIMEFRAME.value} CSV files found in {data_dir}")
        return

    results = []
    
    print(f"{'Month':<15} | {'Net Profit':<11} | {'Fees':<10} | {'Win Rate':<8} | {'Trades':<6} | {'Max DD':<9}")
    print("-" * 75)

    total_net = Decimal("0")
    total_fees = Decimal("0")
    total_trades = 0
    months_profitable = 0

    for file_path in csv_files:
        # Expected format: BTCUSDT-4h-2024-01.csv
        parts = os.path.basename(file_path).split("-")
        month_name = f"{parts[2]}-{parts[3].replace('.csv', '')}" 
        
        try:
            report, fees = run_backtest_for_file(file_path, TARGET_TIMEFRAME)
            
            if report.net_profit > 0:
                months_profitable += 1
                
            print(f"{month_name:<15} | ${report.net_profit:>10.2f} | ${fees:>9.2f} | {report.win_rate:>7.2f}% | {report.total_trades:>6} | ${report.max_drawdown:>8.2f}")
            
            results.append({
                "month": month_name,
                "profit": report.net_profit,
                "fees": fees,
                "win_rate": report.win_rate,
                "trades": report.total_trades,
                "dd": report.max_drawdown
            })
            
            total_net += report.net_profit
            total_fees += fees
            total_trades += report.total_trades
            
        except Exception as e:
            print(f"{month_name:<15} | Error: {e}")

    print("-" * 75)
    print(f"{'TOTAL':<15} | ${total_net:>10.2f} | ${total_fees:>9.2f} | {'N/A':<8} | {total_trades:>6} | {'N/A':<9}")
    print(f"\nMonths Profitable: {months_profitable}/{len(csv_files)}")
    print(f"Total Strategy Net (After Fees) for {TARGET_TIMEFRAME.value}: ${total_net:.2f}")

if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/run_oos_validation.py
================================================================================


import sys
import os
import asyncio
from decimal import Decimal
import pandas as pd

# Add project root to path
sys.path.append(os.getcwd())

from src.database.connection import get_db_session
from src.database.models import Strategy
from sqlalchemy import desc
# from src.core.config import Config # Config not found, removing for now if unused or fixing path
# Checking if Config is actually needed. RiskManager uses RiskConfig which is imported from risk.
# StrategyRegistry is used. TJRStrategy is used.
# Let's remove this line for now.
# from src.strategy.registry import StrategyRegistry
from src.strategy.engine import TJRStrategy
from src.simulation.broker import InMemoryBroker
from src.utils.data_loader import load_binance_csv
from src.core.market import MarketState
from src.execution.executor import TradeExecutor
from src.execution.broker import OrderRequest, OrderSide, OrderType
from src.execution.risk import RiskManager, RiskConfig
from src.core.timeframe import Timeframe

def run_validation():
    print("=== OOS VALIDATION START (Data: May 2024) ===")
    
    # 1. Fetch Top 3 Strategies
    print("Fetching top 3 strategies...")
    with get_db_session() as db:
        strategies = db.query(Strategy)\
            .filter(Strategy.status == 'APPROVED')\
            .order_by(desc(Strategy.profit_factor))\
            .limit(3)\
            .all()
        strategy_data = []
        for s in strategies:
             strategy_data.append({
                 "name": s.name,
                 "pf": float(s.profit_factor) if s.profit_factor else 0.0,
                 "params": s.parameters,
                 "id": s.id
             })

    if not strategy_data:
        print("No approved strategies found.")
        return

    target_strategy_name = "TJR_Base_mut_58f9db"
    
    # 2. Define OOS Months
    oos_months = [
        {"name": "JUNE 2024", "file": "BTCUSDT-4h-2024-06.csv"},
        {"name": "JULY 2024", "file": "BTCUSDT-4h-2024-07.csv"}
    ]

    for month_info in oos_months:
        print(f"\n{'#'*20} TESTING {month_info['name']} {'#'*20}")
        
        data_path = os.path.join(os.getcwd(), 'data', 'raw', month_info['file'])
        if not os.path.exists(data_path):
            print(f"Error: File not found {data_path}")
            continue
            
        all_candles = load_binance_csv(data_path, Timeframe.H4)
        all_candles.sort(key=lambda c: c.timestamp)
        print(f"Data Loaded: {len(all_candles)} candles (4h).")

        for strat_info in strategy_data:
            if strat_info['name'] != target_strategy_name:
                continue
                
            print(f"\n>>> Strategy: {strat_info['name']}")
            
            # Instantiate Components
            raw_params = strat_info['params']
            sl_val = raw_params.get('stop_loss')
            tp_val = raw_params.get('take_profit_multiplier')
            
            strategy = TJRStrategy(
                fixed_stop_loss=Decimal(str(sl_val)),
                take_profit_multiplier=Decimal(str(tp_val))
            )
            
            broker = InMemoryBroker(
                balance=Decimal("10000"),
                fee_rate=Decimal("0.001")
            )
            
            risk_manager = RiskManager(
                config=RiskConfig(risk_percentage=Decimal("0.01"))
            )
            
            market = MarketState.empty("BTCUSDT")
            trades_count = 0
            
            for candle in all_candles:
                market = market.update(candle)
                broker.update_positions(Decimal(str(candle.close)))
                
                if not broker.get_positions():
                    signal = strategy.analyze(market, Timeframe.H4)
                    if signal:
                         # Signal object contains calculated SL/TP
                         sl_price = signal.stop_loss
                         tp_price = signal.take_profit
                         
                         if sl_price and tp_price:
                             entry_price = Decimal(str(candle.close))
                             size = risk_manager.calculate_position_size(
                                 account_balance=broker.get_balance(),
                                 entry_price=entry_price,
                                 stop_loss=sl_price
                             )
                             
                             if size > 0:
                                 order_req = OrderRequest(
                                     symbol="BTCUSDT",
                                     side=signal.side,
                                     type=OrderType.MARKET, 
                                     quantity=size,
                                     price=entry_price, 
                                     stop_loss=sl_price,
                                     take_profit=tp_price
                                 )
                                 
                                 broker.place_order(order_req)
                                 trades_count += 1
            
            # Metrics
            closed_trades = broker.get_closed_positions()
            wins = [t for t in closed_trades if t.pnl > 0]
            losses = [t for t in closed_trades if t.pnl <= 0]
            
            gross_profit = sum(t.pnl for t in wins)
            gross_loss = abs(sum(t.pnl for t in losses))
            
            pf = 0.0
            if gross_loss == 0:
                if gross_profit > 0:
                    pf = float('inf')
            else:
                pf = float(gross_profit / gross_loss)
                
            win_rate = (len(wins) / len(closed_trades) * 100) if closed_trades else 0.0
            
            print(f"Trades: {len(closed_trades)}")
            print(f"PF: {pf:.2f}")
            print(f"WR: {win_rate:.1f}%")
            
            if pf > 1.5:
                print(">>> STATUS: PASS (Robust)")
            else:
                print(">>> STATUS: FAIL or WEAK")

if __name__ == "__main__":
    run_validation()


================================================================================
# FILE: scripts/run_oos_validation_draft.py
================================================================================


import sys
import os
import asyncio

# Add project root to path
sys.path.append(os.getcwd())

from src.database.connection import get_db_session
from src.database.models import Strategy
from sqlalchemy import desc
from src.agents.worker import OptimizerWorker
from src.core.config import Config
from src.strategy.registry import StrategyRegistry
# Import necessary strategy classes to ensure registration
from src.strategy.implementations.tjr_strategy import TJRStrategy

async def run_validation():
    print("=== OOS VALIDATION START (Data: May 2024) ===")
    
    # 1. Fetch Top 3 Strategies
    print("Fetching top 3 strategies...")
    with get_db_session() as db:
        strategies = db.query(Strategy)\
            .filter(Strategy.status == 'APPROVED')\
            .order_by(desc(Strategy.profit_factor))\
            .limit(3)\
            .all()
        strategy_data = []
        for s in strategies:
             strategy_data.append({
                 "name": s.name,
                 "pf": float(s.profit_factor) if s.profit_factor else 0.0,
                 "params": s.parameters,
                 "id": s.id
             })

    # 2. Setup Worker for OOS
    # We use May 2024 (month 5). Config months are 0-indexed in some systems, 
    # but usually data loader takes 1-based integers for filenames.
    # OptimizerWorker might take a list of months.
    
    # We'll instantiate a worker just to use its run_strategy_backtest logic or similar.
    # Actually, let's use the core backtesting components directly or via a modified worker 
    # if possible, but worker is tied to queues.
    # Let's verify how to run a single pass. 
    # Looking at src/agents/worker.py (previously viewed), it has .run() which does everything.
    # Maybe we can use the internal _run_backtest method if exposed, or create a minimal runner.
    
    # Let's assume we can create a temporary worker and call a method to run a specific config.
    # Or better, re-use the logic: Broker -> Engine -> Run.
    
    from src.simulation.broker import InMemoryBroker
    from src.data.feed import DataFeed
    from src.utils.data_loader import DataLoader
    from src.strategy.engine import StrategyEngine
    
    # Load May Data
    print("Loading May 2024 Data...")
    loader = DataLoader() # Assuming default args correct or we pass root
    # We need to construct Candle series. 
    # data_loader.load_data usually takes pairs, years, months.
    candles = loader.load_data(
        pairs=['BTCUSDT'],
        years=[2024],
        months=[5], # May
        timeframes=['4h', '1h', '15m'] # Start with 4h as base
    )
    
    if not candles:
        print("Error: No data loaded for May 2024")
        return

    print(f"Data Loaded: {len(candles)} keys.")

    for strat_info in strategy_data:
        print(f"\n--- Testing {strat_info['name']} ---")
        print(f"IS (In-Sample) PF: {strat_info['pf']:.2f}")
        
        # Instantiate Strategy
        # We need the class. TJR_Base -> TJRStrategy
        strategy_cls = StrategyRegistry.get('TJR_Base')
        if not strategy_cls:
            strategy_cls = TJRStrategy # Fallback if registry empty
            
        # Params need to be compliant
        params = strat_info['params']
        
        # Run Backtest
        broker = InMemoryBroker()
        
        # Setup Engine
        engine = StrategyEngine(strategy_cls, params, broker)
        
        # Feed data
        # Engine usually runs on a feed. checking engine.run() signature in mind...
        # If we don't have the engine source in front, i'll write a generic loop 
        # based on standard patterns or assume engine.run(candles).
        
        # Looking at previous context, OptimizerWorker uses:
        # engine = StrategyEngine(strategy, broker)
        # for timestamp in timeline: engine.update(timestamp, market_data)
        
        # We need a timeline (sorted unique timestamps from 4h candles)
        # Assuming candles['BTCUSDT']['4h'] is a list of Candle objects
        c4h = candles['BTCUSDT']['4h']
        timeline = sorted([c.timestamp for c in c4h])
        
        print(f"Running simulation on {len(timeline)} ticks (4h)...")
        
        # Simple Loop
        for ts in timeline:
             # Construct slice for this timestamp
             # Market Snapshot needed by engine?
             # Or does engine calculate indicators itself? 
             # Usually TJRStrategy needs historical data up to TS to calc indicators.
             
             # The engine.on_candle(candle) model?
             # Let's check engine usage in worker.py via direct look if needed. 
             # For now, simplistic approach:
             
             # Actually, simpler: Use the actual OptimizerWorker but force the strategy and data.
             # But Worker is complex with Celery. 
             
             # Let's try to infer engine.run method or just Iterate.
             pass 

        # Since I can't easily see Engine signature without reading again, 
        # I will READ worker.py quickly to copy the exact execution loop.
        pass

async def main():
    await run_validation()

if __name__ == "__main__":
    # This is a placeholder until I read worker.py to copy execution logic
    pass


================================================================================
# FILE: scripts/run_wfo.py
================================================================================

"""
Ejecuta Walk-Forward Optimization completo.
8 windows, GA optimization, an√°lisis de resultados.
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import json
import time
from datetime import datetime
from typing import Dict, Any, List, Tuple
from pathlib import Path
import argparse
import math

from src.core.market import load_candles_from_csv
from src.agents.worker import OptimizerWorker
from src.optimization.windows import generate_windows, WindowConfig
from src.optimization.param_space import get_default_param_space
from src.optimization.fitness import calculate_fitness, SegmentMetrics
from src.optimization.genetic_algorithm import GeneticAlgorithm, GAConfig


def create_fitness_function(
    worker: OptimizerWorker,
    subtrain_data,
    valtrain_data,
    param_space,
    window_warmup_data
):
    """
    Crea funci√≥n de fitness que usa Worker REAL.
    
    Esta funci√≥n se llama 256 veces por window (GA evaluations).
    """
    def fitness_fn(params: Dict[str, Any]) -> float:
        """
        Eval√∫a params haciendo backtest en SubTrain y ValTrain.
        """
        try:
            # Backtest en SubTrain
            subtrain_months = extract_months_from_candles(subtrain_data)
            config_sub = {
                "pair": "BTCUSDT",
                "timeframe": "4h",
                "year": 2024,
                "months": subtrain_months,
                "backtest_run_id": "wfo_subtrain",
                "initial_balance": 10000.0,
                "stop_loss": 100,  # Fallback only; GA's stop_loss_atr_mult takes priority
                "take_profit_multiplier": 2.0,  # Fallback only; GA's take_profit_r_mult takes priority
                "fee_rate": 0.001,
                "risk_per_trade_pct": 1.0,  # Fallback only; GA's risk_per_trade_pct takes priority
                "use_msc": True,
                "max_portfolio_risk": 0.10,
                "use_dd_scaling": True
            }
            
            # Backtest en SubTrain
            # Use window_warmup_data for SubTrain (as it is the start of Train)
            result_sub = worker.run(
                config=config_sub,
                params=params,
                candles=subtrain_data,
                warmup_candles=window_warmup_data,
                initial_balance=10000.0
            )
            
            # DEBUG LOGGING
            print(f"    SubTrain: {result_sub.get('total_trades', 0)} trades, "
                  f"PF={result_sub.get('profit_factor', 0):.2f}, "
                  f"Return={result_sub.get('return', 0) if result_sub.get('return') else 0.0:.1f}%")
            
            metrics_sub = SegmentMetrics(
                trades=result_sub.get("total_trades", 0),
                return_pct=result_sub.get("return", 0.0) / 100.0 if result_sub.get("return") else 0.0,
                maxdd=result_sub.get("max_drawdown", 0.0) / 100.0 if result_sub.get("max_drawdown") else 0.0,
                sharpe=result_sub.get("sharpe", 0.0),
                pf=result_sub.get("profit_factor", 0.0),
                gross_profit=result_sub.get("gross_profit", 0.0),
                gross_loss=result_sub.get("gross_loss", 0.0)
            )
            
            # DEBUG LOGGING
            print(f"    ‚Üí SegmentMetrics: {metrics_sub.trades} trades")
            
            # Backtest en ValTrain
            valtrain_months = extract_months_from_candles(valtrain_data)
            config_val = {
                "pair": "BTCUSDT",
                "timeframe": "4h",
                "year": 2024,
                "months": valtrain_months,
                "backtest_run_id": "wfo_valtrain",
                "initial_balance": 10000.0,
                "stop_loss": 100,  # Fallback only; GA's stop_loss_atr_mult takes priority
                "take_profit_multiplier": 2.0,  # Fallback only; GA's take_profit_r_mult takes priority
                "fee_rate": 0.001,
                "risk_per_trade_pct": 1.0,  # Fallback only; GA's risk_per_trade_pct takes priority
                "use_msc": True,
                "max_portfolio_risk": 0.10,
                "use_dd_scaling": True
            }
            
            # Backtest en ValTrain
            # Warmup for ValTrain is the end of SubTrain
            val_warmup = subtrain_data[-240:] if len(subtrain_data) >= 240 else subtrain_data
            
            result_val = worker.run(
                config=config_val,
                params=params,
                candles=valtrain_data,
                warmup_candles=val_warmup,
                initial_balance=10000.0
            )
            
            metrics_val = SegmentMetrics(
                trades=result_val.get("total_trades", 0),
                return_pct=result_val.get("return", 0.0) / 100.0 if result_val.get("return") else 0.0,
                maxdd=result_val.get("max_drawdown", 0.0) / 100.0 if result_val.get("max_drawdown") else 0.0,
                sharpe=result_val.get("sharpe", 0.0),
                pf=result_val.get("profit_factor", 0.0),
                gross_profit=result_val.get("gross_profit", 0.0),
                gross_loss=result_val.get("gross_loss", 0.0)
            )
            
            # Calcular fitness
            fitness = calculate_fitness(
                params=params,
                metrics_sub=metrics_sub,
                metrics_val=metrics_val,
                param_space=param_space
            )
            
            return fitness
            
        except Exception as e:
            print(f"Error en fitness evaluation: {e}")
            return float('-inf')
    
    return fitness_fn


def extract_months_from_candles(candles) -> List[int]:
    """
    Extrae lista de meses √∫nicos de un conjunto de candles.
    """
    if not candles:
        return []
    
    months = set()
    for candle in candles:
        # candle.timestamp is Unix timestamp in ms
        dt = datetime.fromtimestamp(candle.timestamp / 1000)
        months.add(dt.month)
    
    return sorted(list(months))


def split_train_data(train_data, n_months_train: int):
    """
    Split train data en SubTrain (primeros N-1 meses) y ValTrain (√∫ltimo mes).
    """
    subtrain_months = n_months_train - 1
    split_idx = len(train_data) * subtrain_months // n_months_train
    
    subtrain_data = train_data[:split_idx]
    valtrain_data = train_data[split_idx:]
    
    return subtrain_data, valtrain_data


def run_wfo(population_size: int = 50, num_generations: int = 10, window_limit: int = None):
    """
    Ejecuta Walk-Forward Optimization completo.
    """
    print("="*70)
    print("WALK-FORWARD OPTIMIZATION - BOT8000 v3")
    print("="*70)
    print()
    
    # Configuraci√≥n
    param_space = get_default_param_space()
    
    config_windows = WindowConfig(
        train_months=4,
        test_months=1,
        step_months=1,
        year=2024,
        warmup_bars=240
    )
    
    config_ga = GAConfig(
        population_size=population_size,
        num_generations=num_generations,
        tournament_size=3,
        crossover_rate=0.8,
        mutation_rate=0.15,
        mutation_sigma_pct=0.10,
        elitism_count=2,
        early_stopping_generations=3,
        seed=42
    )
    
    print(f"WFO Configuration:")
    print(f"  Train months: {config_windows.train_months}")
    print(f"  Test months: {config_windows.test_months}")
    print(f"  Step months: {config_windows.step_months}")
    print(f"  Warmup bars: {config_windows.warmup_bars}")
    print()
    print(f"GA Configuration:")
    print(f"  Population: {config_ga.population_size}")
    print(f"  Generations: {config_ga.num_generations}")
    print(f"  Max evaluations per window: ~256")
    print()
    
    # Cargar datos completos
    print("Loading candles from CSV...")
    candles_path = "data/BTCUSDT_4h_2024.csv"
    
    if not os.path.exists(candles_path):
        print(f"ERROR: {candles_path} not found")
        print("Please ensure you have 2024 data available")
        return
    
    full_data = load_candles_from_csv(candles_path)
    print(f"Loaded {len(full_data)} candles")
    print()
    
    # Generar windows
    print("Generating windows...")
    windows = generate_windows(full_data, config_windows)
    print(f"Generated {len(windows)} windows")
    
    if window_limit:
        windows = windows[:window_limit]
        print(f"Limiting to {window_limit} windows for testing.")
    print()
    
    # Resultados
    results = []
    cumulative_balance = 10000.0
    
    # Procesar cada window
    for i, window in enumerate(windows):
        print("="*70)
        print(f"WINDOW {i+1}/{len(windows)}: {window.label}")
        print("="*70)
        print()
        
        start_time = time.time()
        
        # Split train
        print(f"  Train data: {len(window.train_data)} candles")
        subtrain_data, valtrain_data = split_train_data(
            window.train_data,
            config_windows.train_months
        )
        print(f"  SubTrain: {len(subtrain_data)} candles")
        print(f"  ValTrain: {len(valtrain_data)} candles")
        print()
        
        # Crear worker
        worker = OptimizerWorker(worker_id=f"wfo_w{i+1}")
        
        # Crear fitness function
        print(f"  Creating fitness function...")
        fitness_fn = create_fitness_function(
            worker=worker,
            subtrain_data=subtrain_data,
            valtrain_data=valtrain_data,
            param_space=param_space,
            window_warmup_data=window.warmup_data
        )
        
        # Ejecutar GA
        print(f"  Running GA optimization...")
        print(f"  This may take several hours...")
        print()
        
        ga = GeneticAlgorithm(
            param_space=param_space,
            config=config_ga,
            fitness_function=fitness_fn
        )
        
        best_individual, history = ga.optimize()
        
        # Phase 4: Guard against all-`-inf` GA result
        if not math.isfinite(best_individual.fitness):
            print(f"  ‚ö†Ô∏è WARNING: No valid solution found (fitness=-inf). Using default params.")
            best_individual.params = param_space.get_defaults()
        
        elapsed = time.time() - start_time
        print()
        print(f"  GA completed in {elapsed/3600:.1f} hours")
        best_fit_str = f"{best_individual.fitness:.4f}" if math.isfinite(best_individual.fitness) else "-inf"
        print(f"  Best fitness (train): {best_fit_str}")
        print(f"  Generations run: {len(history)}")
        print()
        
        # Backtest en Test OOS con params √≥ptimos
        print(f"  Running OOS backtest with optimal params...")
        test_months = extract_months_from_candles(window.test_data)
        config_test = {
            "pair": "BTCUSDT",
            "timeframe": "4h",
            "year": 2024,
            "months": test_months,
            "backtest_run_id": f"wfo_test_w{i+1}",
            "initial_balance": cumulative_balance,
            "stop_loss": 100,  # Fallback only; GA's stop_loss_atr_mult takes priority
            "take_profit_multiplier": 2.0,  # Fallback only; GA's take_profit_r_mult takes priority
            "fee_rate": 0.001,
            "risk_per_trade_pct": 1.0,  # Fallback only; GA's risk_per_trade_pct takes priority
            "use_msc": True,
            "max_portfolio_risk": 0.10,
            "use_dd_scaling": True,
        }
        
        # Warmup for Test is the end of Train
        test_warmup = window.train_data[-240:] if len(window.train_data) >= 240 else window.train_data
        
        result_test = worker.run(
            config=config_test,
            params=best_individual.params,
            candles=window.test_data,
            warmup_candles=test_warmup,
            initial_balance=cumulative_balance
        )
        
        # Actualizar balance acumulado
        new_balance = result_test.get("final_balance", cumulative_balance)
        window_return = (new_balance - cumulative_balance) / cumulative_balance
        
        print(f"  Test Results:")
        print(f"    Trades: {result_test.get('trades', 0)}")
        print(f"    Win Rate: {result_test.get('win_rate', 0.0):.1f}%")
        print(f"    Profit Factor: {result_test.get('profit_factor', 0.0):.2f}")
        print(f"    Return: {window_return*100:.2f}%")
        print(f"    Balance: ${cumulative_balance:.2f} ‚Üí ${new_balance:.2f}")
        print()
        
        # Guardar resultado
        window_result = {
            "window_id": i + 1,
            "label": window.label,
            "train_fitness": best_individual.fitness,
            "train_generations": len(history),
            "optimal_params": best_individual.params,
            "test_trades": result_test.get("total_trades", 0),
            "test_win_rate": result_test.get("win_rate", 0.0),
            "test_pf": result_test.get("profit_factor", 0.0),
            "test_return_pct": window_return * 100,
            "test_maxdd": result_test.get("max_drawdown", 0.0),
            "test_sharpe": result_test.get("sharpe", 0.0),
            "start_balance": cumulative_balance,
            "end_balance": new_balance,
            "elapsed_hours": elapsed / 3600
        }
        
        results.append(window_result)
        cumulative_balance = new_balance

        # Partial Save (Auto-save after each window)
        try:
            partial_file = Path("results/wfo/partial_results.json")
            partial_file.parent.mkdir(parents=True, exist_ok=True)
            with open(partial_file, 'w') as pf:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "windows_completed": len(results),
                    "current_balance": cumulative_balance,
                    "results": results
                }, pf, indent=2)
            print(f"  [AUTO-SAVE] Window {i+1} saved to {partial_file}")
        except Exception as e:
            print(f"  [WARN] Failed to save partial results: {e}")
    
    # An√°lisis agregado
    print()
    print("="*70)
    print("WFO COMPLETE - AGGREGATE RESULTS")
    print("="*70)
    print()
    
    final_balance = cumulative_balance
    total_return = (final_balance - 10000) / 10000
    
    avg_test_pf = sum(r["test_pf"] for r in results) / len(results)
    median_test_pf = sorted([r["test_pf"] for r in results])[len(results)//2]
    
    winning_windows = sum(1 for r in results if r["test_pf"] > 1.1)
    pass_rate = winning_windows / len(results)
    
    print(f"Initial Balance: $10,000")
    print(f"Final Balance: ${final_balance:.2f}")
    print(f"Total Return: {total_return*100:.2f}%")
    print()
    print(f"Test PF Statistics:")
    print(f"  Average: {avg_test_pf:.2f}")
    print(f"  Median: {median_test_pf:.2f}")
    print(f"  Pass Rate (PF > 1.1): {pass_rate*100:.0f}%")
    print()
    
    # Overfitting detection
    pf_values = [r["test_pf"] for r in results]
    log_pf_values = [math.log(max(pf, 0.01)) for pf in pf_values]
    std_log_pf = (sum((x - sum(log_pf_values)/len(log_pf_values))**2 for x in log_pf_values) / len(log_pf_values)) ** 0.5
    
    failing_windows = sum(1 for r in results if r["test_pf"] < 1.0)
    
    print(f"Overfitting Analysis:")
    print(f"  Std(log(PF)): {std_log_pf:.3f} {'‚ö†Ô∏è HIGH VARIANCE' if std_log_pf > 0.30 else '‚úÖ OK'}")
    print(f"  Failing windows (PF < 1.0): {failing_windows}/8 {'‚ö†Ô∏è TOO MANY' if failing_windows >= 3 else '‚úÖ OK'}")
    print()
    
    # Guardar resultados
    output_dir = Path("results/wfo")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = output_dir / f"wfo_results_{timestamp}.json"
    
    output = {
        "config": {
            "windows": len(windows),
            "ga_population": config_ga.population_size,
            "ga_generations": config_ga.num_generations
        },
        "summary": {
            "initial_balance": 10000.0,
            "final_balance": final_balance,
            "total_return_pct": total_return * 100,
            "avg_test_pf": avg_test_pf,
            "median_test_pf": median_test_pf,
            "pass_rate": pass_rate,
            "std_log_pf": std_log_pf,
            "failing_windows": failing_windows
        },
        "windows": results
    }
    
    with open(results_file, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"Results saved to: {results_file}")
    print()
    print("="*70)
    print("WFO COMPLETE")
    print("="*70)
    
    # Decisi√≥n final
    print()
    print("NEXT STEPS:")
    print()
    
    if avg_test_pf > 1.2 and pass_rate > 0.67 and std_log_pf < 0.30:
        print("‚úÖ SYSTEM READY FOR PAPER TRADING")
        print("   - Strong performance (avg PF > 1.2)")
        print("   - High pass rate (> 67%)")
        print("   - Low variance (stable)")
        print()
        print("   Proceed to 30-day paper trading validation")
    else:
        print("‚ö†Ô∏è SYSTEM NEEDS ADJUSTMENT")
        if avg_test_pf <= 1.2:
            print(f"   - Low average PF ({avg_test_pf:.2f})")
        if pass_rate <= 0.67:
            print(f"   - Low pass rate ({pass_rate*100:.0f}%)")
        if std_log_pf >= 0.30:
            print(f"   - High variance ({std_log_pf:.3f})")
        print()
        print("   Consider:")
        print("   - Adjust fitness function weights")
        print("   - Modify parameter ranges")
        print("   - Add more training data")
        print("   - Review worst-performing windows")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run Walk-Forward Optimization')
    parser.add_argument('--population', type=int, default=50, help='GA Population size')
    parser.add_argument('--generations', type=int, default=10, help='GA Generations')
    parser.add_argument('--limit', type=int, default=None, help='Limit number of windows (for testing)')
    
    args = parser.parse_args()
    
    run_wfo(population_size=args.population, num_generations=args.generations, window_limit=args.limit)


================================================================================
# FILE: scripts/setup_telegram_alerts.py
================================================================================

"""
Setup Telegram Alerts for WFO Monitor.
Interactive setup wizard.
"""
import json
import sys
from pathlib import Path

try:
    import requests
except ImportError:
    print("Installing requests...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "requests"])
    import requests


CONFIG_PATH = Path("results/wfo/telegram_config.json")


def test_bot_token(token: str) -> bool:
    """Test if bot token is valid."""
    try:
        url = f"https://api.telegram.org/bot{token}/getMe"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data.get("ok"):
                bot_name = data["result"]["username"]
                print(f"‚úÖ Bot v√°lido: @{bot_name}")
                return True
        print(f"‚ùå Token inv√°lido: {response.text}")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False


def send_test_message(token: str, chat_id: str) -> bool:
    """Send test message."""
    try:
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": "ü§ñ *BOT8000 WFO Monitor*\n\n‚úÖ Configuraci√≥n exitosa!\n\nRecibir√°s alertas aqu√≠.",
            "parse_mode": "Markdown"
        }
        response = requests.post(url, data=data, timeout=10)
        return response.status_code == 200
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False


def main():
    print("="*70)
    print("CONFIGURACI√ìN DE ALERTAS TELEGRAM")
    print("="*70)
    print()
    
    # Check existing config
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH) as f:
            existing = json.load(f)
        print(f"‚úÖ Configuraci√≥n existente encontrada")
        print(f"   Token: {existing['bot_token'][:20]}...")
        print(f"   Chat ID: {existing['chat_id']}")
        print()
        
        choice = input("¬øReconfigurar? (s/n): ").strip().lower()
        if choice != 's':
            print("Usando configuraci√≥n existente.")
            return
    
    print()
    print("PASO 1: TOKEN DEL BOT")
    print("-"*40)
    print("Necesitas crear un bot en Telegram:")
    print("1. Abre Telegram y busca @BotFather")
    print("2. Env√≠a /newbot")
    print("3. Sigue las instrucciones")
    print("4. Copia el token que te da")
    print()
    
    token = input("Pega tu bot token: ").strip()
    
    if not token:
        # Use default token
        token = "8413964929:AAEcrozVHC6mkmNIwr65oTTs2Sjb-k2a_mQ"
        print(f"Usando token predeterminado")
    
    if not test_bot_token(token):
        print("Token inv√°lido. Verifica e intenta de nuevo.")
        return
    
    print()
    print("PASO 2: CHAT ID")
    print("-"*40)
    print("Ahora necesito tu Chat ID:")
    print("1. Abre Telegram")
    print("2. Busca tu bot y √°brelo")
    print("3. Env√≠a el mensaje /start")
    print()
    
    input("Presiona Enter despu√©s de enviar /start al bot...")
    
    print()
    print("Buscando tu Chat ID...")
    
    try:
        url = f"https://api.telegram.org/bot{token}/getUpdates"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            print(f"‚ùå Error: {response.text}")
            return
        
        data = response.json()
        
        if not data.get("result"):
            print("‚ùå No se encontraron mensajes")
            print("Aseg√∫rate de haber enviado /start al bot")
            
            # Manual entry
            chat_id = input("Ingresa tu Chat ID manualmente: ").strip()
        else:
            # Find chat_id
            chat_id = None
            for update in data["result"]:
                if "message" in update:
                    chat_id = str(update["message"]["chat"]["id"])
                    break
            
            if not chat_id:
                chat_id = input("Ingresa tu Chat ID manualmente: ").strip()
            else:
                print(f"‚úÖ Chat ID encontrado: {chat_id}")
    
    except Exception as e:
        print(f"‚ùå Error: {e}")
        chat_id = input("Ingresa tu Chat ID manualmente: ").strip()
    
    if not chat_id:
        print("No se pudo obtener Chat ID")
        return
    
    # Save config
    config = {
        "bot_token": token,
        "chat_id": chat_id
    }
    
    CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(CONFIG_PATH, 'w') as f:
        json.dump(config, f, indent=2)
    
    print()
    print(f"‚úÖ Configuraci√≥n guardada en: {CONFIG_PATH}")
    
    # Test message
    print()
    print("Enviando mensaje de prueba...")
    if send_test_message(token, chat_id):
        print("‚úÖ Mensaje de prueba enviado! Revisa tu Telegram.")
    else:
        print("‚ö†Ô∏è No se pudo enviar mensaje de prueba")
    
    print()
    print("="*70)
    print("CONFIGURACI√ìN COMPLETADA")
    print("="*70)
    print()
    print("Ahora puedes ejecutar el monitor con alertas:")
    print()
    print("  python scripts/monitor_wfo.py --pid <PID> --interval 300")
    print()


if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/validate_alpha_engine.py
================================================================================

# scripts/validate_alpha_engine.py
from src.agents.worker import OptimizerWorker
from src.utils.data_loader import load_binance_csv
from src.core.timeframe import Timeframe
import uuid

def run_comparison():
    # Dataset config
    pair = 'BTCUSDT'
    tf_str = '4h'
    timeframe = Timeframe(tf_str)
    year = 2024
    months = [1]
    
    config_base = {
        'pair': pair,
        'timeframe': tf_str,
        'year': year,
        'months': months,
        'stop_loss': 2000,
        'take_profit_multiplier': 2.0,
        'initial_balance': 10000,
        'fee_rate': 0.001,
        'risk_per_trade_pct': 1.0,
        'backtest_run_id': str(uuid.uuid4())
    }
    
    print(f"Loading data for {pair} {tf_str} {year}...")
    
    # Test 1: Legacy TJR
    print("\nRunning Test 1: Legacy TJR Strategy...")
    worker_legacy = OptimizerWorker("legacy-test")
    config_legacy = {**config_base, 'use_alpha_engine': False}
    result_legacy = worker_legacy.run(config_legacy)
    
    # Test 2: Alpha Engine
    print("\nRunning Test 2: Pure Alpha Engine...")
    worker_alpha = OptimizerWorker("alpha-test")
    config_alpha = {**config_base, 'use_alpha_engine': True, 'alpha_threshold': 0.1}
    result_alpha = worker_alpha.run(config_alpha)
    
    # Compare
    print("\n" + "="*30)
    print("      VALIDATION RESULTS")
    print("="*30)
    print(f"Legacy TJR:")
    print(f"  Trades: {result_legacy.get('total_trades', 0)}")
    print(f"  PF: {result_legacy.get('profit_factor', 0):.2f}")
    print(f"  Win Rate: {result_legacy.get('win_rate', 0):.1f}%")
    print(f"  Final Balance: ${result_legacy.get('final_balance', 0):.2f}")
    
    print(f"\nAlpha Engine:")
    print(f"  Trades: {result_alpha.get('total_trades', 0)}")
    print(f"  PF: {result_alpha.get('profit_factor', 0):.2f}")
    print(f"  Win Rate: {result_alpha.get('win_rate', 0):.1f}%")
    print(f"  Final Balance: ${result_alpha.get('final_balance', 0):.2f}")
    print("="*30)
    
    # Validation minimum requirements
    assert result_alpha.get('total_trades', 0) > 0, "Alpha Engine generated NO trades"
    print("\n‚úì Alpha Engine validation PASSED")

if __name__ == "__main__":
    run_comparison()


================================================================================
# FILE: scripts/validate_full_system.py
================================================================================

# scripts/validate_full_system.py
import sys
import os
# Asegurar que el root del proyecto est√© en el path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.agents.worker import OptimizerWorker
import uuid

def run_full_comparison():
    # Dataset: BTCUSDT 4h, todo 2024
    # Nota: El user pidi√≥ meses 1-12.
    config_base = {
        'pair': 'BTCUSDT',
        'timeframe': '4h',
        'year': 2024,
        'months': list(range(1, 13)),  # Ene-Dic
        'stop_loss': 2000,
        'take_profit_multiplier': 2.0,
        'initial_balance': 10000,
        'fee_rate': 0.001,
        'risk_per_trade_pct': 1.0,
        'alpha_threshold': 0.1 # Threshold bajo para asegurar que veamos actividad en este test de validaci√≥n
    }
    
    results = {}
    
    # Test 1: Legacy TJR
    print("\n=== Test 1: Legacy TJR ===")
    worker1 = OptimizerWorker("legacy")
    config1 = {**config_base, 'backtest_run_id': str(uuid.uuid4()), 'use_msc': False, 'use_alpha_engine': False}
    results['legacy'] = worker1.run(config1)
    
    # Test 2: Alpha Engine
    print("\n=== Test 2: Alpha Engine ===")
    worker2 = OptimizerWorker("alpha")
    config2 = {**config_base, 'use_alpha_engine': True, 'use_msc': False, 'backtest_run_id': str(uuid.uuid4())}
    results['alpha'] = worker2.run(config2)
    
    # Test 3: MSC Orchestrator
    print("\n=== Test 3: MSC Orchestrator ===")
    worker3 = OptimizerWorker("msc")
    config3 = {**config_base, 'use_msc': True, 'use_alpha_engine': False, 'backtest_run_id': str(uuid.uuid4())}
    results['msc'] = worker3.run(config3)
    
    # Comparaci√≥n
    print("\n" + "="*60)
    print("COMPARACI√ìN DE SISTEMAS (2024 COMPLETO)")
    print("="*60)
    
    # Tabla simple
    print(f"{'Sistema':<20} | {'Trades':<8} | {'WinRate':<10} | {'PF':<6} | {'Balance':<12} | {'MaxDD':<8}")
    print("-" * 75)
    
    for system, result in results.items():
        print(f"{system.upper():<20} | "
              f"{result.get('total_trades', 0):<8} | "
              f"{result.get('win_rate', 0):.1f}%{' ':<5} | "
              f"{result.get('profit_factor', 0):.2f}{' ':<2} | "
              f"${result.get('final_balance', 0):<11,.2f} | "
              f"{result.get('max_drawdown_pct', 0):.1f}%")
    
    print("="*60)
    
    # Validaci√≥n m√≠nima
    assert results['msc'].get('total_trades', 0) > 0, "MSC no gener√≥ trades"
    print("\n‚úì Full system validation PASSED")

if __name__ == "__main__":
    run_full_comparison()


================================================================================
# FILE: scripts/validate_walk_forward.py
================================================================================


import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import uuid
import numpy as np
import pandas as pd
from typing import List, Dict, Any
from src.agents.worker import OptimizerWorker

class WalkForwardValidator:
    def __init__(self):
        self.year = 2024
        self.base_config = {
            'pair': 'BTCUSDT',
            'timeframe': '4h',
            'year': self.year,
            'stop_loss': 2000,
            'take_profit_multiplier': 2.0,
            'initial_balance': 10000,
            'fee_rate': 0.001,
            'risk_per_trade_pct': 1.0,
            'use_msc': True,  # Use MSC (v3) as requested
            'use_alpha_engine': False,
            'alpha_threshold': 0.1
        }
    
    def generate_windows(self) -> List[Dict[str, Any]]:
        """
        Generate 9 rolling windows:
        W1: Train [1,2,3] -> Test 4
        ...
        W9: Train [9,10,11] -> Test 12
        """
        windows = []
        for i in range(1, 10):
            train_start = i
            train_end = i + 2
            test_month = i + 3
            windows.append({
                'id': i,
                'train_months': list(range(train_start, train_end + 1)),
                'test_month': test_month,
                'label': f"Train {train_start}-{train_end} -> Test {test_month}"
            })
        return windows

    def run(self) -> Dict[str, Any]:
        """Execute the WFV process."""
        windows = self.generate_windows()
        window_results = []
        
        print("\n=== STARTING WALK-FORWARD VALIDATION (MSC v3) ===")
        print(f"Data: {self.year} | Windows: {len(windows)}\n")
        
        for win in windows:
            print(f"Running Window {win['id']}: {win['label']}...")
            
            # NOTE: Normally we would Optimize on Train then Validate on Test.
            # For this validation request, we are assuming the Strategy logic is constant (Layer 1 Brain)
            # and we just want to see how it performs on the Out-of-Sample month.
            # So we only run the backtest on the 'test_month'.
            
            worker = OptimizerWorker(f"WFV-Win{win['id']}")
            config = self.base_config.copy()
            config['months'] = [win['test_month']]
            config['backtest_run_id'] = str(uuid.uuid4())
            
            try:
                result = worker.run(config)
                
                # Add window metadata
                result['window_id'] = win['id']
                result['test_month'] = win['test_month']
                window_results.append(result)
                
                print(f"-> Result: PF {result.get('profit_factor', 0):.2f} | WR {result.get('win_rate', 0)}% | Trades {result.get('total_trades', 0)}")
                
            except Exception as e:
                print(f"-> Error in Window {win['id']}: {e}")
                # Append empty/failed result to keep index alignment? Or just skip using
                window_results.append({
                     'window_id': win['id'],
                     'test_month': win['test_month'],
                     'profit_factor': 0.0,
                     'win_rate': 0.0,
                     'total_trades': 0,
                     'final_balance': 10000.0,
                     'max_drawdown_pct': 0.0
                })
        
        # Calculate stats
        stats = self.calculate_statistics(window_results)
        
        return {
            'window_results': window_results,
            'stats': stats
        }
    
    def calculate_statistics(self, results: List[Dict]) -> Dict[str, float]:
        if not results:
            return {}
            
        pfs = [r.get('profit_factor', 0) for r in results]
        wrs = [r.get('win_rate', 0) for r in results]
        bals = [r.get('final_balance', 10000) for r in results]
        
        avg_pf = float(np.mean(pfs))
        std_pf = float(np.std(pfs))
        
        avg_wr = float(np.mean(wrs))
        
        # Total return of the system (compounding or sum?)
        # Sum of net profits:
        total_profit = sum(b - 10000 for b in bals)
        final_system_balance = 10000 + total_profit
        
        return {
            'avg_pf': round(avg_pf, 2),
            'consistency_score': round(std_pf, 2),
            'avg_win_rate': round(avg_wr, 1),
            'total_profit': round(total_profit, 2),
            'final_est_balance': round(final_system_balance, 2)
        }

    def print_report(self, summary: Dict):
        print("\n" + "="*80)
        print("WALK-FORWARD VALIDATION REPORT")
        print("="*80)
        
        print(f"{'Window':<8} | {'Test Month':<12} | {'Trades':<8} | {'WinRate':<10} | {'PF':<6} | {'Balance':<12} | {'MaxDD':<8}")
        print("-" * 80)
        
        for res in summary['window_results']:
            print(f"W{res.get('window_id', 0):<7} | "
                  f"{res.get('test_month', 0):<12} | "
                  f"{res.get('total_trades', 0):<8} | "
                  f"{res.get('win_rate', 0):.1f}%{' ':<5} | "
                  f"{res.get('profit_factor', 0):.2f}{' ':<2} | "
                  f"${res.get('final_balance', 0):<11,.2f} | "
                  f"{res.get('max_drawdown_pct', 0):.1f}%")
        
        print("-" * 80)
        stats = summary['stats']
        print(f"\nAVERAGE PROFIT FACTOR: {stats.get('avg_pf', 0)} (Target > 1.2)")
        print(f"CONSISTENCY SCORE (StdDev): {stats.get('consistency_score', 0)} (Target < 0.5)")
        print(f"ESTIMATED TOTAL BALANCE: ${stats.get('final_est_balance', 0):,.2f}")
        print("="*80)

if __name__ == "__main__":
    validator = WalkForwardValidator()
    summary = validator.run()
    validator.print_report(summary)


================================================================================
# FILE: src/agents/__init__.py
================================================================================

# src/agents/__init__.py
from src.agents.base import BaseAgent
from src.agents.types import AgentStatus, AgentMessage, AgentProgress

__all__ = [
    'BaseAgent',
    'AgentStatus',
    'AgentMessage',
    'AgentProgress',
]


================================================================================
# FILE: src/agents/base.py
================================================================================

# src/agents/base.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from datetime import datetime
import logging
import uuid

from src.agents.types import AgentStatus, AgentProgress
from src.database import get_db_session
from src.database.repository import AgentLogRepository

class BaseAgent(ABC):
    """Clase base para todos los agentes del sistema"""
    
    def __init__(self, agent_name: str):
        self.agent_name = agent_name
        self.agent_id = uuid.uuid4()
        self.status = AgentStatus.IDLE
        self.progress: Optional[AgentProgress] = None
        self.started_at: Optional[datetime] = None
        self.completed_at: Optional[datetime] = None
        self.error: Optional[str] = None
        self._db_enabled = True
        
        # Setup logging
        self.logger = logging.getLogger(f"agent.{agent_name}")
        self._setup_logging()
    
    def _setup_logging(self):
        """Configurar logging del agente"""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                f'[{self.agent_name}] %(asctime)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def log(self, level: str, message: str, context: Optional[Dict[str, Any]] = None):
        """Log con persistencia en DB"""
        # Log a consola
        log_method = getattr(self.logger, level.lower())
        log_method(message)
        
        # Log a DB
        # Log a DB
        if self._db_enabled:
            try:
                with get_db_session() as db:
                    AgentLogRepository.log(db, self.agent_name, level.upper(), message, context)
            except Exception as e:
                self._db_enabled = False
                self.logger.warning(f"DB Logging disabled due to error: {e}")
    
    def update_progress(self, current: int, total: int, message: str):
        """Actualizar progreso del agente"""
        self.progress = AgentProgress.create(
            current=current,
            total=total,
            status=self.status,
            message=message
        )
        self.log('INFO', f"Progress: {self.progress.percentage}% - {message}")
    
    def start(self):
        """Iniciar agente"""
        self.status = AgentStatus.RUNNING
        self.started_at = datetime.utcnow()
        self.log('INFO', f"Agent {self.agent_name} started")
    
    def complete(self):
        """Marcar agente como completado"""
        self.status = AgentStatus.COMPLETED
        self.completed_at = datetime.utcnow()
        duration = (self.completed_at - self.started_at).total_seconds() if self.started_at else 0
        self.log('INFO', f"Agent {self.agent_name} completed in {duration:.2f}s")
    
    def fail(self, error: str):
        """Marcar agente como fallido"""
        self.status = AgentStatus.FAILED
        self.error = error
        self.completed_at = datetime.utcnow()
        self.log('ERROR', f"Agent {self.agent_name} failed: {error}")
    
    @abstractmethod
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Ejecutar l√≥gica del agente (debe ser implementado por subclases)"""
        pass
    
    def execute(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Ejecutar agente con manejo de errores"""
        try:
            self.start()
            result = self.run(config)
            self.complete()
            return result
        except Exception as e:
            self.fail(str(e))
            raise
    
    def get_status(self) -> Dict[str, Any]:
        """Obtener status actual del agente"""
        return {
            'agent_name': self.agent_name,
            'agent_id': str(self.agent_id),
            'status': self.status.value,
            'progress': {
                'current': self.progress.current if self.progress else 0,
                'total': self.progress.total if self.progress else 0,
                'percentage': self.progress.percentage if self.progress else 0,
                'message': self.progress.message if self.progress else ''
            } if self.progress else None,
            'started_at': self.started_at.isoformat() if self.started_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'error': self.error
        }


================================================================================
# FILE: src/agents/breakout_hunter.py
================================================================================

# src/agents/breakout_hunter.py
from src.agents.trading_agent import TradingAgent
from src.core.regime import MarketRegime

class BreakoutHunterAgent(TradingAgent):
    """
    Agent specialized in identifying potential breakouts.
    Bias: High Liquidity and OB Quality.
    """
    
    def __init__(self):
        super().__init__(
            alpha_weights={
                'ob_quality': 2.0,
                'liquidity': 3.0,
                'volatility': 1.5,
                'ml_confidence': 1.0,
                'momentum': 1.0
            },
            active_regimes=[
                MarketRegime.BREAKOUT_PENDING
            ]
        )


================================================================================
# FILE: src/agents/data_agent.py
================================================================================

# src/agents/data_agent.py
from typing import Dict, Any, List
from pathlib import Path
import os

from src.agents.base import BaseAgent
from src.data.downloader import download_binance_data

class DataAgent(BaseAgent):
    """
    Agente responsable de descargar y gestionar datos hist√≥ricos
    
    Responsabilidades:
    - Descargar CSVs de Binance Vision
    - Validar integridad de archivos
    - Reportar metadata de datasets
    """
    
    def __init__(self):
        super().__init__("DataAgent")
        self.data_dir = Path("data/raw")
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ejecutar descarga de datos
        
        Args:
            config: {
                'pairs': ['BTCUSDT', 'ETHUSDT'],
                'timeframes': ['4h', '1d'],
                'years': [2023, 2024],
                'months': [1, 2, ..., 12]  # opcional
            }
        
        Returns:
            {
                'downloaded_files': ['BTCUSDT-4h-2024-01.csv', ...],
                'total_files': 24,
                'total_candles': 52560,
                'data_dir': 'data/raw/'
            }
        """
        pairs = config.get('pairs', [])
        timeframes = config.get('timeframes', [])
        years = config.get('years', [])
        months = config.get('months', list(range(1, 13)))
        
        # Calcular total de archivos proyectados
        total_files_projected = len(pairs) * len(timeframes) * len(years) * len(months)
        
        self.log('INFO', f"Starting download request for {total_files_projected} potential files", {
            'pairs': pairs,
            'timeframes': timeframes,
            'years': years
        })
        
        # El downloader corporativo ya itera sobre todo si le pasamos listas.
        # Pero para tener granularidad en el progreso del agente, podr√≠amos llamarlo por par/tf.
        
        downloaded_files = []
        total_candles = 0
        current_step = 0
        total_steps = len(pairs) * len(timeframes) * len(years)
        
        for pair in pairs:
            for timeframe in timeframes:
                for year in years:
                    self.log('INFO', f"Processing {pair} {timeframe} {year}")
                    
                    try:
                        # Adaptado para usar la firma correcta de src/data/downloader.py
                        download_binance_data(
                            pairs=[pair],
                            timeframes=[timeframe],
                            years=[year],
                            months=months,
                            data_dir=str(self.data_dir)
                        )
                        
                        # Verificar qu√© se descarg√≥ realmente
                        for month in months:
                            filename = f"{pair}-{timeframe}-{year}-{month:02d}.csv"
                            filepath = self.data_dir / filename
                            
                            if filepath.exists():
                                # Solo agregamos si no estaba ya en la lista (evitar duplicados si corre varias veces)
                                if filename not in downloaded_files:
                                    downloaded_files.append(filename)
                                    # Contar l√≠neas (velas)
                                    with open(filepath, 'r') as f:
                                        # Restamos el header si existe
                                        lines = sum(1 for _ in f)
                                        if lines > 0:
                                            total_candles += (lines - 1)
                        
                        current_step += 1
                        self.update_progress(
                            current_step,
                            total_steps,
                            f"Processed {pair} {timeframe} {year}. Total files: {len(downloaded_files)}"
                        )
                    
                    except Exception as e:
                        self.log('ERROR', f"Error processing {pair} {timeframe} {year}: {str(e)}")
                        current_step += 1
                        continue
        
        result = {
            'downloaded_files': downloaded_files,
            'total_files': len(downloaded_files),
            'total_candles': total_candles,
            'data_dir': str(self.data_dir)
        }
        
        self.log('INFO', f"DataAgent run completed: {len(downloaded_files)} files active, {total_candles} candles total")
        
        return result
    
    def get_available_data(self) -> List[Dict[str, Any]]:
        """Listar datos disponibles en disco"""
        files = []
        for filepath in self.data_dir.glob("*.csv"):
            parts = filepath.stem.split('-')
            if len(parts) >= 4:
                files.append({
                    'filename': filepath.name,
                    'pair': parts[0],
                    'timeframe': parts[1],
                    'year': parts[2],
                    'month': parts[3],
                    'size_bytes': filepath.stat().st_size,
                    'path': str(filepath)
                })
        return files


================================================================================
# FILE: src/agents/mean_reversion.py
================================================================================

# src/agents/mean_reversion.py
from src.agents.trading_agent import TradingAgent
from src.core.regime import MarketRegime

class MeanReversionAgent(TradingAgent):
    """
    Agent specialized in identifying overextended moves and mean reversion setups.
    Bias: High OB Quality and Negative Momentum.
    """
    
    def __init__(self):
        super().__init__(
            alpha_weights={
                'ob_quality': 3.0,
                'momentum': -1.5, # Contrarian bias
                'volatility': 0.5,
                'ml_confidence': 1.0,
                'liquidity': 0.8
            },
            active_regimes=[
                MarketRegime.SIDEWAYS_RANGE
            ]
        )


================================================================================
# FILE: src/agents/optimizer_swarm.py
================================================================================

# src/agents/optimizer_swarm.py
from typing import Dict, Any, List
from concurrent.futures import ProcessPoolExecutor, as_completed
import uuid
from datetime import datetime

from src.agents.base import BaseAgent
from src.agents.worker import OptimizerWorker
from src.database import get_db_session
from src.database.repository import BacktestRunRepository

class OptimizerSwarm(BaseAgent):
    """
    Orquestador de m√∫ltiples workers paralelos
    
    Crea un backtest run y distribuye trabajo entre workers
    """
    
    def __init__(self, num_workers: int = 4):
        super().__init__("OptimizerSwarm")
        self.num_workers = num_workers
    
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Distribuir backtests entre workers paralelos
        
        Args:
            config: {
                'pairs': ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'BNBUSDT'],
                'timeframes': ['4h', '1d'],
                'years': [2024],
                'months': [1, 2, ..., 12],
                'stop_losses': [1000, 1500, 2000],
                'risk_rewards': [1.5, 2.0, 2.5],
                'fee_rates': [0.001, 0.0004],
                'initial_balance': 10000,
                'risk_per_trade_pct': 1.0
            }
        """
        # Create backtest run en DB
        run_id = uuid.uuid4()
        
        with get_db_session() as db:
            run = BacktestRunRepository.create(db, config)
            run_id = run.run_id
        
        self.log('INFO', f"Created backtest run {run_id}")
        
        # Generate all configuration combinations
        configs = self._generate_configs(config, run_id)
        total_configs = len(configs)
        
        self.log('INFO', f"Generated {total_configs} configurations to test with {self.num_workers} workers")
        
        # Execute in parallel
        results = []
        completed = 0
        failed = 0
        
        with ProcessPoolExecutor(max_workers=self.num_workers) as pool:
            # Submit all tasks
            # Note: _run_worker must be at module level or static if using ProcessPool on some systems
            # but usually within a class method it needs to be carefully handled.
            # In Python, we often use a helper function at the top level.
            future_to_config = {
                pool.submit(_execute_worker_task, i, cfg): cfg
                for i, cfg in enumerate(configs)
            }
            
            # Process as they complete
            for future in as_completed(future_to_config):
                cfg = future_to_config[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    self.update_progress(
                        completed + failed,
                        total_configs,
                        f"Completed {completed}/{total_configs} configs. Latest: {cfg['pair']} {cfg['timeframe']}"
                    )
                
                except Exception as e:
                    self.log('ERROR', f"Config failed for {cfg['pair']} {cfg['timeframe']}: {str(e)}")
                    failed += 1
        
        # Update backtest run status
        with get_db_session() as db:
            BacktestRunRepository.complete(db, run_id)
        
        final_result = {
            'backtest_run_id': str(run_id),
            'total_configs': total_configs,
            'completed': completed,
            'failed': failed,
            'results': results
        }
        
        self.log('INFO', f"Optimizer swarm completed: {completed} successful, {failed} failed")
        
        return final_result
    
    def _generate_configs(self, config: Dict[str, Any], run_id: uuid.UUID) -> List[Dict[str, Any]]:
        """Generar todas las combinaciones de configuraciones"""
        pairs = config.get('pairs', [])
        timeframes = config.get('timeframes', [])
        years = config.get('years', [])
        months = config.get('months', list(range(1, 13)))
        stop_losses = config.get('stop_losses', [1000])
        risk_rewards = config.get('risk_rewards', [2.0])
        fee_rates = config.get('fee_rates', [0.001])
        
        combinations = []
        
        for pair in pairs:
            for timeframe in timeframes:
                for year in years:
                    for sl in stop_losses:
                        for rr in risk_rewards:
                            for fee in fee_rates:
                                combinations.append({
                                    'pair': pair,
                                    'timeframe': timeframe,
                                    'year': year,
                                    'months': months,
                                    'stop_loss': sl,
                                    'take_profit_multiplier': rr,
                                    'fee_rate': fee,
                                    'initial_balance': config.get('initial_balance', 10000),
                                    'risk_per_trade_pct': config.get('risk_per_trade_pct', 1.0),
                                    'backtest_run_id': run_id
                                })
        
        return combinations

def _execute_worker_task(worker_id: int, config: Dict[str, Any]) -> Dict[str, Any]:
    """Helper function for ProcessPoolExecutor"""
    worker = OptimizerWorker(f"W{worker_id}")
    return worker.execute(config)


================================================================================
# FILE: src/agents/orchestrator.py
================================================================================

# src/agents/orchestrator.py
from typing import Optional, Dict, Any, List, Tuple
from src.core.regime import MarketRegime
from src.core.market import MarketState
from src.core.classifier import classify_regime
from src.execution.executor import TradeSignal

# Import Agents
from src.agents.trend_hunter import TrendHunterAgent
from src.agents.mean_reversion import MeanReversionAgent
from src.agents.volatility_filter import VolatilityFilterAgent
from src.agents.breakout_hunter import BreakoutHunterAgent
from src.agents.sentiment_scout import SentimentScoutAgent

# Import Alphas
from src.alphas.base import Alpha
from src.alphas.combiner import AlphaCombiner
from src.alphas.ob_quality import Alpha_OB_Quality
from src.alphas.momentum import Alpha_Momentum
from src.alphas.volatility import Alpha_Volatility
from src.alphas.liquidity import Alpha_Liquidity
from src.alphas.ml_confidence import Alpha_ML_Confidence

class MSCOrchestrator:
    """
    Multi-Strategy Coordinator (MSC) Orchestrator (Layer 1).
    Selects the "Single Winner" specialized agent based on the market regime.
    Now also capable of Weighted Alpha Blending when parameters are provided (WFO).
    """
    
    def __init__(self):
        # 1. Map regimes to specialized agents
        self.agents = {
            MarketRegime.TRENDING_BULLISH: TrendHunterAgent(),
            MarketRegime.TRENDING_BEARISH: TrendHunterAgent(),
            MarketRegime.SIDEWAYS_RANGE: MeanReversionAgent(),
            MarketRegime.HIGH_VOLATILITY: VolatilityFilterAgent(),
            MarketRegime.BREAKOUT_PENDING: BreakoutHunterAgent(),
            MarketRegime.NEWS_DRIVEN: SentimentScoutAgent()
        }
        
        # 2. Initialize Alphas for WFO/Blending Mode
        self.alpha_ob = Alpha_OB_Quality()
        self.alpha_mom = Alpha_Momentum()
        self.alpha_vol = Alpha_Volatility()
        self.alpha_liq = Alpha_Liquidity()
        self.alpha_ml = Alpha_ML_Confidence() 
        
    def decide(self, market_state: MarketState, params: Dict[str, Any] = None) -> Optional[TradeSignal]:
        """
        Main decision method.
        If 'params' are provided (WFO), uses Weighted Alpha Logic.
        Otherwise, uses standard Regime Switching logic.
        """
        # 1. Classify with Params
        regime = classify_regime(market_state, params)
        
        if params:
            # --- WFO Dynamic Alpha Logic ---
            
            # Base Weights (Standard Portfolio)
            w_ob = 1.5 * params.get('alpha_ob_weight_mult', 1.0)
            w_mom = 2.0 * params.get('alpha_mom_weight_mult', 1.0)
            w_vol = 0.5 * params.get('alpha_vol_weight_mult', 1.0)
            w_liq = 0.8 * params.get('alpha_liq_weight_mult', 1.0)
            
            # Construct Weighted List
            alphas_list: List[Tuple[Alpha, float]] = [
                (self.alpha_ob, w_ob),
                (self.alpha_mom, w_mom),
                (self.alpha_vol, w_vol),
                (self.alpha_liq, w_liq)
            ]
            
            # Note: We create a light combiner just for the signal calculation
            combiner = AlphaCombiner(alphas_list)
            threshold = params.get('alpha_threshold', 0.6)
            
            signal = combiner.get_signal(market_state, threshold)
            
            # Inject Metadata
            if signal:
                object.__setattr__(signal, 'metadata', {
                    'agent': 'WFO_Alpha_Combiner',
                    'regime': regime.value,
                    'timestamp': getattr(market_state, 'timestamp', 0),
                    'params_hash': str(hash(str(params)))[:8]
                })
                
            return signal
            
        else:
            # --- Legacy / Standard Switching Logic ---
            return self.get_signal(market_state, regime_override=regime)
    
    def get_signal(self, market_state: MarketState, regime_override: Optional[MarketRegime] = None) -> Optional[TradeSignal]:
        """
        Coordinates the brain's decision.
        1. Classifies regime.
        2. Delegates to the specialist.
        3. Injects metadata for audit.
        """
        # 1. Classify Regime (The Brain)
        regime = regime_override if regime_override else classify_regime(market_state)
        
        # 2. Select Agent (The specialist)
        agent = self.agents.get(regime)
        if not agent:
            # Fallback to Sentiment Scout (Balanced) if for some reason regime is not mapped
            agent = self.agents[MarketRegime.NEWS_DRIVEN]
            
        # 3. Generate Signal
        signal = agent.generate_signal(market_state)
        
        # 4. Inject Metadata for audit (Layer 1 requirement)
        if signal:
            object.__setattr__(signal, 'metadata', {
                'agent': agent.__class__.__name__,
                'regime': regime.value,
                'timestamp': getattr(market_state, 'timestamp', 0)
            })
            
        return signal

# Backward-compatible alias
OrchestratorAgent = MSCOrchestrator


================================================================================
# FILE: src/agents/pattern_detective.py
================================================================================

# src/agents/pattern_detective.py
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, _tree
from decimal import Decimal
import uuid

from src.agents.base import BaseAgent
from src.database import get_db_session
from src.database.repository import TradeRepository, PatternRepository

class PatternDetective(BaseAgent):
    """
    Agente que analiza el historial de trades para detectar patrones correlacionados con el fracaso.
    """
    
    def __init__(self):
        super().__init__("PatternDetective")
        
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analizar trades y salvar patrones detectados.
        
        Args:
            config: {
                'backtest_run_id': uuid (opcional),
                'min_sample_size': 20,
                'max_win_rate_threshold': 35.0 # Solo nos interesan patrones que pierdan mucho
            }
        """
        run_id = config.get('backtest_run_id')
        min_samples = config.get('min_sample_size', 20)
        threshold = config.get('max_win_rate_threshold', 35.0)
        
        self.log('INFO', f"Iniciando detecci√≥n de patrones peligrosos (threshold < {threshold}%)")
        
        data = []
        trade_pairs = set()
        trade_tfs = set()
        
        with get_db_session() as db:
            if run_id:
                trades_objs = TradeRepository.get_by_backtest_run(db, uuid.UUID(str(run_id)))
            else:
                from src.database.models import Trade
                trades_objs = db.query(Trade).limit(5000).all()
            
            if len(trades_objs) < min_samples:
                self.log('WARNING', f"Muestra muy peque√±a ({len(trades_objs)}). Abortando.")
                return {'patterns_found': 0, 'reason': 'insufficient_data'}

            for t in trades_objs:
                features = t.market_state.copy()
                features['target'] = 1 if t.result == 'WIN' else 0
                data.append(features)
                trade_pairs.add(str(t.pair))
                trade_tfs.add(str(t.timeframe))
            
        # 2. Preparar Dataframe para ML
        df = pd.DataFrame(data)
        
        # Filtrar solo columnas num√©ricas para el √°rbol b√°sica
        X = df.drop(columns=['target', 'trend', 'error'], errors='ignore').select_dtypes(include=[np.number])
        y = df['target']
        
        if X.empty:
            self.log('ERROR', "No hay features num√©ricas en market_state para analizar.")
            return {'patterns_found': 0, 'reason': 'no_numeric_features'}

        # 3. Entrenar √Årbol de Decisi√≥n para reglas simples
        clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=min_samples)
        clf.fit(X, y)
        
        # 4. Extraer reglas del √°rbol
        patterns = self._extract_rules(clf, X.columns, X, y, threshold)
        
        # 5. Guardar en DB
        saved_count = 0
        with get_db_session() as db:
            for p in patterns:
                pattern_data = {
                    'pattern_type': 'market_condition_risk',
                    'description': f"Condiciones detectadas con win rate de {p['win_rate']}%",
                    'conditions': p['conditions'],
                    'win_rate': Decimal(str(p['win_rate'])),
                    'sample_size': p['sample_size'],
                    'confidence_score': Decimal("0.8"), # Constante para este agente b√°sico
                    'applicable_pairs': list(trade_pairs),
                    'applicable_timeframes': list(trade_tfs),
                    'is_active': True
                }
                PatternRepository.create(db, pattern_data)
                saved_count += 1
                
        self.log('INFO', f"Detective finalizado. Se encontraron y guardaron {saved_count} patrones.")
        
        return {
            'patterns_found': saved_count,
            'total_trades_analyzed': len(data)
        }

    def _extract_rules(self, clf, feature_names, X, y, threshold):
        """Extrae reglas de las hojas del √°rbol que cumplen el threshold de perdidas."""
        tree_ = clf.tree_
        feature_name = [
            feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
            for i in tree_.feature
        ]

        patterns = []

        def recurse(node, depth, current_conditions):
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_name[node]
                threshold_val = tree_.threshold[node]
                
                # Rama Izquierda ( <= threshold )
                left_cond = current_conditions.copy()
                left_cond.append(f"{name} <= {threshold_val:.4f}")
                recurse(tree_.children_left[node], depth + 1, left_cond)
                
                # Rama Derecha ( > threshold )
                right_cond = current_conditions.copy()
                right_cond.append(f"{name} > {threshold_val:.4f}")
                recurse(tree_.children_right[node], depth + 1, right_cond)
            else:
                # Estamos en una hoja. Calculamos win rate.
                node_samples = tree_.n_node_samples[node]
                values = tree_.value[node][0] # [[losses, wins]]
                wins = values[1]
                win_rate = (wins / node_samples) * 100
                
                if win_rate <= threshold:
                    patterns.append({
                        'conditions': current_conditions,
                        'win_rate': round(win_rate, 2),
                        'sample_size': int(node_samples)
                    })

        recurse(0, 1, [])
        return patterns


================================================================================
# FILE: src/agents/sentiment_scout.py
================================================================================

# src/agents/sentiment_scout.py
from src.agents.trading_agent import TradingAgent
from src.core.regime import MarketRegime

class SentimentScoutAgent(TradingAgent):
    """
    Agent specialized in news-driven or sentiment-based regimes.
    Currently acts as a balanced baseline.
    """
    
    def __init__(self):
        super().__init__(
            alpha_weights={
                'ob_quality': 1.0,
                'momentum': 1.0,
                'volatility': 1.0,
                'ml_confidence': 1.0,
                'liquidity': 1.0
            },
            active_regimes=[
                MarketRegime.NEWS_DRIVEN
            ]
        )


================================================================================
# FILE: src/agents/strategy_mutator.py
================================================================================

# src/agents/strategy_mutator.py
from typing import Dict, Any, List, Optional
import uuid
import random

from src.agents.base import BaseAgent
from src.database import get_db_session
from src.database.repository import StrategyRepository, PatternRepository

class StrategyMutator(BaseAgent):
    """
    Agente que crea variaciones de estrategias basadas en patrones ML detectados.
    """
    
    def __init__(self):
        super().__init__("StrategyMutator")
        
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Crear mutaciones de una estrategia base.
        
        Args:
            config: {
                'base_strategy_name': str,
                'num_mutations': int,
                'apply_ml_filters': bool
            }
        """
        base_name = config.get('base_strategy_name', 'TJR_Base')
        num_mutations = config.get('num_mutations', 3)
        use_ml = config.get('apply_ml_filters', True)
        
        self.log('INFO', f"Iniciando mutaci√≥n de {base_name}")
        
        # 1. Obtener patrones activos si se requiere ML
        active_patterns_data = []
        if use_ml:
            with get_db_session() as db:
                patterns = PatternRepository.get_active_patterns(db)
                for p in patterns:
                    active_patterns_data.append({
                        'description': str(p.description),
                        'conditions': p.conditions
                    })
            self.log('INFO', f"Encontrados {len(active_patterns_data)} patrones ML para aplicar.")

        # 2. Generar mutaciones
        mutations_created = []
        
        for i in range(num_mutations):
            new_params = self._mutate_params(config.get('initial_params', {}))
            new_filters = {}
            
            if use_ml and active_patterns_data:
                # Seleccionar un patr√≥n aleatorio para mitigar o todos
                pattern = random.choice(active_patterns_data)
                new_filters['ml_avoidance'] = {
                    'description': pattern['description'],
                    'conditions': pattern['conditions']
                }
            
            mutation_name = f"{base_name}_mut_{uuid.uuid4().hex[:6]}"
            
            strategy_data = {
                'name': mutation_name,
                'description': f"Mutaci√≥n de {base_name} con {'filtros ML' if new_filters else 'par√°metros variados'}",
                'base_strategy': base_name,
                'parameters': new_params,
                'filters': new_filters,
                'status': 'TESTING'
            }
            
            with get_db_session() as db:
                StrategyRepository.create(db, strategy_data)
                
            mutations_created.append(mutation_name)
            self.log('INFO', f"Creada mutaci√≥n: {mutation_name}")
            
        return {
            'base_strategy': base_name,
            'mutations_created': mutations_created,
            'count': len(mutations_created)
        }
        
    def _mutate_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Aplica variaciones aleatorias a los par√°metros base."""
        mutated = params.copy()
        
        # Ejemplo: variamos el SL o el RR ligeramente
        if 'stop_loss' in mutated:
            variation = random.uniform(0.9, 1.1)
            mutated['stop_loss'] = int(float(mutated['stop_loss']) * variation)
            
        if 'take_profit_multiplier' in mutated:
            variation = random.uniform(0.9, 1.1)
            mutated['take_profit_multiplier'] = round(float(mutated['take_profit_multiplier']) * variation, 2)
            
        return mutated


================================================================================
# FILE: src/agents/trading_agent.py
================================================================================

# src/agents/trading_agent.py
from abc import ABC
from typing import List, Dict, Optional
from src.alphas.combiner import AlphaCombiner
from src.alphas.ob_quality import Alpha_OB_Quality
from src.alphas.momentum import Alpha_Momentum
from src.alphas.volatility import Alpha_Volatility
from src.alphas.ml_confidence import Alpha_ML_Confidence
from src.alphas.liquidity import Alpha_Liquidity
from src.core.regime import MarketRegime
from src.core.market import MarketState
from src.execution.executor import TradeSignal

class TradingAgent(ABC):
    """
    Base class for specialized trading agents in the MSC Layer 2.
    Each agent has a specific Alpha personality (weights) and target regimes.
    """
    
    def __init__(self, alpha_weights: Dict[str, float], active_regimes: List[MarketRegime]):
        """
        Initialize the agent with its alpha weights and activation regimes.
        """
        alphas = [
            (Alpha_OB_Quality(), alpha_weights.get('ob_quality', 1.0)),
            (Alpha_Momentum(), alpha_weights.get('momentum', 1.0)),
            (Alpha_Volatility(), alpha_weights.get('volatility', 1.0)),
            (Alpha_ML_Confidence(), alpha_weights.get('ml_confidence', 1.0)),
            (Alpha_Liquidity(), alpha_weights.get('liquidity', 1.0))
        ]
        self.combiner = AlphaCombiner(alphas)
        self.active_regimes = active_regimes

    def should_activate(self, regime: MarketRegime) -> bool:
        """Determines if the agent should operate in the current market regime."""
        return regime in self.active_regimes

    def generate_signal(self, market_state: MarketState) -> Optional[TradeSignal]:
        """Generates a trading signal using the agent's unique alpha combination."""
        return self.combiner.get_signal(market_state, threshold=0.6)


================================================================================
# FILE: src/agents/trend_hunter.py
================================================================================

# src/agents/trend_hunter.py
from src.agents.trading_agent import TradingAgent
from src.core.regime import MarketRegime

class TrendHunterAgent(TradingAgent):
    """
    Agent specialized in following strong trends.
    Bias: High Momentum and OB Quality.
    """
    
    def __init__(self):
        super().__init__(
            alpha_weights={
                'momentum': 3.0,
                'ob_quality': 2.0,
                'volatility': 1.0,
                'ml_confidence': 1.0,
                'liquidity': 0.8
            },
            active_regimes=[
                MarketRegime.TRENDING_BULLISH,
                MarketRegime.TRENDING_BEARISH
            ]
        )


================================================================================
# FILE: src/agents/types.py
================================================================================

# src/agents/types.py
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, Any
from datetime import datetime
import uuid

class AgentStatus(Enum):
    """Estados posibles de un agente"""
    IDLE = "idle"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class AgentMessage:
    """Mensaje entre agentes"""
    id: uuid.UUID
    from_agent: str
    to_agent: str
    message_type: str
    payload: Dict[str, Any]
    timestamp: datetime
    
    @classmethod
    def create(cls, from_agent: str, to_agent: str, message_type: str, payload: Dict[str, Any]):
        return cls(
            id=uuid.uuid4(),
            from_agent=from_agent,
            to_agent=to_agent,
            message_type=message_type,
            payload=payload,
            timestamp=datetime.utcnow()
        )

@dataclass
class AgentProgress:
    """Progreso de un agente"""
    current: int
    total: int
    percentage: float
    status: AgentStatus
    message: str
    
    @classmethod
    def create(cls, current: int, total: int, status: AgentStatus, message: str):
        percentage = (current / total * 100) if total > 0 else 0
        return cls(
            current=current,
            total=total,
            percentage=round(percentage, 2),
            status=status,
            message=message
        )


================================================================================
# FILE: src/agents/validator.py
================================================================================

from typing import Dict, Any, List, Optional
import uuid
from datetime import datetime

from src.agents.base import BaseAgent
from src.agents.worker import OptimizerWorker
from src.database import get_db_session
from src.database.repository import StrategyRepository, TradeRepository
from src.portfolio.correlation import calculate_correlation

class ValidatorAgent(BaseAgent):
    """
    Agente que valida estrategias en status TESTING usando el OptimizerWorker.
    Si pasan los criterios, las aprueba para producci√≥n.
    """
    
    def __init__(self):
        super().__init__("ValidatorAgent")
        
    def run(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validar estrategias pendientes.
        
        Args:
            config: {
                'validation_period': {'year': 2024, 'months': [10, 11, 12]},
                'criteria': {'min_profit_factor': 1.2, 'min_win_rate': 45.0}
            }
        """
        # Support extended config for WFA
        val_periods = config.get('validation_periods', [])
        if not val_periods:
            val_periods = [config.get('validation_period', {'year': 2024, 'months': [11]})]
            
        criteria = config.get('criteria', {'min_profit_factor': 1.1, 'min_win_rate': 40.0})
        
        self.log('INFO', f"Validaci√≥n: {len(val_periods)} periodos configurados.")
        
        # 1. Obtener estrategias TESTING
        pending_data = []
        with get_db_session() as db:
            pending_objs = StrategyRepository.get_by_status(db, 'TESTING')
            for obj in pending_objs:
                pending_data.append({
                    'strategy_id': obj.strategy_id,
                    'name': str(obj.name),
                    'parameters': obj.parameters.copy() if obj.parameters else {}
                })
            
        self.log('INFO', f"Encontradas {len(pending_data)} estrategias para validar.")
        
        results = []
        approved_count = 0
        
        for strategy in pending_data:
            # WFA: Strategy must pass ALL periods (Consistency)
            passed_all = True
            period_results = []
            
            for period in val_periods:
                worker_config = {
                    'pair': 'BTCUSDT', 
                    'timeframe': '4h',
                    'year': period['year'],
                    'months': period['months'],
                    'stop_loss': strategy['parameters'].get('stop_loss', 2000),
                    'take_profit_multiplier': strategy['parameters'].get('take_profit_multiplier', 2.0),
                    'fee_rate': 0.001,
                    'initial_balance': 10000,
                    'risk_per_trade_pct': 1.0,
                    'backtest_run_id': uuid.uuid4(),
                    'use_ml_model': config.get('use_ml_model', True),
                    'ml_prob_threshold': config.get('ml_prob_threshold', 0.6),
                    'strategy_version': strategy['name']  # Pass name for tracking
                }
                
                worker = OptimizerWorker(f"Validator_{strategy['name']}")
                try:
                    res = worker.execute(worker_config)
                    pf = res.get('profit_factor', 0)
                    wr = res.get('win_rate', 0)
                    
                    is_valid = pf >= criteria['min_profit_factor'] and wr >= criteria['min_win_rate']
                    period_results.append({'period': period, 'pf': pf, 'wr': wr, 'pass': is_valid})
                    
                    if not is_valid:
                        passed_all = False
                        break # Fail fast
                except Exception as e:
                    self.log('ERROR', f"Fallo validaci√≥n {strategy['name']}: {e}")
                    passed_all = False
                    break
            
            status_update = {
                'name': strategy['name'],
                'passed': passed_all,
                'details': period_results
            }
            
            with get_db_session() as db:
                if passed_all:
                    # Pure Alpha Check: Correlation
                    is_correlated = False
                    candidate_trades = TradeRepository.get_by_strategy_version(db, strategy['name'])
                    
                    approved_strategies = StrategyRepository.get_approved(db, limit=100)
                    for existing in approved_strategies:
                        if existing.name == strategy['name']: continue # Skip self
                        
                        existing_trades = TradeRepository.get_by_strategy_version(db, existing.name)
                        if not existing_trades: continue
                            
                        corr = calculate_correlation(candidate_trades, existing_trades)
                        # Bridgewater rule: > 0.3 is too correlated
                        if corr > 0.3:
                            self.log('WARNING', f"‚ùå RECHAZADA por Correlaci√≥n: {strategy['name']} vs {existing.name} (Corr: {corr:.2f})")
                            is_correlated = True
                            break
                    
                    if not is_correlated:
                        avg_pf = sum(d['pf'] for d in period_results) / len(period_results)
                        avg_wr = sum(d['wr'] for d in period_results) / len(period_results)
                        
                        StrategyRepository.approve(db, strategy['strategy_id'], avg_pf, avg_wr)
                        self.log('INFO', f"‚úÖ APROBADA (Robust & Unique): {strategy['name']} (Avg PF: {avg_pf:.2f})")
                        approved_count += 1
                else:
                    self.log('WARNING', f"‚ùå RECHAZADA: {strategy['name']}")
            
            results.append(status_update)
            
        return {
            'total_validated': len(pending_data),
            'approved_count': approved_count,
            'results': results
        }


================================================================================
# FILE: src/agents/volatility_filter.py
================================================================================

# src/agents/volatility_filter.py
from src.agents.trading_agent import TradingAgent
from src.core.regime import MarketRegime

class VolatilityFilterAgent(TradingAgent):
    """
    Agent specialized in high volatility conditions.
    Bias: High Volatility alpha weight.
    """
    
    def __init__(self):
        super().__init__(
            alpha_weights={
                'ob_quality': 0.5,
                'momentum': 0.5,
                'volatility': 4.0,
                'ml_confidence': 0.5,
                'liquidity': 0.5
            },
            active_regimes=[
                MarketRegime.HIGH_VOLATILITY
            ]
        )


================================================================================
# FILE: src/agents/worker.py
================================================================================

# src/agents/worker.py
from typing import Dict, Any, List, Optional
from pathlib import Path
from decimal import Decimal
import uuid
from datetime import datetime
import statistics
import pandas as pd
import numpy as np
import math

from src.agents.base import BaseAgent
from src.database import get_db_session
from src.database.repository import TradeRepository, BacktestRunRepository
from src.utils.data_loader import load_binance_csv
from src.strategy.engine import TJRStrategy
from src.simulation.broker import InMemoryBroker
from src.execution.risk import RiskManager
from src.execution.executor import TradeExecutor
from src.core.market import MarketState
from src.core.timeframe import Timeframe
from src.ml.features import FeatureExtractor
from src.ml.analyzer import PatternAnalyzer

# Alphas
from src.alphas.ob_quality import Alpha_OB_Quality
from src.alphas.momentum import Alpha_Momentum
from src.alphas.volatility import Alpha_Volatility
from src.alphas.ml_confidence import Alpha_ML_Confidence
from src.alphas.liquidity import Alpha_Liquidity
from src.alphas.combiner import AlphaCombiner
from src.agents.orchestrator import MSCOrchestrator

class OptimizerWorker(BaseAgent):
    """
    Worker individual que corre backtests para un par/timeframe
    
    Guarda cada trade en DB con market_state completo
    """
    
    def __init__(self, worker_id: str):
        super().__init__(f"Worker-{worker_id}")
        self.worker_id = worker_id
        self.feature_extractor = FeatureExtractor()
        self._combiner: Optional[AlphaCombiner] = None
        self.db_failures = 0
        self._cached_features_map: Optional[pd.DataFrame] = None
        self._cached_candles_id: Optional[int] = None

    
    def run(self, config: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        Ejecutar backtest y guardar trades en DB
        
        Args:
            config: Base configuration
            **kwargs: Overrides/Additional params (params, warmup_data, initial_balance, etc.)
        """
        # Merge kwargs into config (kwargs take precedence)
        config = {**config, **kwargs}
        
        pair = config.get('pair', 'BTCUSDT') # Default for safety
        timeframe_str = config.get('timeframe', '4h')

        timeframe = Timeframe(timeframe_str)
        year = config['year']
        months = config.get('months', list(range(1, 13)))
        backtest_run_id = config['backtest_run_id']
        
        # ML Config
        use_ml = config.get('use_ml_model', False)
        ml_model_path = config.get('ml_model_path', 'data/models/rf_model_v1.pkl')
        ml_threshold = config.get('ml_prob_threshold', 0.6)
        
        analyzer = None
        if use_ml:
            analyzer = PatternAnalyzer(ml_model_path)
            if not analyzer.is_trained:
                self.log('WARNING', "ML Model requested but no trained model found. Running without ML.")
                analyzer = None # Fallback
            else:
                self.log('INFO', f"Running with ML Filter (Threshold: {ml_threshold})")
        
        
        # Extract WFO Params (Optimization)
        # Using config.get('params') which comes from GA
        wfo_params = config.get('params')
        if wfo_params is None:
            wfo_params = {}
            # self.log('DEBUG', "No WFO params provided, using defaults")
        
        # Alpha Engine Setup (Task 6.9)
        use_alpha_engine = config.get('use_alpha_engine', False)
        if use_alpha_engine:
            self.log('INFO', "Alpha Engine active for signal generation")
            
            # Map WFO params to Alpha Weights
            # Defaults match the original hardcoded values if not optimizing
            w_ob = wfo_params.get('g_ob_quality', 1.5)
            w_mom = wfo_params.get('g_momentum', 2.0)
            w_vol = wfo_params.get('g_volatility', 0.5)
            w_ml = wfo_params.get('g_ml_confidence', 1.0)
            w_liq = wfo_params.get('g_liquidity', 0.8)
            
            # Iniciar alphas con los pesos definidos por el usuario / GA
            self._combiner = AlphaCombiner([
                (Alpha_OB_Quality(), w_ob),
                (Alpha_Momentum(), w_mom),
                (Alpha_Volatility(), w_vol),
                (Alpha_ML_Confidence(analyzer=analyzer), w_ml),
                (Alpha_Liquidity(), w_liq)
            ])
        
        # MSC Orchestrator Setup (Task 8.5)
        use_msc = config.get('use_msc', False)
        orchestrator = None
        if use_msc:
            self.log('INFO', "MSC Orchestrator active (Layer 1 Brain)")
            # Pass optimized thresholds if available
            # Note: MSCOrchestrator might need to be updated to accept these, 
            # or we set them after init. For now, we just init it.
            orchestrator = MSCOrchestrator()
        
        self.log('INFO', f"Starting backtest for {pair} {timeframe_str} {year}")
        
        # Load data (Memory vs Disk)
        candles_arg = config.get('candles')
        warmup_candles_arg = config.get('warmup_candles', [])
        
        if candles_arg:
            # In-memory execution (WFO/Optimization)
            # Combine warmup + main data for feature calculation
            all_candles = warmup_candles_arg + candles_arg
            # Trading starts after warmup
            start_index = len(warmup_candles_arg)
            self.log('INFO', f"Using in-memory data: {len(candles_arg)} main + {len(warmup_candles_arg)} warmup candles")
        else:
            # Disk loading (Legacy/Manual run)
            all_candles = []
            data_dir = Path("data/raw")
            
            for month in months:
                filename = f"{pair}-{timeframe_str}-{year}-{month:02d}.csv"
                filepath = data_dir / filename
                
                if not filepath.exists():
                    self.log('WARNING', f"File not found: {filename}")
                    continue
                
                try:
                    candles = load_binance_csv(str(filepath), timeframe)
                    all_candles.extend(candles)
                except Exception as e:
                    self.log('ERROR', f"Failed to load {filename}: {str(e)}")
            
            if not all_candles:
                raise ValueError(f"No data found for {pair} {timeframe_str} {year}")
                
            # Legacy: warmup_data might be passed but not prepended to file data
            # Use strict approach: if loaded from disk, start_index=0 (unless warmup logic added later)
            start_index = 0
            
            # Legacy warmup handling (kept for compatibility if needed, but discouraged)
            # If warmup_data passed in kwargs but loading from disk, we prepend it?
            # For now, keep simple: disk loading = 0 offset.
        
        total_candles = len(all_candles)
        self.log('INFO', f"Total candles to process: {total_candles} (Start trading at index {start_index})")
        
        # Pre-compute features for performance if ML is needed or for reporting
        # Convert candles to DataFrame for feature extraction
        df_candles = pd.DataFrame([{
            'timestamp': c.timestamp,
            'open': float(c.open),
            'high': float(c.high),
            'low': float(c.low),
            'close': float(c.close),
            'volume': float(c.volume)
        } for c in all_candles])
        
        # Check timestamps
        df_candles['timestamp'] = pd.to_datetime(df_candles['timestamp'], unit='ms')
        
        # Optimization for Mac M1: Cache features if candles haven't changed (Task 8.9)
        current_candles_id = id(all_candles)
        if self._cached_candles_id == current_candles_id and self._cached_features_map is not None:
            features_map = self._cached_features_map
        else:
            # Calculate features (Expensive operation)
            self.log('INFO', f"Calculating features for {len(all_candles)} candles...")
            df_features = self.feature_extractor.add_all_features(df_candles)
            features_map = df_features.set_index('timestamp')
            # Store in cache
            self._cached_features_map = features_map
            self._cached_candles_id = current_candles_id
        
        # Initialize trading components
        # Strategy Params
        # Allow WFO params to override config defaults
        stop_loss_param = config.get('stop_loss', 100)
        # If optimization param 'stop_loss_atr_mult' exists, we might pass it.
        # TJRStrategy currently takes fixed SL. We need to update TJRStrategy 
        # to support ATR multiplier if we want to optimize it.
        # For now, we create it assuming TJRStrategy will support it or we pass it via **kwargs if flexible?
        # Let's assume we update TJRStrategy signature.
        
        str_stop_loss = wfo_params.get('fixed_stop_loss', config.get('stop_loss')) # Only if optimizing fixed
        # But param_space uses 'stop_loss_atr_mult'. 
        atr_mult_sl = wfo_params.get('stop_loss_atr_mult', None)
        
        take_profit_mult = wfo_params.get('take_profit_r_mult', config.get('take_profit_multiplier', 2.0))
        
        strategy = TJRStrategy(
            fixed_stop_loss=Decimal(str(str_stop_loss)) if str_stop_loss else None,
            take_profit_multiplier=Decimal(str(take_profit_mult)),
            stop_loss_atr_multiplier=Decimal(str(atr_mult_sl)) if atr_mult_sl else None
        )
        
        broker = InMemoryBroker(
            balance=Decimal(str(config.get('initial_balance', config['initial_balance']))),
            fee_rate=Decimal(str(config['fee_rate']))
        )
        
        from src.execution.risk import RiskConfig # Import needed inside or at top
        
        # Risk Params
        risk_pct = wfo_params.get('risk_per_trade_pct', config.get('risk_per_trade_pct', 1.0))
        max_portfolio_risk = config.get('max_portfolio_risk', None)
        use_dd_scaling = config.get('use_dd_scaling', False)
        
        from src.execution.risk import RiskConfig  # Ensure import is available

        risk_manager = RiskManager(
            config=RiskConfig(
                risk_percentage=Decimal(str(risk_pct)) / 100,
                max_portfolio_risk=Decimal(str(max_portfolio_risk)) if max_portfolio_risk else None,
                use_dd_scaling=use_dd_scaling
            )
        )
        
        executor = TradeExecutor(
            broker=broker,
            risk_manager=risk_manager,
            strategy=strategy
        )
        
        market = MarketState.empty(pair)
        
        # --- Warmup Phase (Integrated) ---
        # No explicit separate loop needed as features are calc on all_candles
        # Just ensure market state is updated for warmup part
        # Logic below handles it via 'start_index' skipping trade logic
                    
        trades_saved = 0
        filtered_trades = 0
        pending_trades = []
        
        
        # Extract WFO params if present (redundant reassignment but keeping for clarity if used later)
        # wfo_params extracted at top of run()
        
        for i, candle in enumerate(all_candles):
            # Update market state
            market = market.update(candle)
            
            # Logic manual para poder interceptar la se√±al
            if hasattr(broker, 'update_positions'):
                broker.update_positions(candle.close)
            
            # SKIP TRADING during Warmup
            if i < start_index:
                continue

            if not broker.get_positions():
                # Bifurcation (Task 8.5)
                signal = None
                if use_msc and orchestrator:
                    # Use decide() with params
                    signal = orchestrator.decide(market, params=wfo_params)
                elif use_alpha_engine and self._combiner:
                    signal = self._combiner.get_signal(market, threshold=config.get('alpha_threshold', 0.6))
                else:
                    signal = strategy.analyze(market, timeframe)
                
                # Check for signal validity and normalize for execution
                if signal:
                    # In case of MSC/Alpha, we might have 0.0 placeholders or need Decimal conversion
                    # For signals coming from MSC/Alpha, we overrideSL/TP/Price with current candle data
                    # unless it's already properly set (Compatibility layer)
                    if hasattr(signal, 'metadata') or signal.entry_price == 0.0:
                        from src.execution.executor import TradeSignal as TS
                        
                        entry_price = candle.close
                        
                        # --- Dynamic SL/TP Calculation (WFO) ---
                        if wfo_params:
                            atr_mult = float(wfo_params.get('stop_loss_atr_mult', 1.5))
                            tp_r_mult = float(wfo_params.get('take_profit_r_mult', 2.0))
                            
                            # Get ATR
                            current_atr = market.atr[-1] if market.atr else 0.0
                            if current_atr == 0.0:
                                 # Fallback if ATR not ready
                                 sl_dist = Decimal(str(config['stop_loss']))
                            else:
                                 sl_dist = Decimal(str(current_atr)) * Decimal(str(atr_mult))
                                 
                            tp_dist = sl_dist * Decimal(str(tp_r_mult))
                            
                        else:
                            # Legacy Fixed Logic
                            sl_dist = Decimal(str(config['stop_loss']))
                            tp_dist = sl_dist * Decimal(str(config['take_profit_multiplier']))
                        
                        if signal.side.value == 'BUY':
                            stop_loss = entry_price - sl_dist
                            take_profit = entry_price + tp_dist
                        else:
                            stop_loss = entry_price + sl_dist
                            take_profit = entry_price - tp_dist
                            
                        # Create actual execution signal based on current candle price
                        # Note: We keep the same side and confidence, but provide real prices
                        signal = TS(
                            symbol=signal.symbol,
                            side=signal.side,
                            entry_price=entry_price,
                            stop_loss=stop_loss,
                            take_profit=take_profit,
                            confidence=signal.confidence,
                            metadata=getattr(signal, 'metadata', None)
                        )
                        
                        is_allowed = True
                        prob = 0.0
                        
                        if analyzer:
                            # Buscamos features para esta vela
                            ts_key = pd.to_datetime(candle.timestamp, unit='ms')
                            if ts_key in features_map.index:
                                feats = features_map.loc[[ts_key]]
                                prob = analyzer.predict_proba(feats)
                                
                                if prob < ml_threshold:
                                    is_allowed = False
                                    filtered_trades += 1
                                    # self.log('DEBUG', f"Trade blocked by ML: Prob {prob:.2f} < {ml_threshold}")
                            else:
                                # Si no hay features (e.g. primeras velas), default allow or block?
                                # Default allow suele ser mejor para evitar 0 data, pero si falta feature es riesgo.
                                pass
                                
                        if is_allowed:
                            executor.execute_trade(signal)
                
                # Check for completed trades
                closed_positions = broker.get_closed_positions()
                
                # Batch Persistence Logic
                new_closed_count = len(closed_positions)
                if new_closed_count > trades_saved:
                    # Accumulate pending writes
                    for position in closed_positions[trades_saved:]:
                        # Usar features pre-calculadas si existen
                        ts_key = pd.to_datetime(candle.timestamp, unit='ms')
                        market_features = {}
                        
                        if ts_key in features_map.index:
                            # Convertir panda Series a dict
                            market_features_series = features_map.loc[ts_key]
                            # Handle duplicate timestamps if any (robustness)
                            if isinstance(market_features_series, pd.DataFrame):
                                market_features_series = market_features_series.iloc[0]
                            market_features = market_features_series.to_dict()
                            # Sanitize types
                            market_features = {k: float(v) if isinstance(v, (np.float32, np.float64)) else v for k,v in market_features.items()}
                        else:
                            market_features = self._extract_market_state(candle, all_candles[max(0, i-50):i])
                            
                        trade_record = {
                            'timestamp': datetime.fromtimestamp(candle.timestamp / 1000),
                            'pair': pair,
                            'symbol': pair,
                            'timeframe': timeframe_str,
                            'side': 'LONG' if position.side.value == 'BUY' else 'SHORT',
                            'entry_price': position.entry_price,
                            'exit_price': position.exit_price,
                            'stop_loss': position.stop_loss,
                            'take_profit': position.take_profit,
                            'result': 'WIN' if position.pnl > 0 else 'LOSS',
                            'profit_loss': position.pnl,
                            'profit_loss_pct': (position.pnl / (position.entry_price * position.quantity) * 100) if position.entry_price > 0 else 0,
                            'risk_reward': float(config['take_profit_multiplier']),
                            'market_state': market_features,
                            'strategy_version': config.get('strategy_version', 'MSC_v1_Worker' if use_msc else 'TJR_ML_v3_Worker'),
                            'backtest_run_id': backtest_run_id,
                            'worker_id': self.worker_id,
                            # Layer 1 Metrics (Task 8.5)
                            'agent_name': position.metadata.get('agent', 'Legacy') if hasattr(position, 'metadata') and position.metadata else 'Legacy',
                            'market_regime': position.metadata.get('regime', 'Unknown') if hasattr(position, 'metadata') and position.metadata else 'Unknown'
                        }
                        pending_trades.append(trade_record)
                    
                    trades_saved = new_closed_count
                
                # Flush batch every 50 trades or if many pending
                if len(pending_trades) >= 50:
                    self._flush_trades(pending_trades)
                    pending_trades = []
                
            # Update progress every 1000 candles
            if i % 1000 == 0:
                self.update_progress(
                    i,
                    total_candles,
                    f"Processed {i}/{total_candles}. Saved: {trades_saved}. ML Blocked: {filtered_trades}"
                )
        
        # Final Flush (blocks outside loop)
        if pending_trades:
            self._flush_trades(pending_trades)
        
        # Final stats
        closed_positions = broker.get_closed_positions()
        final_balance = broker.get_balance()
        total_trades = len(closed_positions)
        result = self._calculate_metrics(
            final_balance=final_balance,
            initial_balance=Decimal(str(config.get("initial_balance", 10000))),
            pair=pair,
            timeframe_str=timeframe_str,
            year=year,
            trades_saved=trades_saved,
            filtered_trades=filtered_trades,
            closed_positions=closed_positions,
            equity_curve=broker.equity_curve
        )
        
        # Extract win_rate from result for logging
        win_rate = result.get('win_rate', 0.0)
        self.log('INFO', f"Backtest finished. WinRate: {win_rate:.2f}%. ML Filtered: {filtered_trades} trades.")
        
        return result
    
    def _calculate_metrics(
        self, 
        final_balance, 
        initial_balance, 
        pair, 
        timeframe_str, 
        year, 
        trades_saved, 
        filtered_trades, 
        closed_positions, 
        equity_curve
    ) -> Dict[str, Any]:
        """Calculate comprehensive backtest metrics for WFO fitness function"""
        total_trades = len(closed_positions)
        
        # Guard against zero trades to avoid -inf/NaN in fitness
        if total_trades == 0:
            return {
                'pair': pair,
                'timeframe': timeframe_str,
                'year': year,
                'total_trades': 0,
                'winning_trades': 0,
                'losing_trades': 0,
                'final_balance': float(final_balance),
                'net_profit': 0.0,
                'return': 0.0,
                'win_rate': 0.0,
                'gross_profit': 0.0,
                'gross_loss': 0.0,
                'profit_factor': 1.0,
                'max_drawdown': 0.0,
                'max_drawdown_pct': 0.0,
                'sharpe': 0.0,
                'trades_saved_to_db': trades_saved,
                'ml_filtered_trades': filtered_trades
            }

        winning_trades = sum(1 for p in closed_positions if p.pnl > 0)
        losing_trades = total_trades - winning_trades
        win_rate = (winning_trades / total_trades * 100)
        
        gross_profit = float(sum(p.pnl for p in closed_positions if p.pnl > 0))
        gross_loss = float(abs(sum(p.pnl for p in closed_positions if p.pnl < 0)))
        
        # PF Logic: Cap at 10.0 if no loss, avoid 0.0 for winners
        if gross_loss == 0:
            profit_factor = 10.0 if gross_profit > 0 else 1.0
        else:
            profit_factor = gross_profit / gross_loss
            
        net_profit = float(final_balance - initial_balance)
        return_pct = (net_profit / float(initial_balance) * 100) if initial_balance > 0 else 0.0
        
        # Max Drawdown Calculation
        max_drawdown = 0.0
        if equity_curve:
            peak = float(equity_curve[0])
            for val in equity_curve:
                val_f = float(val)
                if val_f > peak:
                    peak = val_f
                if peak > 0:
                    dd = (peak - val_f) / peak
                    if dd > max_drawdown:
                        max_drawdown = dd
        
        max_drawdown_pct = max_drawdown * 100.0
        
        # Sharpe Ratio (Simple, non-annualized, good for relative fitness)
        sharpe = 0.0
        if equity_curve and len(equity_curve) > 1:
            eq = [float(x) for x in equity_curve]
            # Calculate returns series
            rets = []
            for i in range(1, len(eq)):
                if eq[i-1] > 0:
                    rets.append((eq[i] / eq[i-1]) - 1.0)
            
            if len(rets) >= 2:
                std = statistics.pstdev(rets)
                if std > 0:
                    sharpe = (statistics.mean(rets) / std) * math.sqrt(len(rets))
        
        return {
            'pair': pair,
            'timeframe': timeframe_str,
            'year': year,
            'total_trades': total_trades,
            'winning_trades': winning_trades,
            'losing_trades': losing_trades,
            'final_balance': float(final_balance),
            'net_profit': float(net_profit),
            'return': round(return_pct, 2),              # REQUIRED by run_wfo
            'win_rate': round(win_rate, 2),
            'gross_profit': round(gross_profit, 2),       # REQUIRED by fitness
            'gross_loss': round(gross_loss, 2),            # REQUIRED by fitness
            'profit_factor': round(float(profit_factor), 4),
            'max_drawdown': round(max_drawdown_pct, 2),    # REQUIRED by fitness (in %)
            'max_drawdown_pct': round(max_drawdown_pct, 2),
            'sharpe': round(float(sharpe), 4),             # REQUIRED by fitness
            'trades_saved_to_db': trades_saved,
            'ml_filtered_trades': filtered_trades
        }

    def _extract_market_state(self, current_candle, historical_candles: List) -> Dict[str, Any]:
        """Legacy manual extraction (fallback)"""
        if not historical_candles: return {}
        # Simple extraction for fallback
        return {'price': float(current_candle.close), 'volume': float(current_candle.volume)}

    def _flush_trades(self, trades: List[Dict]):
        """Persist a batch of trades to DB"""
        if not trades: return
        if self.db_failures > 0: return  # Skip if DB is down
        
        # Optimization for Mac M1: Disable DB persistence during WFO optimization (Task 8.9)
        # We identify optimization runs by their backtest_run_id (wfo_subtrain/wfo_valtrain)
        if trades[0].get('backtest_run_id') in ['wfo_subtrain', 'wfo_valtrain']:
            return
        
        try:
            with get_db_session() as db:
                for t in trades:
                    TradeRepository.create(db, t)
        except Exception as e:
            self.db_failures += 1
            self.log('WARNING', f"DB connection failed. Disabling persistence for this worker. Error: {str(e)}")


================================================================================
# FILE: src/alphas/base.py
================================================================================

# src/alphas/base.py
from abc import ABC, abstractmethod
from src.core.market import MarketState

class Alpha(ABC):
    """
    Abstract Base Class for all Alphas in the Pure Alpha Engine.
    
    An Alpha is an independent agent that provides a directional score 
    reflecting its opinion on the market state.
    """
    
    @abstractmethod
    def get_score(self, market_state: MarketState) -> float:
        """
        Calculate the alpha score for the given market state.
        
        Args:
            market_state: The current state of the market.
            
        Returns:
            float: A score between -1.0 (Strong Short) and 1.0 (Strong Long).
                   0.0 implies Neutral.
        """
        pass


================================================================================
# FILE: src/alphas/combiner.py
================================================================================

# src/alphas/combiner.py
from typing import List, Tuple, Optional, Any
from src.alphas.base import Alpha
from src.core.market import MarketState
from src.execution.executor import TradeSignal
from src.execution.broker import OrderSide

class AlphaCombiner:
    """
    Combines multiple Alpha signals into a single aggregate score and trade decision.
    
    Implements the weighted average of independent opinions (Pure Alpha philosophy).
    """
    
    def __init__(self, alphas_with_weights: List[Tuple[Alpha, float]]):
        """
        Args:
            alphas_with_weights: List of (AlphaInstance, weight)
        """
        self.alphas = alphas_with_weights
        
    def get_aggregate_score(self, market_state: MarketState) -> float:
        """
        Calculates the weighted average score of all alphas.
        
        Returns:
            float: Normalized score in range [-1.0, 1.0]
        """
        if not self.alphas:
            return 0.0
            
        total_score = 0.0
        total_weight = 0.0
        
        for alpha, weight in self.alphas:
            score = alpha.get_score(market_state)
            total_score += score * weight
            total_weight += weight
            
        if total_weight == 0:
            return 0.0
            
        return total_score / total_weight
        
    def get_signal(self, market_state: MarketState, threshold: float = 0.6) -> Optional[TradeSignal]:
        """
        Generates a trade signal if the aggregate score exceeds the threshold.
        
        Note: SL/TP should be determined by RiskManager, this provides direction & confidence.
        """
        score = self.get_aggregate_score(market_state)
        
        if abs(score) < threshold:
            return None
            
        # Direction
        side = OrderSide.BUY if score > 0 else OrderSide.SELL
        
        # We return a TradeSignal object. 
        # For compatibility with legacy system, we might need to fill entry_price.
        # But per requirements, direction is the key.
        
        # Accessing symbol from market_state
        symbol = getattr(market_state, 'symbol', 'BTCUSDT')
        
        # Note: In a full integration, SL/TP would be calculated outside or here 
        # using ATR, but for this task we focus on the combiner architecture.
        # We fill them with 0.0 here, expecting the RiskManager to override or 
        # the executor to handle them.
        
        return TradeSignal(
            symbol=symbol,
            side=side,
            entry_price=0.0, # Placeholder
            stop_loss=0.0,    # Placeholder
            take_profit=0.0,  # Placeholder
            confidence=abs(score) # Custom field added to TradeSignal if possible, or just used in logs
        )


================================================================================
# FILE: src/alphas/liquidity.py
================================================================================

# src/alphas/liquidity.py
from src.alphas.base import Alpha
from src.core.market import MarketState
import statistics

class Alpha_Liquidity(Alpha):
    """
    Alpha based on Volume and Microstructure (Liquidity).
    
    Score reflects relative volume expansion or contraction.
    High Volume Relative to Mean -> Positive score (Suggests strong institutional activity).
    Low Volume Relative to Mean -> Negative score (Suggests low liquidity/trap).
    """
    
    def __init__(self, period: int = 20):
        self.period = period
    
    def get_score(self, market_state: MarketState) -> float:
        series = market_state.h4
        if len(series) < self.period + 1:
            return 0.0
            
        # Extract volumes
        volumes = [float(series.get(i).volume) for i in range(len(series) - self.period, len(series))]
        current_volume = volumes[-1]
        
        # Mean volume of the lookback period
        avg_volume = statistics.mean(volumes[:-1])
        
        if avg_volume == 0:
            return 0.0
            
        # Ratio: (Current / Avg) - 1.0
        # 300 / 100 - 1 = 2.0
        # 20 / 100 - 1 = -0.8
        score = (current_volume / avg_volume) - 1.0
        
        # Scale and clip
        # Max score of 1.0 reached at 2x average volume expansion
        return max(-1.0, min(1.0, score))


================================================================================
# FILE: src/alphas/ml_confidence.py
================================================================================

# src/alphas/ml_confidence.py
import pandas as pd
from typing import Optional
from src.alphas.base import Alpha
from src.core.market import MarketState
from src.ml.analyzer import PatternAnalyzer
from src.ml.features import FeatureExtractor

class Alpha_ML_Confidence(Alpha):
    """
    Alpha based on Machine Learning model confidence.
    
    Transforms the model's win probability [0, 1] to a directional score [-1, 1].
    Probability 0.5 -> Score 0.0 (Neutral)
    Probability > 0.5 -> Positive score (Confidence in Alpha/Setup)
    Probability < 0.5 -> Negative score (Model expects failure)
    """
    
    def __init__(self, analyzer: Optional[PatternAnalyzer] = None):
        # Allow injecting analyzer for testing
        self.analyzer = analyzer or PatternAnalyzer()
        self.extractor = FeatureExtractor()
    
    def get_score(self, market_state: MarketState) -> float:
        # 1. Transform MarketState to DataFrame for extraction
        series = market_state.h4
        if len(series) < 50: # Need enough data for technical indicators
            return 0.0
            
        data = []
        for i in range(len(series)):
            c = series.get(i)
            data.append({
                'timestamp': c.timestamp,
                'open': float(c.open),
                'high': float(c.high),
                'low': float(c.low),
                'close': float(c.close),
                'volume': float(c.volume)
            })
            
        df = pd.DataFrame(data)
        
        # 2. Extract Features
        df_features = self.extractor.add_all_features(df)
        
        # 3. Predict Probability
        prob = self.analyzer.predict_proba(df_features)
        
        # 4. Normalize to [-1, 1]
        # (prob - 0.5) * 2.0
        # 0.8 -> (0.3) * 2 = 0.6
        # 0.5 -> (0.0) * 2 = 0.0
        # 0.2 -> (-0.3) * 2 = -0.6
        score = (prob - 0.5) * 2.0
        
        return max(-1.0, min(1.0, score))


================================================================================
# FILE: src/alphas/momentum.py
================================================================================

# src/alphas/momentum.py
from src.alphas.base import Alpha
from src.core.market import MarketState

class Alpha_Momentum(Alpha):
    """
    Alpha based on Trend Strength and Momentum using RSI.
    
    Score is normalized from RSI [0, 100] to [-1.0, 1.0].
    RSI > 50 (Bullish) -> Positive score.
    RSI < 50 (Bearish) -> Negative score.
    """
    
    def get_score(self, market_state: MarketState) -> float:
        # Use cached RSI from market state
        rsi_series = market_state.rsi
        if not rsi_series:
            return 0.0
            
        current_rsi = rsi_series[-1]
        
        # Normalize: (RSI - 50) / 50
        # 100 -> 1.0
        # 50  -> 0.0
        # 0   -> -1.0
        score = (current_rsi - 50) / 50.0
        
        # Clip to ensure bounds just in case of rounding or edge cases
        return max(-1.0, min(1.0, score))


================================================================================
# FILE: src/alphas/ob_quality.py
================================================================================

# src/alphas/ob_quality.py
from src.alphas.base import Alpha
from src.core.market import MarketState
from src.strategy.ob import detect_ob, OBType

class Alpha_OB_Quality(Alpha):
    """
    Alpha based on the quality and presence of TJR Order Blocks (OB).
    
    Score is positive for Bullish OBs and negative for Bearish OBs.
    The magnitude reflects the perceived 'strength' or 'quality' of the setup.
    """
    
    def get_score(self, market_state: MarketState) -> float:
        # Use H4 as the primary timeframe for TJR OB detection
        series = market_state.h4
        if len(series) == 0:
            return 0.0
            
        current_idx = len(series) - 1
        
        # 1. Detect OB using existing strategy logic
        ob = detect_ob(series, current_idx)
        
        if not ob:
            return 0.0
            
        # 2. Calculate Directional Base Score
        score = 1.0 if ob.type == OBType.BULLISH else -1.0
        
        # 3. Quality Multiplier (Placeholder for future complexity)
        # For now, we return full strength if detected.
        # Future: factor in sweep volume, BOS displacement, etc.
        quality_multiplier = 1.0
        
        return score * quality_multiplier


================================================================================
# FILE: src/alphas/volatility.py
================================================================================

# src/alphas/volatility.py
from src.alphas.base import Alpha
from src.core.market import MarketState
import statistics

class Alpha_Volatility(Alpha):
    """
    Alpha based on Volatility Regime Detection using ATR.
    
    Score reflects ATR expansion or contraction relative to its average.
    Positive score -> Volatility is expanding (favorable for trend/TJR).
    Negative score -> Volatility is contracting (risk of chop/range).
    """
    
    def __init__(self, period: int = 14):
        self.period = period
    
    def get_score(self, market_state: MarketState) -> float:
        # Use cached ATR series
        atr_series = market_state.atr
        if len(atr_series) < self.period + 1:
            return 0.0
            
        current_atr = atr_series[-1]
        # Calculate moving average of ATR
        avg_atr = statistics.mean(atr_series[-self.period:])
        
        if avg_atr == 0:
            return 0.0
            
        # Ratio: Current / Average - 1.0
        # Expansion (2.0 / 1.0) - 1.0 = 1.0
        # Contraction (0.5 / 1.0) - 1.0 = -0.5
        score = (current_atr / avg_atr) - 1.0
        
        # Scale and clip
        # Assume a ratio of 2.0 (100% increase) is the max score of 1.0
        return max(-1.0, min(1.0, score))


================================================================================
# FILE: src/api/main.py
================================================================================

# src/api/main.py
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uuid
import threading
import asyncio

from src.database import get_db_session, init_db
from src.database.repository import TradeRepository, StrategyRepository, PatternRepository, BacktestRunRepository
from src.database.models import Trade, Strategy, Pattern
from src.agents.orchestrator import OrchestratorAgent

from fastapi.staticfiles import StaticFiles

app = FastAPI(title="BOT8000 Trading System", version="3.0.0")
app.mount("/static", StaticFiles(directory="src/api/static"), name="static")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global State (para demo)
active_tasks = {}

class OrchestratorConfig(BaseModel):
    pairs: List[str] = ["BTCUSDT"]
    years: List[int] = [2024]
    num_mutations: int = 3
    train_months: List[int] = [1, 2, 3]
    val_months: List[int] = [4]

@app.on_event("startup")
def startup_event():
    init_db()

from fastapi.responses import RedirectResponse

@app.get("/")
def read_root():
    return RedirectResponse(url="/static/index.html")

@app.post("/api/run-pipeline")
async def run_pipeline(config: OrchestratorConfig, background_tasks: BackgroundTasks):
    """Inicia el pipeline completo de ML en background."""
    task_id = str(uuid.uuid4())
    
    def run_task(tid, cfg):
        orchestrator = OrchestratorAgent()
        try:
            res = orchestrator.execute(cfg.dict())
            active_tasks[tid]['status'] = 'COMPLETED'
            active_tasks[tid]['result'] = res
        except Exception as e:
            active_tasks[tid]['status'] = 'FAILED'
            active_tasks[tid]['error'] = str(e)

    active_tasks[task_id] = {'status': 'RUNNING', 'config': config.dict()}
    background_tasks.add_task(run_task, task_id, config)
    
    return {"task_id": task_id, "status": "started"}

@app.get("/api/tasks/{task_id}")
def get_task_status(task_id: str):
    if task_id not in active_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    return active_tasks[task_id]

@app.get("/api/stats/trades")
def get_trades_stats():
    with get_db_session() as db:
        trades = db.query(Trade).limit(100).all()
        # Simple stats
        if not trades:
            return {"count": 0}
        
        wins = [t for t in trades if t.profit_loss is not None and t.profit_loss > 0]
        losses = [t for t in trades if t.profit_loss is not None and t.profit_loss <= 0]
        
        return {
            "total_trades": len(trades),
            "win_rate": len(wins) / len(trades) * 100 if trades else 0,
            "recent_trades": [
                {
                    "pair": t.pair,
                    "side": t.side,
                    "pnl": float(t.profit_loss) if t.profit_loss is not None else 0.0,
                    "result": t.result,
                    "date": t.timestamp
                } for t in trades[-10:]
            ]
        }

@app.get("/api/strategies/approved")
def get_approved_strategies():
    with get_db_session() as db:
        strats = StrategyRepository.get_by_status(db, "APPROVED")
        return [
            {
                "name": s.name,
                "pf": float(s.profit_factor) if s.profit_factor else 0,
                "wr": float(s.win_rate) if s.win_rate else 0,
                "params": s.parameters
            } for s in strats
        ]

@app.get("/api/ml/patterns")
def get_patterns():
    with get_db_session() as db:
        patterns = PatternRepository.get_active_patterns(db)
        return [
            {
                "desc": p.description,
                "win_rate": float(p.win_rate),
                "samples": p.sample_size
            } for p in patterns
        ]


================================================================================
# FILE: src/core/candle.py
================================================================================

from dataclasses import dataclass
from .types import Price, Volume, Timestamp
from .timeframe import Timeframe

@dataclass(frozen=True)
class Candle:
    timestamp: Timestamp
    open: Price
    high: Price
    low: Price
    close: Price
    volume: Volume
    timeframe: Timeframe
    complete: bool = False

    def __post_init__(self) -> None:
        # Invariant checks
        if self.high < self.low:
            raise ValueError(f"High ({self.high}) < Low ({self.low})")
        
        if self.volume < 0:
            raise ValueError(f"Volume ({self.volume}) < 0")
            
        # Wick integrity checks
        if self.high < self.open:
            raise ValueError(f"High ({self.high}) < Open ({self.open})")
        if self.high < self.close:
            raise ValueError(f"High ({self.high}) < Close ({self.close})")
            
        if self.low > self.open:
            raise ValueError(f"Low ({self.low}) > Open ({self.open})")
        if self.low > self.close:
            raise ValueError(f"Low ({self.low}) > Close ({self.close})")


================================================================================
# FILE: src/core/classifier.py
================================================================================

# src/core/classifier.py
from src.core.regime import MarketRegime
from src.core.market import MarketState

from typing import Dict, Any

def classify_regime(market_state: MarketState, params: Dict[str, Any] = None) -> MarketRegime:
    """
    Classifies the current market state into a specific MarketRegime.
    
    Logic hierarchy:
    1. High Volatility (ATR expansion)
    2. Trending (ADX and EMA alignment)
    3. Sideways (Low ADX)
    4. Breakout Pending (Extreme low vol + no trend)
    """
    # Defaults (si no se pasan params)
    if params is None:
        adx_trend_thresh = 25
        adx_sideways_thresh = 20
        atr_high_mult = 1.5
        atr_low_mult = 0.7
    else:
        adx_trend_thresh = params.get("adx_trend_threshold", 25)
        adx_sideways_thresh = params.get("adx_sideways_threshold", 15)
        atr_high_mult = params.get("atr_high_mult", 1.5)
        atr_low_mult = params.get("atr_low_mult", 0.65)
        
    adx = market_state.adx
    # Use last value of ATR series
    current_atr = market_state.atr[-1] if market_state.atr else 1.0
    atr_avg = market_state.atr_avg_14
    ema_alignment = market_state.ema_alignment
    
    # 1. High volatility priority
    if current_atr > (atr_avg * atr_high_mult):
        return MarketRegime.HIGH_VOLATILITY
        
    # 2. Strong trend
    if adx > adx_trend_thresh:
        if ema_alignment == 'bullish':
            return MarketRegime.TRENDING_BULLISH
        elif ema_alignment == 'bearish':
            return MarketRegime.TRENDING_BEARISH
            
    # 3. Consolidation (Low vol + No trend) -> Priority over generic Sideways
    if current_atr < (atr_avg * atr_low_mult) and adx < adx_trend_thresh:
        return MarketRegime.BREAKOUT_PENDING

    # 4. Sideways range
    if adx < adx_sideways_thresh:
        return MarketRegime.SIDEWAYS_RANGE
        
    # 5. Default
    return MarketRegime.SIDEWAYS_RANGE


================================================================================
# FILE: src/core/market.py
================================================================================


from dataclasses import dataclass, field
from typing import List, cast, Dict, Any
import pandas as pd
from ta.momentum import RSIIndicator
from ta.volatility import AverageTrueRange
from ta.trend import ADXIndicator, EMAIndicator
from .candle import Candle
from .series import MarketSeries
from .timeframe import Timeframe

@dataclass(frozen=True)
class MarketState:
    symbol: str
    m5: MarketSeries
    m15: MarketSeries
    h1: MarketSeries
    h4: MarketSeries
    _cache: Dict[str, Any] = field(default_factory=dict, init=False, repr=False)

    def __post_init__(self):
        # Use object.__setattr__ because the dataclass is frozen
        object.__setattr__(self, '_cache', {})

    @property
    def rsi(self):
        """Lazy cached RSI calculation (placeholder)."""
        if 'rsi' not in self._cache:
            self._cache['rsi'] = calculate_rsi(self.h4)
        return self._cache['rsi']

    @property
    def atr(self):
        """Lazy cached ATR calculation (placeholder)."""
        if 'atr' not in self._cache:
            self._cache['atr'] = calculate_atr(self.h4)
        return self._cache['atr']

    @property
    def adx(self) -> float:
        """Lazy cached ADX calculation."""
        if 'adx' not in self._cache:
            self._cache['adx'] = calculate_adx(self.h4)
        return self._cache['adx']

    @property
    def atr_avg_14(self) -> float:
        """Lazy cached ATR average (14 periods)."""
        if 'atr_avg_14' not in self._cache:
            # Note: For simplicity, we calculate it from the atr series
            atr_series = self.atr
            if len(atr_series) >= 14:
                self._cache['atr_avg_14'] = sum(atr_series[-14:]) / 14.0
            else:
                self._cache['atr_avg_14'] = sum(atr_series) / len(atr_series) if atr_series else 1.0
        return self._cache['atr_avg_14']

    @property
    def ema_alignment(self) -> str:
        """Lazy cached EMA alignment (20 vs 50)."""
        if 'ema_alignment' not in self._cache:
            ema_20 = calculate_ema(self.h4, 20)
            ema_50 = calculate_ema(self.h4, 50)
            
            if ema_20 > ema_50:
                alignment = 'bullish'
            elif ema_20 < ema_50:
                alignment = 'bearish'
            else:
                alignment = 'neutral'
            
            self._cache['ema_alignment'] = alignment
        return self._cache['ema_alignment']

    @classmethod
    def empty(cls, symbol: str) -> 'MarketState':
        """Factory method to create an empty market state."""
        empty_series = MarketSeries([])
        return cls(
            symbol=symbol,
            m5=empty_series,
            m15=empty_series,
            h1=empty_series,
            h4=empty_series
        )

    def update(self, candle: Candle) -> 'MarketState':
        """
        Returns a NEW MarketState with the candle added to the correct series.
        """
        if candle.timeframe == Timeframe.M5:
            return MarketState(
                symbol=self.symbol,
                m5=self.m5.add(candle),
                m15=self.m15,
                h1=self.h1,
                h4=self.h4
            )
        elif candle.timeframe == Timeframe.M15:
            return MarketState(
                symbol=self.symbol,
                m5=self.m5,
                m15=self.m15.add(candle),
                h1=self.h1,
                h4=self.h4
            )
        elif candle.timeframe == Timeframe.H1:
            return MarketState(
                symbol=self.symbol,
                m5=self.m5,
                m15=self.m15,
                h1=self.h1.add(candle),
                h4=self.h4
            )
        elif candle.timeframe == Timeframe.H4:
            return MarketState(
                symbol=self.symbol,
                m5=self.m5,
                m15=self.m15,
                h1=self.h1,
                h4=self.h4.add(candle)
            )
        else:
            # Should be unreachable if Timeframe enum is exhaustive for this bot
            return self

    def get_series(self, timeframe: Timeframe) -> MarketSeries:
        """Polymorphic access to series by timeframe."""
        if timeframe == Timeframe.M5:
            return self.m5
        elif timeframe == Timeframe.M15:
            return self.m15
        elif timeframe == Timeframe.H1:
            return self.h1
        elif timeframe == Timeframe.H4:
            return self.h4
        else:
            raise ValueError(f"Unsupported timeframe: {timeframe}")


def calculate_rsi(series: MarketSeries, period: int = 14) -> List[float]:
    """Real RSI calculation using ta library."""
    if len(series) < period:
        return [50.0] * len(series)  # Not enough data
    
    closes = pd.Series([c.close for c in series]).astype(float)
    rsi = RSIIndicator(closes, window=period)
    return rsi.rsi().fillna(50.0).tolist()

def calculate_atr(series: MarketSeries, period: int = 14) -> List[float]:
    """Real ATR calculation using ta library."""
    if len(series) < period:
        return [1.0] * len(series)
    
    highs = pd.Series([c.high for c in series]).astype(float)
    lows = pd.Series([c.low for c in series]).astype(float)
    closes = pd.Series([c.close for c in series]).astype(float)
    
    atr = AverageTrueRange(highs, lows, closes, window=period)
    return atr.average_true_range().fillna(1.0).tolist()

def calculate_adx(series: MarketSeries, period: int = 14) -> float:
    """Real ADX calculation using ta library."""
    if len(series) < period * 2:
        return 20.0  # Not enough data
    
    highs = pd.Series([c.high for c in series]).astype(float)
    lows = pd.Series([c.low for c in series]).astype(float)
    closes = pd.Series([c.close for c in series]).astype(float)
    
    adx = ADXIndicator(highs, lows, closes, window=period)
    result = adx.adx().iloc[-1]
    
    return float(result) if not pd.isna(result) else 20.0

def calculate_ema(series: MarketSeries, period: int) -> float:
    """Real EMA calculation using ta library."""
    if len(series) < period:
        return float(series.get(-1).close) if len(series) > 0 else 0.0
    
    closes = pd.Series([c.close for c in series]).astype(float)
    ema = EMAIndicator(closes, window=period)
    result = ema.ema_indicator().iloc[-1]
    
    return float(result) if not pd.isna(result) else float(closes.iloc[-1])


def load_candles_from_csv(filepath: str) -> List[Candle]:
    """Helper to load candles using standard utils."""
    from src.utils.data_loader import load_binance_csv
    from .timeframe import Timeframe
    # Default to H4 for WFO as per request/files
    return load_binance_csv(filepath, Timeframe.H4)




================================================================================
# FILE: src/core/regime.py
================================================================================

# src/core/regime.py
from enum import Enum

class MarketRegime(Enum):
    TRENDING_BULLISH = "trending_bullish"
    TRENDING_BEARISH = "trending_bearish"
    SIDEWAYS_RANGE = "sideways_range"
    HIGH_VOLATILITY = "high_volatility"
    BREAKOUT_PENDING = "breakout_pending"
    NEWS_DRIVEN = "news_driven"


================================================================================
# FILE: src/core/series.py
================================================================================

from typing import List, Optional, Iterator
from .candle import Candle

class MarketSeries:
    def __init__(self, candles: List[Candle]):
        # Enforce sorting by timestamp on creation
        self._candles = sorted(candles, key=lambda c: c.timestamp)
    
    @property
    def candles(self) -> List[Candle]:
        return self._candles
    
    @property
    def current(self) -> Candle:
        if not self._candles:
            raise IndexError("Series is empty")
        return self._candles[-1]
    
    @property
    def last_closed(self) -> Optional[Candle]:
        """Returns the last candle where complete=True."""
        for c in reversed(self._candles):
            if c.complete:
                return c
        return None

    def __len__(self) -> int:
        return len(self._candles)
        
    def __iter__(self) -> Iterator[Candle]:
        return iter(self._candles)

    def add(self, candle: Candle) -> 'MarketSeries':
        """
        Functional add: Returns a NEW MarketSeries instance with the new candle.
        Does NOT mutate the current instance.
        """
        # Optimized for readability, not performance (yet)
        return MarketSeries(self._candles + [candle])
    
    def get(self, index: int) -> Candle:
        return self._candles[index]


================================================================================
# FILE: src/core/timeframe.py
================================================================================

from enum import Enum

class Timeframe(Enum):
    M5 = "5m"
    M15 = "15m"
    H1 = "1h"
    H4 = "4h"


================================================================================
# FILE: src/core/types.py
================================================================================

from decimal import Decimal

Price = Decimal
Volume = Decimal
Timestamp = int  # Unix timestamp in milliseconds


================================================================================
# FILE: src/data/downloader.py
================================================================================

import os
import requests
import zipfile
import time
from pathlib import Path
from typing import List
from tqdm import tqdm

def download_file(url: str, dest_path: Path, max_retries: int = 3) -> bool:
    """Download a file with retry logic and progress bar."""
    if dest_path.exists():
        return True # Skip if raw zip exists (though usually we clean it up)

    for attempt in range(max_retries):
        try:
            response = requests.get(url, stream=True, timeout=10)
            if response.status_code == 404:
                return False # Not found (e.g. future month)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(dest_path, 'wb') as f, tqdm(
                desc=dest_path.name,
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
                leave=False
            ) as bar:
                for data in response.iter_content(chunk_size=1024):
                    size = f.write(data)
                    bar.update(size)
            return True
            
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1) # Backoff
            else:
                print(f"Failed to download {url}: {e}")
                if dest_path.exists():
                    dest_path.unlink() # Cleanup partial
                return False
    return False

def download_binance_data(
    pairs: List[str],
    timeframes: List[str],
    years: List[int],
    months: List[int],
    data_dir: str = "data/raw"
):
    """
    Downloads monthly CSVs for the specified permutations.
    """
    base_url = "https://data.binance.vision/data/spot/monthly/klines"
    Path(data_dir).mkdir(parents=True, exist_ok=True)
    
    print(f"--- Checking Logic: Data Availability ({len(pairs)} pairs, {len(timeframes)} tfs, {len(years)} years) ---")
    
    for pair in pairs:
        for tf in timeframes:
            for year in years:
                for month in months:
                    # Target CSV naming convention: BTCUSDT-1h-2024-01.csv
                    # Binance ZIP naming: BTCUSDT-1h-2024-01.zip
                    month_str = f"{month:02d}"
                    file_base = f"{pair}-{tf}-{year}-{month_str}"
                    target_csv = Path(data_dir) / f"{file_base}.csv"
                    
                    # 1. Check if CSV exists and is valid
                    if target_csv.exists() and target_csv.stat().st_size > 100:
                        continue # Skip
                        
                    # 2. Download ZIP
                    zip_name = f"{file_base}.zip"
                    url = f"{base_url}/{pair}/{tf}/{zip_name}"
                    temp_zip = Path(data_dir) / zip_name
                    
                    # print(f"Downloading {zip_name}...")
                    success = download_file(url, temp_zip)
                    
                    if success:
                        # 3. Extract
                        try:
                            with zipfile.ZipFile(temp_zip, 'r') as zip_ref:
                                # Binance zip usually contains one file with the same name as the zip but .csv
                                zip_ref.extractall(data_dir)
                            
                            # Cleanup
                            temp_zip.unlink()
                            
                            # Verify extraction
                            if not target_csv.exists():
                                print(f"Warning: Extraction did not produce expected file {target_csv}")
                                
                        except zipfile.BadZipFile:
                            print(f"Error: Corrupted zip file {zip_name}")
                            temp_zip.unlink()
                    else:
                        # 404 or fail, just continue
                        pass

if __name__ == "__main__":
    # Test
    download_binance_data(["BTCUSDT"], ["4h"], [2024], [1])


================================================================================
# FILE: src/database/__init__.py
================================================================================

# src/database/__init__.py
from src.database.connection import (
    engine,
    SessionLocal,
    get_db,
    get_db_session,
    init_db,
    test_connection
)
from src.database.models import (
    Base,
    Trade,
    Pattern,
    Strategy,
    BacktestRun,
    AgentLog
)

__all__ = [
    'engine',
    'SessionLocal',
    'get_db',
    'get_db_session',
    'init_db',
    'test_connection',
    'Base',
    'Trade',
    'Pattern',
    'Strategy',
    'BacktestRun',
    'AgentLog',
]


================================================================================
# FILE: src/database/connection.py
================================================================================

# src/database/connection.py
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import QueuePool
from contextlib import contextmanager
import os
from typing import Generator

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres123@localhost:5432/trading_bot_ml"
)

# Engine con connection pooling
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True,  # Verifica conexiones antes de usar
    echo=False  # Set True para debug SQL
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db() -> Generator[Session, None, None]:
    """Dependency para FastAPI"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@contextmanager
def get_db_session() -> Generator[Session, None, None]:
    """Context manager para uso general"""
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()

def init_db():
    """Inicializar DB (crear tablas si no existen)"""
    from src.database.models import Base
    Base.metadata.create_all(bind=engine)

def test_connection() -> bool:
    """Probar conexi√≥n a DB"""
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        return True
    except Exception as e:
        print(f"Error conectando a DB: {e}")
        return False


================================================================================
# FILE: src/database/models.py
================================================================================

# src/database/models.py
from datetime import datetime
from typing import Optional, Dict, Any, List
from decimal import Decimal
from sqlalchemy import (
    Column, Integer, String, DateTime, Numeric, 
    Boolean, Text, CheckConstraint, Index, ARRAY
)
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid

Base = declarative_base()

class Trade(Base):
    __tablename__ = 'trades'
    
    id = Column(Integer, primary_key=True)
    trade_id = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, nullable=False)
    timestamp = Column(DateTime, nullable=False)
    pair = Column(String(20), nullable=False)
    timeframe = Column(String(10), nullable=False)
    symbol = Column(String(20), nullable=False, index=True)
    side = Column(String(10), nullable=False)
    
    # Prices
    entry_price = Column(Numeric(20, 8), nullable=False)
    exit_price = Column(Numeric(20, 8))
    stop_loss = Column(Numeric(20, 8), nullable=False)
    take_profit = Column(Numeric(20, 8), nullable=False)
    
    # Results
    result = Column(String(10))
    profit_loss = Column(Numeric(20, 8))
    profit_loss_pct = Column(Numeric(10, 4))
    risk_reward = Column(Numeric(10, 2))
    
    # Market state
    market_state = Column(JSONB, nullable=False)
    
    # Metadata
    strategy_version = Column(String(50), nullable=False)
    backtest_run_id = Column(UUID(as_uuid=True), nullable=False)
    worker_id = Column(String(50))
    agent_name = Column(String(50))
    market_regime = Column(String(50))
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    __table_args__ = (
        CheckConstraint("side IN ('LONG', 'SHORT')", name='check_side'),
        CheckConstraint("result IN ('WIN', 'LOSS', 'BREAKEVEN', 'OPEN')", name='check_result'),
        Index('idx_trades_pair_timeframe', 'pair', 'timeframe'),
        Index('idx_trades_result', 'result'),
        Index('idx_trades_timestamp', 'timestamp'),
        Index('idx_trades_backtest_run', 'backtest_run_id'),
        Index('idx_trades_market_state', 'market_state', postgresql_using='gin'),
    )

class Pattern(Base):
    __tablename__ = 'patterns'
    
    id = Column(Integer, primary_key=True)
    pattern_id = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, nullable=False)
    pattern_type = Column(String(100), nullable=False)
    description = Column(Text)
    conditions = Column(JSONB, nullable=False)
    
    # Metrics
    win_rate = Column(Numeric(5, 2))
    avg_profit = Column(Numeric(20, 8))
    sample_size = Column(Integer, nullable=False)
    confidence_score = Column(Numeric(5, 2))
    
    # Applicability
    applicable_pairs = Column(ARRAY(String))
    applicable_timeframes = Column(ARRAY(String))
    
    # Metadata
    discovered_at = Column(DateTime, default=datetime.utcnow)
    last_validated_at = Column(DateTime)
    is_active = Column(Boolean, default=True)
    
    __table_args__ = (
        Index('idx_patterns_type', 'pattern_type'),
        Index('idx_patterns_active', 'is_active'),
    )

class Strategy(Base):
    __tablename__ = 'strategies'
    
    id = Column(Integer, primary_key=True)
    strategy_id = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, nullable=False)
    symbol = Column(String(20), nullable=False, index=True)
    name = Column(String(100), unique=True, nullable=False)
    description = Column(Text)
    
    # Configuration
    base_strategy = Column(String(50), nullable=False)
    parameters = Column(JSONB, nullable=False)
    filters = Column(JSONB)
    
    # Backtest results
    backtest_results = Column(JSONB)
    
    # Performance metrics
    total_trades = Column(Integer)
    win_rate = Column(Numeric(5, 2))
    profit_factor = Column(Numeric(10, 4))
    max_drawdown = Column(Numeric(10, 4))
    sharpe_ratio = Column(Numeric(10, 4))
    
    # Status
    status = Column(String(20), default='TESTING')
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    tested_at = Column(DateTime)
    approved_at = Column(DateTime)
    
    __table_args__ = (
        CheckConstraint("status IN ('TESTING', 'APPROVED', 'REJECTED', 'ARCHIVED')", name='check_status'),
        Index('idx_strategies_status', 'status'),
        Index('idx_strategies_base', 'base_strategy'),
    )

class BacktestRun(Base):
    __tablename__ = 'backtest_runs'
    
    id = Column(Integer, primary_key=True)
    run_id = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, nullable=False)
    config = Column(JSONB, nullable=False)
    status = Column(String(20), default='RUNNING')
    
    # Stats
    total_trades = Column(Integer, default=0)
    completed_trades = Column(Integer, default=0)
    failed_trades = Column(Integer, default=0)
    
    # Timing
    started_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    duration_seconds = Column(Integer)
    
    # Worker info
    workers = Column(JSONB)
    
    __table_args__ = (
        CheckConstraint("status IN ('RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED')", name='check_status'),
        Index('idx_backtest_runs_status', 'status'),
        Index('idx_backtest_runs_started', 'started_at'),
    )

class AgentLog(Base):
    __tablename__ = 'agent_logs'
    
    id = Column(Integer, primary_key=True)
    agent_name = Column(String(50), nullable=False)
    log_level = Column(String(10), nullable=False)
    message = Column(Text, nullable=False)
    context = Column(JSONB)
    timestamp = Column(DateTime, default=datetime.utcnow)
    
    __table_args__ = (
        CheckConstraint("log_level IN ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')", name='check_log_level'),
        Index('idx_agent_logs_agent', 'agent_name'),
        Index('idx_agent_logs_level', 'log_level'),
        Index('idx_agent_logs_timestamp', 'timestamp'),
    )


================================================================================
# FILE: src/database/repository.py
================================================================================

# src/database/repository.py
from typing import List, Optional, Dict, Any
from datetime import datetime
from decimal import Decimal
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func, desc, Float
import uuid

from src.database.models import Trade, Pattern, Strategy, BacktestRun, AgentLog

class TradeRepository:
    """Repository para operaciones CRUD de trades"""
    
    @staticmethod
    def create(db: Session, trade_data: Dict[str, Any]) -> Trade:
        """Crear nuevo trade"""
        trade = Trade(**trade_data)
        db.add(trade)
        db.flush()  # Get ID sin commit
        return trade
    
    @staticmethod
    def bulk_create(db: Session, trades_data: List[Dict[str, Any]]) -> List[Trade]:
        """Crear m√∫ltiples trades (optimizado)"""
        trades = [Trade(**data) for data in trades_data]
        db.bulk_save_objects(trades)
        db.flush()
        return trades
    
    @staticmethod
    def get_by_id(db: Session, trade_id: uuid.UUID) -> Optional[Trade]:
        """Obtener trade por ID"""
        return db.query(Trade).filter(Trade.trade_id == trade_id).first()
    
    @staticmethod
    def get_by_backtest_run(db: Session, run_id: uuid.UUID) -> List[Trade]:
        """Obtener todos los trades de un backtest run"""
        return db.query(Trade).filter(Trade.backtest_run_id == run_id).all()
    
    @staticmethod
    def get_by_strategy_version(db: Session, strategy_version: str) -> List[Trade]:
        """Obtener trades por versi√≥n de estrategia"""
        return db.query(Trade).filter(Trade.strategy_version == strategy_version).all()

    
    @staticmethod
    def get_losing_trades(
        db: Session, 
        pair: Optional[str] = None,
        timeframe: Optional[str] = None,
        limit: int = 1000
    ) -> List[Trade]:
        """Obtener trades perdedores con filtros opcionales"""
        query = db.query(Trade).filter(Trade.result == 'LOSS')
        
        if pair:
            query = query.filter(Trade.pair == pair)
        if timeframe:
            query = query.filter(Trade.timeframe == timeframe)
        
        return query.order_by(desc(Trade.timestamp)).limit(limit).all()
    
    @staticmethod
    def get_winning_trades(
        db: Session,
        pair: Optional[str] = None,
        timeframe: Optional[str] = None,
        limit: int = 1000
    ) -> List[Trade]:
        """Obtener trades ganadores con filtros opcionales"""
        query = db.query(Trade).filter(Trade.result == 'WIN')
        
        if pair:
            query = query.filter(Trade.pair == pair)
        if timeframe:
            query = query.filter(Trade.timeframe == timeframe)
        
        return query.order_by(desc(Trade.timestamp)).limit(limit).all()
    
    @staticmethod
    def count_by_result(db: Session, backtest_run_id: uuid.UUID) -> Dict[str, int]:
        """Contar trades por resultado"""
        results = db.query(
            Trade.result,
            func.count(Trade.id)
        ).filter(
            Trade.backtest_run_id == backtest_run_id
        ).group_by(Trade.result).all()
        
        return {result: count for result, count in results}
    
    @staticmethod
    def get_trades_by_market_conditions(
        db: Session,
        volatility_min: Optional[float] = None,
        volatility_max: Optional[float] = None,
        volume_min: Optional[float] = None,
        limit: int = 1000
    ) -> List[Trade]:
        """Obtener trades filtrando por condiciones de mercado en JSONB"""
        query = db.query(Trade)
        
        if volatility_min is not None:
            query = query.filter(
                Trade.market_state['volatility'].astext.cast(Float) >= volatility_min
            )
        if volatility_max is not None:
            query = query.filter(
                Trade.market_state['volatility'].astext.cast(Float) <= volatility_max
            )
        if volume_min is not None:
            query = query.filter(
                Trade.market_state['volume'].astext.cast(Float) >= volume_min
            )
        
        return query.limit(limit).all()

class PatternRepository:
    """Repository para operaciones CRUD de patterns"""
    
    @staticmethod
    def create(db: Session, pattern_data: Dict[str, Any]) -> Pattern:
        """Crear nuevo pattern"""
        pattern = Pattern(**pattern_data)
        db.add(pattern)
        db.flush()
        return pattern
    
    @staticmethod
    def get_active_patterns(db: Session) -> List[Pattern]:
        """Obtener patterns activos"""
        return db.query(Pattern).filter(Pattern.is_active == True).all()
    
    @staticmethod
    def get_by_type(db: Session, pattern_type: str) -> List[Pattern]:
        """Obtener patterns por tipo"""
        return db.query(Pattern).filter(Pattern.pattern_type == pattern_type).all()
    
    @staticmethod
    def get_high_confidence(db: Session, min_confidence: float = 0.7) -> List[Pattern]:
        """Obtener patterns con alta confianza"""
        return db.query(Pattern).filter(
            Pattern.confidence_score >= min_confidence,
            Pattern.is_active == True
        ).order_by(desc(Pattern.confidence_score)).all()
    
    @staticmethod
    def deactivate(db: Session, pattern_id: uuid.UUID) -> bool:
        """Desactivar un pattern"""
        pattern = db.query(Pattern).filter(Pattern.pattern_id == pattern_id).first()
        if pattern:
            pattern.is_active = False
            db.flush()
            return True
        return False

class StrategyRepository:
    """Repository para operaciones CRUD de strategies"""
    
    @staticmethod
    def create(db: Session, strategy_data: Dict[str, Any]) -> Strategy:
        """Crear nueva estrategia"""
        strategy = Strategy(**strategy_data)
        db.add(strategy)
        db.flush()
        return strategy
    
    @staticmethod
    def get_by_status(db: Session, status: str) -> List[Strategy]:
        """Obtener estrategias por status"""
        return db.query(Strategy).filter(Strategy.status == status).all()
    
    @staticmethod
    def get_approved(db: Session, limit: int = 10) -> List[Strategy]:
        """Obtener estrategias aprobadas ordenadas por performance"""
        return db.query(Strategy).filter(
            Strategy.status == 'APPROVED'
        ).order_by(
            desc(Strategy.profit_factor)
        ).limit(limit).all()
    
    @staticmethod
    def update_backtest_results(
        db: Session,
        strategy_id: uuid.UUID,
        results: Dict[str, Any]
    ) -> Optional[Strategy]:
        """Actualizar resultados de backtest"""
        strategy = db.query(Strategy).filter(
            Strategy.strategy_id == strategy_id
        ).first()
        
        if strategy:
            strategy.backtest_results = results
            strategy.total_trades = results.get('total_trades')
            strategy.win_rate = results.get('win_rate')
            strategy.profit_factor = results.get('profit_factor')
            strategy.max_drawdown = results.get('max_drawdown')
            strategy.sharpe_ratio = results.get('sharpe_ratio')
            strategy.tested_at = datetime.utcnow()
            db.flush()
        
        return strategy
    
    @staticmethod
    def approve(db: Session, strategy_id: uuid.UUID, profit_factor: Optional[float] = None, win_rate: Optional[float] = None) -> Optional[Strategy]:
        """Aprobar estrategia con m√©tricas finales"""
        strategy = db.query(Strategy).filter(
            Strategy.strategy_id == strategy_id
        ).first()
        
        if strategy:
            strategy.status = 'APPROVED'
            strategy.approved_at = datetime.utcnow()
            if profit_factor is not None:
                strategy.profit_factor = profit_factor
            if win_rate is not None:
                strategy.win_rate = win_rate
            db.flush()
        
        return strategy

class BacktestRunRepository:
    """Repository para operaciones CRUD de backtest runs"""
    
    @staticmethod
    def create(db: Session, config: Dict[str, Any]) -> BacktestRun:
        """Crear nuevo backtest run"""
        run = BacktestRun(
            run_id=uuid.uuid4(),
            config=config,
            status='RUNNING'
        )
        db.add(run)
        db.flush()
        return run
    
    @staticmethod
    def update_progress(
        db: Session,
        run_id: uuid.UUID,
        completed: int,
        failed: int
    ) -> Optional[BacktestRun]:
        """Actualizar progreso del run"""
        run = db.query(BacktestRun).filter(
            BacktestRun.run_id == run_id
        ).first()
        
        if run:
            run.completed_trades = completed
            run.failed_trades = failed
            db.flush()
        
        return run
    
    @staticmethod
    def complete(db: Session, run_id: uuid.UUID) -> Optional[BacktestRun]:
        """Marcar run como completado"""
        run = db.query(BacktestRun).filter(
            BacktestRun.run_id == run_id
        ).first()
        
        if run:
            run.status = 'COMPLETED'
            run.completed_at = datetime.utcnow()
            if run.started_at:
                duration = (run.completed_at - run.started_at).total_seconds()
                run.duration_seconds = int(duration)
            db.flush()
        
        return run
    
    @staticmethod
    def get_active(db: Session) -> List[BacktestRun]:
        """Obtener runs activos"""
        return db.query(BacktestRun).filter(
            BacktestRun.status == 'RUNNING'
        ).all()

class AgentLogRepository:
    """Repository para logs de agentes"""
    
    @staticmethod
    def log(
        db: Session,
        agent_name: str,
        level: str,
        message: str,
        context: Optional[Dict[str, Any]] = None
    ) -> AgentLog:
        """Crear log entry"""
        log = AgentLog(
            agent_name=agent_name,
            log_level=level,
            message=message,
            context=context
        )
        db.add(log)
        db.flush()
        return log
    
    @staticmethod
    def get_by_agent(
        db: Session,
        agent_name: str,
        limit: int = 100
    ) -> List[AgentLog]:
        """Obtener logs de un agente"""
        return db.query(AgentLog).filter(
            AgentLog.agent_name == agent_name
        ).order_by(desc(AgentLog.timestamp)).limit(limit).all()
    
    @staticmethod
    def get_errors(db: Session, limit: int = 100) -> List[AgentLog]:
        """Obtener logs de errores"""
        return db.query(AgentLog).filter(
            AgentLog.log_level.in_(['ERROR', 'CRITICAL'])
        ).order_by(desc(AgentLog.timestamp)).limit(limit).all()


================================================================================
# FILE: src/execution/broker.py
================================================================================

from abc import ABC, abstractmethod
from dataclasses import dataclass
from decimal import Decimal
from enum import Enum
from typing import List, Optional, Protocol

class OrderSide(Enum):
    BUY = "BUY"
    SELL = "SELL"

class OrderType(Enum):
    MARKET = "MARKET"
    LIMIT = "LIMIT"

@dataclass(frozen=True)
class OrderRequest:
    symbol: str
    side: OrderSide
    type: OrderType
    quantity: Decimal
    price: Optional[Decimal] = None
    stop_loss: Optional[Decimal] = None
    take_profit: Optional[Decimal] = None
    metadata: Optional[dict] = None

@dataclass(frozen=True)
class OrderResult:
    order_id: str
    status: str
    filled_price: Optional[Decimal]
    filled_quantity: Decimal

@dataclass(frozen=True)
class Position:
    symbol: str
    side: OrderSide
    quantity: Decimal
    entry_price: Decimal
    unrealized_pnl: Optional[Decimal] = None
    stop_loss: Optional[Decimal] = None
    take_profit: Optional[Decimal] = None
    metadata: Optional[dict] = None

class Broker(ABC):
    """Abstract interface for Broker implementations (Paper, Live, Backtest)."""
    
    @abstractmethod
    def get_balance(self) -> Decimal:
        """Returns current account balance."""
        pass

    @abstractmethod
    def place_order(self, order: OrderRequest) -> OrderResult:
        """Places a new order."""
        pass

    @abstractmethod
    def get_positions(self) -> List[Position]:
        """Returns list of open positions."""
        pass

    @abstractmethod
    def cancel_order(self, order_id: str) -> bool:
        """Cancels an existing order."""
        pass

    @abstractmethod
    def get_current_drawdown_pct(self) -> Decimal:
        """Returns current drawdown percentage (0.0 to 1.0)."""
        pass

    @abstractmethod
    def get_open_risk(self) -> Decimal:
        """Returns total open risk amount (Sum of |Entry-SL|*Qty)."""
        pass


================================================================================
# FILE: src/execution/executor.py
================================================================================

from dataclasses import dataclass
from decimal import Decimal
from typing import Optional, Any
from .broker import Broker, OrderRequest, OrderResult, OrderSide, OrderType
from .risk import RiskManager

@dataclass(frozen=True)
class TradeSignal:
    """Strategy Signal converted to Execution Request."""
    symbol: str
    side: OrderSide
    entry_price: Decimal
    stop_loss: Decimal
    take_profit: Decimal
    confidence: Optional[float] = None
    metadata: Optional[dict] = None

class TradeExecutor:
    def __init__(self, broker: Broker, risk_manager: RiskManager, strategy: Any = None):
        self.broker = broker
        self.risk_manager = risk_manager
        self.strategy = strategy

    def process_candle(self, candle: Any, market: Any, timeframe: Any):
        """Standard V3 loop: update broker -> check signal -> execute."""
        # 1. Update simulation/broker stops
        if hasattr(self.broker, 'update_positions'):
            self.broker.update_positions(candle.close)
            
        # 2. If no positions, look for new trades
        if not self.broker.get_positions() and self.strategy:
            signal = self.strategy.analyze(market, timeframe)
            if signal:
                self.execute_trade(signal)

    def execute_trade(self, signal: TradeSignal) -> Optional[OrderResult]:
        """
        Orchestrates trade execution:
        1. Get available balance.
        2. Calculate position size based on risk.
        3. Place order with broker.
        """
        balance = self.broker.get_balance()
        
        # Risk Overlay metrics
        current_open_risk = self.broker.get_open_risk()
        current_dd_pct = self.broker.get_current_drawdown_pct()
        
        try:
            quantity = self.risk_manager.calculate_position_size(
                account_balance=balance,
                entry_price=signal.entry_price,
                stop_loss=signal.stop_loss,
                current_open_risk=current_open_risk,
                current_drawdown_pct=current_dd_pct
            )
        except ValueError as e:
            # Handle invalid stops or zero division
            print(f"Risk Calculation Error: {e}")
            return None
            
        if quantity <= 0:
            print("Calculated Quantity is 0. Trade Aborted.")
            return None
            
        # Create Order Request
        order_req = OrderRequest(
            symbol=signal.symbol,
            side=signal.side,
            type=OrderType.LIMIT, # Assuming Limit Entry for now from Signal
            quantity=quantity,
            price=signal.entry_price,
            stop_loss=signal.stop_loss,
            take_profit=signal.take_profit,
            metadata=signal.metadata
        )
        
        # Place Order
        result = self.broker.place_order(order_req)
        return result


================================================================================
# FILE: src/execution/risk.py
================================================================================

from dataclasses import dataclass
from decimal import Decimal
from typing import Optional

@dataclass(frozen=True)
class RiskConfig:
    risk_percentage: Decimal = Decimal("0.01") # 1% default
    max_portfolio_risk: Optional[Decimal] = None
    use_dd_scaling: bool = False

class RiskManager:
    def __init__(self, config: RiskConfig):
        self.config = config

    def calculate_position_size(
        self, 
        account_balance: Decimal, 
        entry_price: Decimal, 
        stop_loss: Decimal,
        current_open_risk: Decimal = Decimal("0"),
        current_drawdown_pct: Decimal = Decimal("0")
    ) -> Decimal:
        """
        Calculates position size based on fixed % risk of account balance.
        Formula: Qty = (Balance * Risk%) / |Entry - SL|
        
        Applies Risk Overlay:
        1. Portfolio Heat: Caps total risk exposure if max_portfolio_risk is set.
        2. DD Scaling: Reduces risk during drawdowns if use_dd_scaling is True.
        """
        # Auto-cast for robustness
        current_open_risk = Decimal(str(current_open_risk))
        current_drawdown_pct = Decimal(str(current_drawdown_pct))

        if entry_price == stop_loss:
            raise ValueError("Stop Loss cannot be equal to Entry")
            
        risk_amount = account_balance * self.config.risk_percentage
        
        # 1. Drawdown Scaling
        if self.config.use_dd_scaling and current_drawdown_pct > 0:
            # Formula: multiplier = max(0.5, 1.0 - (dd * 2))
            multiplier = max(Decimal("0.5"), Decimal("1.0") - (current_drawdown_pct * Decimal("2.0")))
            risk_amount *= multiplier
            
        # 2. Portfolio Heat
        if self.config.max_portfolio_risk:
            max_risk_amt = account_balance * self.config.max_portfolio_risk
            available_risk = max_risk_amt - current_open_risk
            
            if available_risk <= 0:
                return Decimal("0")
            
            if risk_amount > available_risk:
                risk_amount = available_risk
        
        price_diff = abs(entry_price - stop_loss)
        
        quantity = risk_amount / price_diff
        
        # Rounding? For crypto usually 8 decimals for BTC, but exchange dependent.
        # For now, let's keep high precision or normalize to reasonable standard (e.g. 6 places).
        # TJR Strategy focuses on precision.
        
        return quantity


================================================================================
# FILE: src/ml/analyzer.py
================================================================================

# src/ml/analyzer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, accuracy_score
import joblib
from pathlib import Path

from src.ml.features import FeatureExtractor

class PatternAnalyzer:
    """
    Motor de Machine Learning para analizar patrones de mercado.
    Entrena modelos para predecir el resultado de trades basados en market features.
    """
    
    def __init__(self, model_path: str = "data/models/rf_model_v1.pkl"):
        self.model_path = Path(model_path)
        self.extractor = FeatureExtractor()
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_leaf=20,
            random_state=42,
            n_jobs=-1
        )
        self.is_trained = False
        self._load_model()

    def _load_model(self):
        if self.model_path.exists():
            try:
                self.model = joblib.load(self.model_path)
                self.is_trained = True
                print(f"Modelo cargado desde {self.model_path}")
            except Exception as e:
                print(f"Error cargando modelo: {e}")

    def prepare_data(self, candles_df: pd.DataFrame, trades_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Prepara dataset uniendo features de velas con etiquetas de trades.
        Args:
            candles_df: OHLCV data
            trades_df: DataFrame con trades hist√≥ricos (debe tener timestamp de entrada y resultado)
        """
        # 1. Extraer features t√©cnicas
        features_df = self.extractor.add_all_features(candles_df)
        
        # 2. Etiquetado (Labeling)
        # Asumimos que trades_df tiene 'entry_time' y 'result' ('WIN'/'LOSS')
        # Alineamos por timestamp. Como el trade ocurre AL CIERRE de la vela o en la vela siguiente,
        # usaremos features de la vela PREVIA o ACTUAL a la entrada.
        # Simplificaci√≥n: Features de la vela donde el timestamp coincide con entry_time.
        
        # Asegurar tipos de timestamp
        features_df.index = pd.to_datetime(features_df['timestamp']) if 'timestamp' in features_df.columns else features_df.index
        trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'])
        
        # Merge asof (cercano) o exacto
        # Unimos trades con features. Usamos features conocidas AL MOMENTO de la entrada.
        # Si trade timestamp es 12:00, usamos features de vela de las 12:00 (si es cierre) o 08:00 (si es open).
        # Asumiremos datos de cierre.
        
        # Requerimos que las features tengan √≠ndice por tiempo
        merged = pd.merge_asof(
            trades_df.sort_values('timestamp'),
            features_df.sort_index(),
            left_on='timestamp',
            right_index=True,
            direction='backward', # Features anteriores o iguales al trade
            tolerance=pd.Timedelta('4h') # Max diferencia permitida
        )
        
        merged = merged.dropna()
        
        # Crear target: 1 si WIN, 0 si LOSS
        y = (merged['result'] == 'WIN').astype(int)
        
        # Seleccionar solo columnas num√©ricas de features
        exclude_cols = ['timestamp', 'pair', 'side', 'result', 'profit_loss', 'entry_price', 'exit_price', 'stop_loss', 'take_profit', 'backtest_run_id', 'id', 'market_state', 'strategy_version', 'profit_loss_pct', 'risk_reward', 'worker_id', 'entry_time']
        
        # Filtrar columnas que vienen de features_df
        feature_cols = [c for c in features_df.columns if c in merged.columns and c not in exclude_cols]
        X = merged[feature_cols].select_dtypes(include=[np.number])
        
        return X, y

    def train(self, candles_df: pd.DataFrame, trades_df: pd.DataFrame) -> Dict[str, float]:
        """Entrena el modelo con nuevos datos."""
        X, y = self.prepare_data(candles_df, trades_df)
        
        if len(X) < 50:
            return {'error': 'Insufficient data (<50 samples)'}
            
        # Split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
        
        # Train
        self.model.fit(X_train, y_train)
        self.is_trained = True
        
        # Evaluacion
        preds = self.model.predict(X_test)
        metrics = {
            'accuracy': accuracy_score(y_test, preds),
            'precision': precision_score(y_test, preds, zero_division=0),
            'recall': recall_score(y_test, preds, zero_division=0),
            'samples': len(X)
        }
        
        # Guardar
        self.model_path.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(self.model, self.model_path)
        
        return metrics

    def predict_proba(self, candle_features: pd.DataFrame) -> float:
        """Predice probabilidad de WIN para el estado actual del mercado."""
        if not self.is_trained:
            return 0.5 # Neutral
            
        # Asegurarse de tener solo las columnas usadas en entrenamiento
        # (Esto requerir√≠a guardar feature_names, para V1 asumimos consistencia en FeatureExtractor)
        try:
            # Filtrar columnas no num√©ricas o extra√±as
            X = candle_features.select_dtypes(include=[np.number])
            # Si hay columnas extra o faltantes vs entrenamiento, RF avisar√°. 
            # En V2 robusteceremos feature alignment.
            
            # Solo tomamos la √∫ltima fila
            last_row = X.iloc[[-1]] 
            prob = self.model.predict_proba(last_row)[0][1] # Probabilidad de clase 1 (WIN)
            return prob
        except Exception as e:
            print(f"Error predicci√≥n: {e}")
            return 0.5


================================================================================
# FILE: src/ml/features.py
================================================================================

# src/ml/features.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any

class FeatureExtractor:
    """
    Motor de ingenier√≠a de caracter√≠sticas para ML en trading.
    Calcula indicadores t√©cnicos y m√©tricas de price action vectorizadas.
    """
    
    def __init__(self, include_patterns: bool = True):
        self.include_patterns = include_patterns

    def add_all_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Aplica todos los indicadores t√©cnicos y caracter√≠sticas al DataFrame.
        Espera columnas: open, high, low, close, volume.
        """
        df = df.copy()
        
        # Validaci√≥n b√°sica de columnas
        required = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required):
            # Intentar normalizar nombres si vienen en may√∫sculas
            df.columns = [c.lower() for c in df.columns]
            if not all(col in df.columns for col in required):
                raise ValueError(f"DataFrame must contain columns: {required}")
        
        # Conversi√≥n a tipos num√©ricos por si acaso
        for col in required:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        # 1. Indicadores de Tendencia
        df = self.add_trend_features(df)
        
        # 2. Indicadores de Momento
        df = self.add_momentum_features(df)
        
        # 3. Indicadores de Volatilidad
        df = self.add_volatility_features(df)
        
        # 4. Indicadores de Volumen
        df = self.add_volume_features(df)
        
        # 5. Price Action Features
        df = self.add_price_action_features(df)
        
        # Limpieza de NaN generados por ventanas rodantes
        df = df.dropna()
        
        return df

    def add_trend_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """EMAs, SMAs, Distancia a medias."""
        # EMAs
        df['ema_9'] = df['close'].ewm(span=9, adjust=False).mean()
        df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()
        df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()
        df['ema_200'] = df['close'].ewm(span=200, adjust=False).mean()
        
        # Tendencia basada en cruces relativos
        df['trend_short'] = np.where(df['ema_9'] > df['ema_20'], 1, -1)
        df['trend_medium'] = np.where(df['ema_20'] > df['ema_50'], 1, -1)
        df['trend_long'] = np.where(df['ema_50'] > df['ema_200'], 1, -1)
        
        # Distancia porcentual a la EMA 200 (Mean Reversion proxy)
        df['dist_ema_200'] = (df['close'] - df['ema_200']) / df['ema_200'] * 100
        
        return df

    def add_momentum_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """RSI, MACD."""
        # RSI 14
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi_14'] = 100 - (100 / (1 + rs))
        df['rsi_14'] = df['rsi_14'].fillna(50) # Fill inicial
        
        # MACD (12, 26, 9)
        ema_12 = df['close'].ewm(span=12, adjust=False).mean()
        ema_26 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        return df

    def add_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ATR, Bollinger Bands."""
        # ATR 14
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        df['atr_14'] = true_range.rolling(window=14).mean()
        
        # Bollinger Bands 20, 2
        sma_20 = df['close'].rolling(window=20).mean()
        std_20 = df['close'].rolling(window=20).std()
        df['bb_upper'] = sma_20 + (std_20 * 2)
        df['bb_lower'] = sma_20 - (std_20 * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / sma_20
        
        # Posici√≥n relativa en BB (0 a 1)
        df['bb_pos'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        return df

    def add_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Volume Ratio, OBV."""
        # Volume SMA
        df['vol_sma_20'] = df['volume'].rolling(window=20).mean()
        df['vol_ratio'] = df['volume'] / df['vol_sma_20'] # >1 significa volumen alto
        
        # OBV (On-Balance Volume)
        df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()
        
        return df

    def add_price_action_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cuerpo de vela, mechas, gaps."""
        # Tama√±o del cuerpo y mechas
        df['body_size'] = np.abs(df['close'] - df['open'])
        df['upper_wick'] = df['high'] - np.maximum(df['close'], df['open'])
        df['lower_wick'] = np.minimum(df['close'], df['open']) - df['low']
        
        # Ratio Cuerpo/Total (Fuerza de la vela)
        df['total_range'] = df['high'] - df['low']
        df['body_ratio'] = np.where(df['total_range'] > 0, df['body_size'] / df['total_range'], 0)
        
        # Retornos logar√≠tmicos
        df['log_ret'] = np.log(df['close'] / df['close'].shift(1))
        
        return df


================================================================================
# FILE: src/optimization/analyzer.py
================================================================================

import pandas as pd
from pathlib import Path
from typing import List, Dict

class ResultAnalyzer:
    def __init__(self, results_file: Path):
        self.results_file = results_file
        self.df = pd.DataFrame()

    def load(self):
        if self.results_file.exists():
            self.df = pd.read_csv(self.results_file)
            
    def generate_summary(self) -> str:
        if self.df.empty:
            return "No results found."
            
        # Ensure numeric types
        numeric_cols = ['net_profit', 'total_trades', 'win_rate', 'max_drawdown', 'fees_paid']
        for col in numeric_cols:
            if col in self.df.columns:
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
        
        # 1. Top 10 by Net Profit
        top_10 = self.df.sort_values(by='net_profit', ascending=False).head(10)
        
        # 2. Best by Timeframe
        best_by_tf = self.df.loc[self.df.groupby('timeframe')['net_profit'].idxmax()]
        
        # 3. Best by Pair (if multiple pairs)
        best_by_pair = self.df.loc[self.df.groupby('pair')['net_profit'].idxmax()]
        
        summary = []
        summary.append("=== OPTIMIZATION SUMMARY ===\n")
        summary.append(f"Total Configurations: {len(self.df)}")
        summary.append(f"Profitable Configs: {len(self.df[self.df['net_profit'] > 0])} ({len(self.df[self.df['net_profit'] > 0])/len(self.df)*100:.2f}%)")
        
        summary.append("\n--- TOP 10 CONFIGURATIONS ---")
        summary.append(top_10[['config_id', 'timeframe', 'pair', 'stop_loss', 'net_profit', 'total_trades']].to_string(index=False))
        
        summary.append("\n\n--- BEST BY TIMEFRAME ---")
        summary.append(best_by_tf[['timeframe', 'config_id', 'net_profit', 'win_rate', 'total_trades']].to_string(index=False))
        
        return "\n".join(summary)

    def save_summary(self, output_file: Path):
        report = self.generate_summary()
        with open(output_file, 'w') as f:
            f.write(report)
        print(f"Summary saved to {output_file}")


================================================================================
# FILE: src/optimization/constraints.py
================================================================================

"""
Constraint projection para parameter space.
Spec: Codex PARTE 3 - Constraints
"""
from typing import Dict, Any
from src.optimization.param_space import ParamSpace, ParamType


def project_constraints(
    params: Dict[str, Any],
    space: ParamSpace
) -> Dict[str, Any]:
    """
    Proyecta par√°metros al espacio v√°lido aplicando constraints.
    
    Constraints seg√∫n Codex PARTE 3:
    1. Clip todos los params a [min, max]
    2. adx_sideways_threshold < adx_trend_threshold
    3. atr_low_mult < 1.00 (ya garantizado por max=0.85)
    
    Args:
        params: Dict de par√°metros (pueden ser inv√°lidos)
        space: ParamSpace con definiciones
        
    Returns:
        Dict de par√°metros proyectados (v√°lidos)
    """
    # Paso 1: Clip todos los params a rangos
    projected = params.copy()
    
    for name, param in space.params.items():
        if name in projected:
            val = projected[name]
            # Clip bounds
            val = max(param.min_value, min(val, param.max_value))
            
            # Enforce types
            if param.param_type == ParamType.INT:
                val = int(val)
            
            projected[name] = val
            
    # Paso 2: Arreglar ADX constraint (adx_sideways_threshold < adx_trend_threshold)
    # Sideways range: [10, 22]
    # Trend range: [20, 35]
    
    if "adx_sideways_threshold" in projected and "adx_trend_threshold" in projected:
        sideways = projected["adx_sideways_threshold"]
        trend = projected["adx_trend_threshold"]
        
        if sideways >= trend:
            # Fix: Ensure sideways is strictly less than trend
            # Prefer lowering sideways if possible, or raising trend if sideways is already at min?
            # Given trend min is 20, and sideways valid range is [10, 22].
            # If trend is 20, sideways must be < 20. Max valid sideways is 19.
            # So `min(sideways, trend - 1)` should always work and be valid > 10
            # since trend >= 20 => trend - 1 >= 19 >= 10.
            
            new_sideways = min(sideways, trend - 1)
            projected["adx_sideways_threshold"] = new_sideways
            
    return projected


================================================================================
# FILE: src/optimization/engine.py
================================================================================

import os
import glob
import json
import itertools
import time
import concurrent.futures
from decimal import Decimal
from typing import List, Iterator, Dict, Any
from pathlib import Path
from tqdm import tqdm

from .types import OptimizerConfig, TestConfig, BacktestResult
from core.timeframe import Timeframe
from core.market import MarketState
from utils.data_loader import load_binance_csv
from simulation.backtest import Backtester
from strategy.engine import TJRStrategy
from execution.executor import TradeExecutor
from execution.risk import RiskManager, RiskConfig
from simulation.broker import InMemoryBroker

class OptimizerEngine:
    def __init__(self, config: OptimizerConfig):
        self.config = config
        self.results: List[BacktestResult] = []
        self.checkpoint_file = Path("results/optimizer_checkpoint.json")
        self.completed_ids = set()

    def generate_configurations(self) -> Iterator[TestConfig]:
        """Cartesian product of all parameters."""
        i = 0
        for timeframe in self.config.timeframes:
            for pair in self.config.pairs:
                for sl in self.config.stop_losses:
                    for tp in self.config.take_profit_multiples:
                        for fee in self.config.fee_rates:
                            config_id = f"cfg_{i:04d}_{pair}_{timeframe}"
                            yield TestConfig(
                                id=config_id,
                                timeframe=timeframe,
                                pair=pair,
                                stop_loss=sl,
                                take_profit_r=tp,
                                fee_rate=fee
                            )
                            i += 1

    def load_checkpoint(self):
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r') as f:
                    data = json.load(f)
                    self.completed_ids = set(data.get("completed_ids", []))
                    # We would ideally load previous results too, but for MVPs often we just skip re-running 
                    # and assume results are already saved in a CSV. 
                    # For this implementation, let's assume we append to a CSV immediately.
            except Exception as e:
                print(f"Failed to load checkpoint: {e}")

    def save_checkpoint(self, results_batch: List[BacktestResult]):
        # Save IDs
        new_ids = {r.config_id for r in results_batch}
        self.completed_ids.update(new_ids)
        
        # Write checkpoint file
        with open(self.checkpoint_file, 'w') as f:
            json.dump({
                "completed_ids": list(self.completed_ids)
            }, f)
            
        # Append to CSV
        csv_file = Path("results/optimizer_results.csv")
        headers = [
            "config_id", "timeframe", "pair", "stop_loss", 
            "take_profit_r", "fee_rate", "total_trades", 
            "win_rate", "net_profit", "max_drawdown", "execution_time"
        ]
        
        write_header = not csv_file.exists()
        
        with open(csv_file, 'a') as f:
            if write_header:
                f.write(",".join(headers) + "\n")
            
            for r in results_batch:
                line = [
                    r.config_id, r.timeframe, r.pair, str(r.stop_loss),
                    str(r.take_profit_r), str(r.fee_rate), str(r.total_trades),
                    f"{r.win_rate:.4f}", f"{r.net_profit:.2f}", 
                    f"{r.max_drawdown:.2f}", f"{r.execution_time:.4f}"
                ]
                f.write(",".join(line) + "\n")

    def run(self):
        # Ensure results dir
        Path("results").mkdir(exist_ok=True)
        
        self.load_checkpoint()
        
        all_configs = list(self.generate_configurations())
        pending_configs = [c for c in all_configs if c.id not in self.completed_ids]
        
        print(f"Total Configs: {len(all_configs)}")
        print(f"Pending: {len(pending_configs)}")
        
        if not pending_configs:
            print("All configurations completed.")
            return

        batch_size = self.config.checkpoint_interval
        
        # Parallel Execution
        # We need to pass the Data Path to the worker so it can find files
        data_path = self.config.data_path
        
        with concurrent.futures.ProcessPoolExecutor() as executor:
            # We process in batches to save checkpoints periodically
            for i in range(0, len(pending_configs), batch_size):
                batch = pending_configs[i : i + batch_size]
                futures = {
                    executor.submit(execute_worker, config, data_path, self.config.initial_balance, self.config.risk_percent): config 
                    for config in batch
                }
                
                results_batch = []
                for future in tqdm(concurrent.futures.as_completed(futures), total=len(batch), desc=f"Batch {i//batch_size + 1}"):
                    try:
                        res = future.result()
                        results_batch.append(res)
                    except Exception as e:
                        print(f"Worker Error: {e}")
                
                self.save_checkpoint(results_batch)

# Static Worker Function (Must be outside class for multiprocessing pickle)
def execute_worker(config: TestConfig, data_path: str, initial_balance: Decimal, risk_percent: Decimal) -> BacktestResult:
    start_time = time.time()
    
    # 1. Load Data
    # Glob for all files matching Pair + Timeframe
    # Convention: {pair}-{timeframe}-*.csv
    # E.g. BTCUSDT-5m-2024-01.csv
    pattern = os.path.join(data_path, f"{config.pair}-{config.timeframe}-*.csv")
    files = sorted(glob.glob(pattern))
    
    if not files:
        # Fallback or Error
        # Return empty result
        return BacktestResult(
            config.id, config.timeframe, config.pair, config.stop_loss, config.take_profit_r, config.fee_rate,
            0, 0, 0, 0.0, Decimal(0), Decimal(0), Decimal(0), Decimal(0), Decimal(0), 0.0
        )
        
    # Map string timeframe to Enum
    tf_map = {
        "5m": Timeframe.M5,
        "15m": Timeframe.M15,
        "1h": Timeframe.H1,
        "4h": Timeframe.H4
    }
    tf_enum = tf_map.get(config.timeframe, Timeframe.M5) # Default/Fallback
    
    all_candles = []
    for f in files:
        month_candles = load_binance_csv(f, tf_enum)
        all_candles.extend(month_candles)
        
    # 2. Setup System
    broker = InMemoryBroker(balance=initial_balance, fee_rate=config.fee_rate) # Need to ensure InMemoryBroker accepts fee_rate constructor? It usually has fixed fee.
    # Wait, InMemoryBroker currently has fixed fee 0.1%. Need to update it?
    # Let's check InMemoryBroker. Using property logic might be needed or Constructor.
    # Assuming we modify InMemoryBroker or it already supports it.
    # Checking InMemoryBroker code... it has `calculate_fee` method but usually hardcoded 0.001.
    # FIX: We will update broker fee_rate after init if it's not in init.
    if hasattr(broker, 'fee_rate'):
         broker.fee_rate = config.fee_rate
    else:
         # Need to ensure Broker supports variable fees. For now let's monkey patch or assume we updated it.
         # Actually let's assume standard 0.001 for now or inject it.
         # For this specific task, if "fee_rates" is an optimization param, we MUST support it.
         # I'll update InMemoryBroker in a separate step or just set it: `broker.trading_fee = config.fee_rate` if public.
         pass
         
    # Risk
    risk = RiskManager(RiskConfig(risk_percent))
    
    # Strategy (The key injection point)
    strategy = TJRStrategy(
        fixed_stop_loss=config.stop_loss if config.stop_loss > 0 else None,
        take_profit_multiplier=Decimal(str(config.take_profit_r))
    )
    
    executor = TradeExecutor(broker, risk)
    backtester = Backtester()
    
    # 3. Run
    # Warning: We must pass the correct timeframe Enum to backtester run
    report = backtester.run(all_candles, broker, strategy, executor, tf_enum)
    
    duration = time.time() - start_time
    
    return BacktestResult(
        config_id=config.id,
        timeframe=config.timeframe,
        pair=config.pair,
        stop_loss=config.stop_loss,
        take_profit_r=config.take_profit_r,
        fee_rate=config.fee_rate,
        total_trades=report.total_trades,
        winning_trades=report.winning_trades,
        losing_trades=report.losing_trades,
        win_rate=report.win_rate,
        gross_profit=report.gross_profit,
        gross_loss=report.gross_loss,
        fees_paid=broker.total_fees_paid,
        net_profit=report.net_profit,
        max_drawdown=report.max_drawdown,
        execution_time=duration
    )


================================================================================
# FILE: src/optimization/fitness.py
================================================================================

"""
Fitness function para Walk-Forward Optimization.
Spec: Codex PARTE 1 - "Fitness function (fitness) EXACTA"
"""
from dataclasses import dataclass
from typing import Dict, Any
import math
from src.optimization.param_space import ParamSpace


@dataclass
class SegmentMetrics:
    """
    M√©tricas de un segmento de backtest (SubTrain o ValTrain).
    """
    trades: int
    return_pct: float          # Return como decimal (0.20 = 20%)
    maxdd: float               # MaxDD como decimal (0.10 = 10%)
    sharpe: float
    pf: float                  # Profit Factor
    gross_profit: float
    gross_loss: float


def calculate_score_segment(metrics: SegmentMetrics) -> float:
    """
    Calcula score de un segmento.
    
    Spec: Codex PARTE 1.2
    
    Formula:
        TradeFactor = min(1.0, trades / 30)
        Calmar = Return / max(MaxDD, 0.05)
        Score = TradeFactor * (0.60 * Calmar + 0.40 * Sharpe)
    
    Args:
        metrics: M√©tricas del segmento
        
    Returns:
        Score (puede ser negativo si Return o Sharpe negativos)
    """
    # 1. TradeFactor = min(1.0, trades / 30.0)
    trade_factor = min(1.0, metrics.trades / 30.0)
    
    # 2. MaxDD_safe = max(metrics.maxdd, 0.05)  # Floor de 5%
    maxdd_safe = max(float(metrics.maxdd), 0.05)
    
    # 3. Calmar = metrics.return_pct / MaxDD_safe
    calmar = float(metrics.return_pct) / maxdd_safe
    
    # 4. Score = TradeFactor * (0.60 * Calmar + 0.40 * metrics.sharpe)
    score = trade_factor * (0.60 * calmar + 0.40 * float(metrics.sharpe))
    
    return score


def calculate_overfit_penalty(
    metrics_sub: SegmentMetrics,
    metrics_val: SegmentMetrics
) -> float:
    """
    Calcula penalty por overfitting (degradaci√≥n Sub ‚Üí Val).
    
    Spec: Codex PARTE 1.2 - "Penalizaci√≥n de overfit (EXACTA)"
    
    Formula:
        PF_degradation = PF_val / max(PF_sub, 0.01)
        Sharpe_degradation = (Sharpe_val + 2.0) / max(Sharpe_sub + 2.0, 0.1)
        
        Penalty = 2.0 * max(0, 0.70 - PF_degradation) + 
                  1.0 * max(0, 0.75 - Sharpe_degradation)
    
    Args:
        metrics_sub: M√©tricas de SubTrain
        metrics_val: M√©tricas de ValTrain
        
    Returns:
        Penalty >= 0 (0 = sin overfitting, mayor = m√°s overfitting)
    """
    # 1. Manejar edge case: si gross_loss == 0, PF = 10.0 (assuming strict handling or pre-calculated)
    # The metrics input (SegmentMetrics) assumes PF is already calculated. 
    # Whatever value is passed in 'metrics.pf' is used.
    
    pf_sub = float(metrics_sub.pf)
    pf_val = float(metrics_val.pf)
    
    # 3. PF_degradation = PF_val / max(PF_sub, 0.01)
    pf_degradation = pf_val / max(pf_sub, 0.01)
    
    # 4. SharpeAdj(x) = x + 2.0
    # 5. Sharpe_degradation = SharpeAdj(Sharpe_val) / max(SharpeAdj(Sharpe_sub), 0.1)
    sharpe_adj_val = float(metrics_val.sharpe) + 2.0
    sharpe_adj_sub = float(metrics_sub.sharpe) + 2.0
    
    sharpe_degradation = sharpe_adj_val / max(sharpe_adj_sub, 0.1)
    
    # 6. Penalty por PF: 2.0 * max(0, 0.70 - PF_degradation)
    penalty_pf = 2.0 * max(0.0, 0.70 - pf_degradation)
    
    # 7. Penalty por Sharpe: 1.0 * max(0, 0.75 - Sharpe_degradation)
    penalty_sharpe = 1.0 * max(0.0, 0.75 - sharpe_degradation)
    
    # 8. Sumar ambas penalties
    return penalty_pf + penalty_sharpe


def calculate_reg_penalty(
    params: Dict[str, Any],
    param_space: ParamSpace
) -> float:
    """
    Calcula penalty por alejarse de defaults (regularizaci√≥n).
    
    Spec: Codex PARTE 1.2 - "Regularizaci√≥n hacia defaults (EXACTA)"
    
    Formula:
        L1_norm = sum_i | (p_i - p_default_i) / (p_max_i - p_min_i) |
        RegPenalty = 0.15 * L1_norm
    
    Args:
        params: Par√°metros actuales
        param_space: Espacio de par√°metros con defaults
        
    Returns:
        Penalty >= 0 (0 = en defaults, mayor = m√°s alejado)
    """
    # 1. defaults = param_space.get_defaults()
    defaults = param_space.get_defaults()
    l1_norm = 0.0
    
    # 3. Para cada param en params:
    for name, value in params.items():
        if name not in param_space.params:
            continue
            
        param_def = param_space.params[name]
        default_val = defaults[name]
        
        # diff = params[name] - defaults[name]
        diff = float(value) - float(default_val)
        
        # range = param_space.params[name].max_value - min_value
        rng = float(param_def.max_value) - float(param_def.min_value)
        if rng == 0:
            continue
            
        # normalized_diff = abs(diff / range)
        normalized_diff = abs(diff / rng)
        
        # L1_norm += normalized_diff
        l1_norm += normalized_diff
        
    # 4. RegPenalty = 0.15 * L1_norm
    return 0.15 * l1_norm


def calculate_fitness(
    params: Dict[str, Any],
    metrics_sub: SegmentMetrics,
    metrics_val: SegmentMetrics,
    param_space: ParamSpace
) -> float:
    """
    Calcula fitness final de un candidato.
    
    Spec: Codex PARTE 1.2 - "Fitness final (a maximizar)"
    
    Formula:
        Fitness = 0.25*ScoreSub + 0.75*ScoreVal - OverfitPenalty - RegPenalty
    
    Hard failures (return -inf):
    - Si metrics_val.trades < 10
    - Si metrics_val.maxdd > 0.25
    - Si metrics_val.return_pct < -0.05
    
    Args:
        params: Par√°metros del candidato
        metrics_sub: M√©tricas de SubTrain
        metrics_val: M√©tricas de ValTrain
        param_space: Espacio de par√°metros
        
    Returns:
        Fitness (a maximizar). -inf si falla hard checks.
    """
    # 1. Hard checks (Codex PARTE 1.3):
    # Note: trades == 0 is handled by the gradual penalty below (factor = 0/10 = 0)
    # This avoids all-`-inf` populations that break GA optimization.
    if metrics_val.maxdd > 0.25:
        return float('-inf')
    if metrics_val.return_pct < -0.05:
        return float('-inf')
    
    # 2. Calcular ScoreSub = calculate_score_segment(metrics_sub)
    score_sub = calculate_score_segment(metrics_sub)
    
    # 3. Calcular ScoreVal = calculate_score_segment(metrics_val)
    score_val = calculate_score_segment(metrics_val)
    
    # 4. Calcular OverfitPenalty = calculate_overfit_penalty(metrics_sub, metrics_val)
    overfit_penalty = calculate_overfit_penalty(metrics_sub, metrics_val)
    
    # 5. Calcular RegPenalty = calculate_reg_penalty(params, param_space)
    reg_penalty = calculate_reg_penalty(params, param_space)
    
    # 6. Fitness = 0.25*ScoreSub + 0.75*ScoreVal - OverfitPenalty - RegPenalty
    fitness = (0.25 * score_sub) + (0.75 * score_val) - overfit_penalty - reg_penalty

    # New Gradual Penalty for Low Trades (Task 5)
    # Replaces previous hard check for trades < 10
    if metrics_val.trades < 10:
        trade_penalty_factor = metrics_val.trades / 10.0
        fitness *= trade_penalty_factor

    return fitness


================================================================================
# FILE: src/optimization/genetic_algorithm.py
================================================================================

"""
Genetic Algorithm para optimizaci√≥n de par√°metros.
Spec: Codex PARTE 2
"""
from dataclasses import dataclass, field
from typing import Dict, Any, List, Callable, Optional, Tuple
import random
import copy
import math
from src.optimization.param_space import ParamSpace, ParamType
from src.optimization.constraints import project_constraints


@dataclass
class Individual:
    """
    Individuo en la poblaci√≥n del GA.
    """
    params: Dict[str, Any]
    fitness: Optional[float] = None


@dataclass
class GAConfig:
    """
    Configuraci√≥n del Genetic Algorithm.
    Spec: Codex PARTE 2.1
    """
    population_size: int = 32
    num_generations: int = 8
    tournament_size: int = 3
    crossover_rate: float = 0.8
    mutation_rate: float = 0.15
    mutation_sigma_pct: float = 0.10
    elitism_count: int = 2
    early_stopping_generations: int = 3
    seed: Optional[int] = None


def population_initialize(
    param_space: ParamSpace,
    population_size: int,
    seed: Optional[int] = None
) -> List[Individual]:
    """
    Inicializa poblaci√≥n aleatoria.
    
    Spec: Codex PARTE 2.2
    
    Args:
        param_space: Espacio de par√°metros
        population_size: Tama√±o de poblaci√≥n
        seed: Random seed
        
    Returns:
        Lista de individuos con params aleatorios, fitness=None
    """
    if seed is not None:
        random.seed(seed)
        
    population = []
    
    for _ in range(population_size):
        # Generar params aleatorios
        # Nota: sample_random ya usa random, as√≠ que el seed global aplica
        params = param_space.sample_random(seed=None)  
        
        # Proyectar a espacio v√°lido (constraints)
        params = project_constraints(params, param_space)
        
        # Crear individuo
        ind = Individual(params=params, fitness=None)
        population.append(ind)
        
    return population


def tournament_selection(
    population: List[Individual],
    tournament_size: int,
    seed: Optional[int] = None
) -> Individual:
    """
    Selecci√≥n por torneo.
    
    Spec: Codex PARTE 2.3
    
    Elige tournament_size individuos al azar,
    retorna el de mejor fitness.
    
    Args:
        population: Poblaci√≥n actual
        tournament_size: N√∫mero de competidores
        seed: Random seed
        
    Returns:
        Individuo ganador del torneo
    """
    if seed is not None:
        random.seed(seed)
        
    # Validar que todos tengan fitness calculado
    # (En teor√≠a siempre deber√≠an tenerlo en este punto)
    
    # Seleccionar competidores al azar
    competitors = random.sample(population, tournament_size)
    
    # Ganador es el que tiene mayor fitness
    # Nota: Si fitness es None, fallar√°. Asumimos fitness v√°lido.
    winner = max(competitors, key=lambda ind: ind.fitness if ind.fitness is not None else float('-inf'))
    
    return winner


def crossover_uniform(
    parent1: Individual,
    parent2: Individual,
    param_space: ParamSpace,
    seed: Optional[int] = None
) -> Individual:
    """
    Crossover uniforme.
    
    Spec: Codex PARTE 2.4
    
    Para cada par√°metro, elige aleatoriamente de parent1 o parent2.
    
    Args:
        parent1: Padre 1
        parent2: Padre 2
        param_space: Espacio de par√°metros
        seed: Random seed
        
    Returns:
        Hijo con genes mezclados, fitness=None
    """
    if seed is not None:
        random.seed(seed)
        
    child_params = {}
    
    for param_name in param_space.params:
        if random.random() < 0.5:
            child_params[param_name] = parent1.params[param_name]
        else:
            child_params[param_name] = parent2.params[param_name]
            
    # Asegurar constraints (aunque si vienen de padres v√°lidos, deber√≠an serlo)
    child_params = project_constraints(child_params, param_space)
    
    return Individual(params=child_params, fitness=None)


def mutate_gaussian(
    individual: Individual,
    param_space: ParamSpace,
    mutation_rate: float,
    sigma_pct: float,
    seed: Optional[int] = None
) -> Individual:
    """
    Mutaci√≥n gaussiana.
    
    Spec: Codex PARTE 2.5
    
    Para cada par√°metro:
    - Con probabilidad mutation_rate:
      - A√±adir ruido gaussiano: N(0, sigma)
      - sigma = sigma_pct * (max - min)
    
    Args:
        individual: Individuo a mutar
        param_space: Espacio de par√°metros
        mutation_rate: Probabilidad de mutar cada param
        sigma_pct: Desviaci√≥n est√°ndar como % del rango
        seed: Random seed
        
    Returns:
        Individuo mutado, fitness=None
    """
    if seed is not None:
        random.seed(seed)
        
    mutated_params = copy.deepcopy(individual.params)
    
    for param_name, param_def in param_space.params.items():
        if param_name not in mutated_params:
            continue
            
        if random.random() < mutation_rate:
            # Calcular sigma
            rng = float(param_def.max_value) - float(param_def.min_value)
            sigma = sigma_pct * rng
            
            # Ruido gaussiano
            noise = random.gauss(0, sigma)
            
            # Aplicar ruido
            mutated_params[param_name] += noise
            
    # Proyectar para clip y constraints (importante tras mutaci√≥n)
    mutated_params = project_constraints(mutated_params, param_space)
    
    return Individual(params=mutated_params, fitness=None)


class GeneticAlgorithm:
    """
    Genetic Algorithm para optimizaci√≥n de hiperpar√°metros.
    
    Spec: Codex PARTE 2
    """
    
    def __init__(
        self,
        param_space: ParamSpace,
        config: GAConfig,
        fitness_function: Callable[[Dict[str, Any]], float]
    ):
        """
        Args:
            param_space: Espacio de par√°metros
            config: Configuraci√≥n del GA
            fitness_function: Funci√≥n que dado params retorna fitness
        """
        self.param_space = param_space
        self.config = config
        self.fitness_function = fitness_function
        
        if config.seed is not None:
            random.seed(config.seed)
    
    def evaluate_individual(self, individual: Individual) -> None:
        """
        Eval√∫a fitness de un individuo (in-place).
        
        Args:
            individual: Individuo a evaluar
        """
        # Si ya tiene fitness, podr√≠amos saltar, pero por ahora re-calculamos
        # para asegurar consistencia si la funci√≥n es estoc√°stica (usualmente no)
        fitness = self.fitness_function(individual.params)
        individual.fitness = fitness
    
    def optimize(self) -> Tuple[Individual, List[Dict]]:
        """
        Ejecuta el GA y retorna el mejor individuo encontrado.
        
        Spec: Codex PARTE 2.6
        
        Returns:
            (best_individual, history)
            
            history: List de dicts con info de cada generaci√≥n:
                {
                    "gen": int,
                    "best_fitness": float,
                    "avg_fitness": float,
                    "evaluations": int
                }
        """
        history = []
        
        # 1. Inicializaci√≥n (Gen 0)
        population = population_initialize(
            self.param_space,
            self.config.population_size,
            seed=self.config.seed
        )
        
        # 2. Evaluar poblaci√≥n inicial
        for ind in population:
            self.evaluate_individual(ind)
            
        evaluations_count = len(population)
        
        # Stats Gen 0
        current_best = max(population, key=lambda ind: ind.fitness if ind.fitness is not None else float('-inf'))
        finite_fitnesses = [ind.fitness for ind in population if ind.fitness is not None and math.isfinite(ind.fitness)]
        avg_fitness = sum(finite_fitnesses) / len(finite_fitnesses) if finite_fitnesses else float('-inf')
        
        best_ever = copy.deepcopy(current_best)
        
        history.append({
            "gen": 0,
            "best_fitness": current_best.fitness,
            "avg_fitness": avg_fitness,
            "evaluations": evaluations_count
        })
        
        best_str = f"{current_best.fitness:.4f}" if math.isfinite(current_best.fitness) else "-inf"
        avg_str = f"{avg_fitness:.4f}" if math.isfinite(avg_fitness) else "-inf"
        print(f"Generation 0: Best Fitness={best_str}, Avg={avg_str}")
        
        generations_without_improvement = 0
        
        # Loop de generaciones (1 a N-1)
        for gen in range(1, self.config.num_generations):
            
            # C) Elitism (usamos pop anterior)
            sorted_pop = sorted(
                population, 
                key=lambda ind: ind.fitness if ind.fitness is not None else float('-inf'), 
                reverse=True
            )
            
            elites = sorted_pop[:self.config.elitism_count]
            # Deepcopy para seguridad
            elites = [copy.deepcopy(elite) for elite in elites]
            
            # D) Offspring
            offspring = []
            needed = self.config.population_size - len(elites)
            
            while len(offspring) < needed:
                # Selection from PREVIOUS population
                parent1 = tournament_selection(population, self.config.tournament_size)
                parent2 = tournament_selection(population, self.config.tournament_size)
                
                # Crossover
                if random.random() < self.config.crossover_rate:
                    child = crossover_uniform(parent1, parent2, self.param_space)
                else:
                    child = Individual(params=copy.deepcopy(parent1.params), fitness=None)
                
                # Mutation
                child = mutate_gaussian(
                    child, 
                    self.param_space,
                    self.config.mutation_rate,
                    self.config.mutation_sigma_pct
                )
                
                offspring.append(child)
            
            # E) Evaluar Offspring
            for child in offspring:
                self.evaluate_individual(child)
                evaluations_count += 1
                
            # F) Nueva poblaci√≥n
            population = elites + offspring
            
            # A) Stats de la NUEVA poblaci√≥n
            current_best = max(population, key=lambda ind: ind.fitness if ind.fitness is not None else float('-inf'))
            finite_fitnesses = [ind.fitness for ind in population if ind.fitness is not None and math.isfinite(ind.fitness)]
            avg_fitness = sum(finite_fitnesses) / len(finite_fitnesses) if finite_fitnesses else float('-inf')
            
            history.append({
                "gen": gen,
                "best_fitness": current_best.fitness,
                "avg_fitness": avg_fitness,
                "evaluations": evaluations_count
            })
            
            best_str = f"{current_best.fitness:.4f}" if math.isfinite(current_best.fitness) else "-inf"
            avg_str = f"{avg_fitness:.4f}" if math.isfinite(avg_fitness) else "-inf"
            print(f"Generation {gen}: Best Fitness={best_str}, Avg={avg_str}")
            
            # B) Early Stopping Check & Updates
            if current_best.fitness > best_ever.fitness:
                best_ever = copy.deepcopy(current_best)
                generations_without_improvement = 0
            else:
                generations_without_improvement += 1
                
            if generations_without_improvement >= self.config.early_stopping_generations:
                # Stop early
                break
            
        return best_ever, history


================================================================================
# FILE: src/optimization/param_space.py
================================================================================

"""
Parameter space definition para WFO.
Spec: Codex PARTE 3
"""
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Any
import random


class ParamType(Enum):
    """Tipo de par√°metro."""
    FLOAT = "float"
    INT = "int"


@dataclass
class ParamDef:
    """
    Definici√≥n de un par√°metro optimizable.
    """
    name: str
    param_type: ParamType
    min_value: float
    max_value: float
    default_value: float
    description: str = ""


class ParamSpace:
    """
    Espacio de par√°metros del sistema MSC v3.
    Define todos los params optimizables, rangos, y defaults.
    """
    
    def __init__(self, params: Dict[str, ParamDef]):
        """
        Args:
            params: Dict de nombre ‚Üí ParamDef
        """
        self.params = params
    
    def sample_random(self, seed: int = None) -> Dict[str, Any]:
        """
        Genera un set de par√°metros aleatorios dentro de rangos.
        
        Args:
            seed: Random seed para reproducibilidad
            
        Returns:
            Dict de nombre ‚Üí valor
        """
        if seed is not None:
            random.seed(seed)
            
        result = {}
        for name, param in self.params.items():
            if param.param_type == ParamType.FLOAT:
                val = random.uniform(param.min_value, param.max_value)
                result[name] = round(val, 2)  # Rounding for cleanliness, though not strictly required
            else:
                val = random.randint(int(param.min_value), int(param.max_value))
                result[name] = val
        return result
    
    def get_defaults(self) -> Dict[str, Any]:
        """
        Retorna valores default de todos los par√°metros.
        """
        return {name: param.default_value for name, param in self.params.items()}


def get_default_param_space() -> ParamSpace:
    """
    Crea el param space seg√∫n Codex PARTE 3.1
    
    Returns:
        ParamSpace con 13 par√°metros optimizables
    """
    params = {}
    
    # helper to add params easily
    def add_param(name, p_type, min_v, max_v, default):
        params[name] = ParamDef(name, p_type, min_v, max_v, default)

    # A) Pure Alpha global multipliers (5 params)
    add_param("g_ob_quality", ParamType.FLOAT, 0.50, 2.00, 1.0)
    add_param("g_momentum", ParamType.FLOAT, 0.50, 2.00, 1.0)
    add_param("g_volatility", ParamType.FLOAT, 0.50, 2.00, 1.0)
    add_param("g_liquidity", ParamType.FLOAT, 0.50, 2.00, 1.0)
    add_param("g_ml_confidence", ParamType.FLOAT, 0.00, 1.50, 1.0)
    
    # B) Alpha threshold (1 param)
    add_param("alpha_threshold", ParamType.FLOAT, 0.45, 0.75, 0.60)
    
    # C) Classifier thresholds (4 params)
    add_param("adx_trend_threshold", ParamType.INT, 20, 35, 25)
    add_param("adx_sideways_threshold", ParamType.INT, 10, 22, 15)
    add_param("atr_high_mult", ParamType.FLOAT, 1.20, 2.00, 1.50)
    add_param("atr_low_mult", ParamType.FLOAT, 0.45, 0.85, 0.65)
    
    # D) Execution/Risk (3 params)
    add_param("stop_loss_atr_mult", ParamType.FLOAT, 1.00, 3.50, 2.0)
    add_param("take_profit_r_mult", ParamType.FLOAT, 1.00, 4.00, 2.0)
    add_param("risk_per_trade_pct", ParamType.FLOAT, 0.25, 1.25, 1.0)
    
    return ParamSpace(params)


================================================================================
# FILE: src/optimization/types.py
================================================================================

from dataclasses import dataclass
from decimal import Decimal
from typing import List, Optional
from pathlib import Path

@dataclass(frozen=True)
class OptimizerConfig:
    timeframes: List[str]
    pairs: List[str]
    stop_losses: List[Decimal] # Fixed USD amount for SL (e.g. 500) or 0 for Structural
    take_profit_multiples: List[float]
    fee_rates: List[Decimal]
    initial_balance: Decimal
    risk_percent: Decimal
    data_path: str
    download_years: List[int]
    download_months: List[int]
    parallel: bool = True
    checkpoint_interval: int = 10

@dataclass(frozen=True)
class TestConfig:
    id: str
    timeframe: str
    pair: str
    stop_loss: Decimal
    take_profit_r: float
    fee_rate: Decimal

@dataclass(frozen=True)
class BacktestResult:
    config_id: str
    timeframe: str
    pair: str
    stop_loss: Decimal
    take_profit_r: float
    fee_rate: Decimal
    total_trades: int
    winning_trades: int
    losing_trades: int
    win_rate: float
    gross_profit: Decimal
    gross_loss: Decimal
    fees_paid: Decimal
    net_profit: Decimal
    max_drawdown: Decimal
    execution_time: float


================================================================================
# FILE: src/optimization/windows.py
================================================================================

"""
Window generation para Walk-Forward Optimization.
Spec: Codex PARTE 2 - Funci√≥n generate_windows()
"""
from dataclasses import dataclass
from typing import List
from datetime import datetime, timezone
from calendar import monthrange
from src.core.market import Candle


@dataclass
class WindowConfig:
    """
    Configuraci√≥n de ventanas WFO.
    Spec: Codex PARTE 7.1
    """
    train_months: int      # Ej: 4
    test_months: int       # Ej: 1
    step_months: int       # Ej: 1
    year: int              # Ej: 2024
    warmup_bars: int       # Ej: 240 para 4H


@dataclass
class Window:
    """
    Una ventana de WFO con train/test/warmup data.
    Spec: Codex PARTE 2 - Window dataclass
    """
    window_id: int
    label: str
    
    train_start_month: int
    train_end_month: int
    test_start_month: int
    test_end_month: int
    
    train_data: List[Candle]
    test_data: List[Candle]
    warmup_data: List[Candle]


def generate_windows(
    full_data: List[Candle],
    config: WindowConfig
) -> List[Window]:
    """
    Genera ventanas de WFO seg√∫n Codex PARTE 2.
    """
    if config.step_months != config.test_months:
        raise ValueError(
            f"step_months ({config.step_months}) debe == test_months ({config.test_months})"
        )
    
    if len(full_data) == 0:
        raise ValueError("full_data est√° vac√≠o")

    windows = []
    
    # Rango de meses total para el a√±o
    # El a√±o tiene 12 meses.
    # Necesitamos calcular cu√°ntas ventanas caben.
    # Empezamos en start_month = 1.
    # La √∫ltima ventana debe terminar su TEST en o antes del mes 12.
    # Test end month = train_start + train_months + test_months - 1
    # Example 4+1: Start=1 -> Train=1..4, Test=5. End=5.
    # Max End = 12.
    # Solve for max Start: Start + train + test - 1 <= 12
    # Start <= 12 - train - test + 1
    
    max_start_month = 12 - config.train_months - config.test_months + 1
    
    for i in range(max_start_month):
        window_id = i
        
        # Calcular meses
        # Nota: start_month es 1-based para la l√≥gica interna
        train_start_month = 1 + i * config.step_months
        train_end_month = train_start_month + config.train_months - 1
        
        test_start_month = train_end_month + 1
        test_end_month = test_start_month + config.test_months - 1
        
        # Labels
        # Format: Train:YYYY-MMtoYYYY-MM_Test:YYYY-MM
        # Helper para formato 02d
        def fmt_m(m): return f"{config.year}-{m:02d}"
        
        label = (
            f"Train:{fmt_m(train_start_month)}to{fmt_m(train_end_month)}_"
            f"Test:{fmt_m(test_start_month)}"
            # Nota: Si test_months > 1, el label podr√≠a necesitar ajuste, 
            # pero el spec dice "Test:YYYY-MM" para el bloque. 
            # Asumimos que si test > 1, indicamos el rango o solo el inicio?
            # El spec ejemplo es "Test:YYYY-MM". Si es 1 mes, es obvio.
            # Si son m√°s, mejor poner rango o seguir convenci√≥n. 
            # Codex 7.2 ejemplos son de 1 mes. 
            # Para test_months > 1, la spec es ambigua en el label exacto, 
            # pero el formato dado es fijo. Nos adherimos al formato del test:
            # "Test:YYYY-MM" (probablemente start chart).
            # Revisando el test `test_window_labels_correct` que escrib√≠, 
            # espera solo el mes de inicio si es 1 mes.
        )
        # Si test dura un mes, start==end.
        
        # Slice Train Data
        train_data = _slice_candles_by_month(
            full_data, config.year, train_start_month, train_end_month
        )
        
        # Slice Test Data
        test_data = _slice_candles_by_month(
            full_data, config.year, test_start_month, test_end_month
        )
        
        if not train_data:
             # Puede pasar si no hay datos para esos meses
             # Si es estricto, raise error. Si es flexible, warn.
             # Codex no especifica comportamiento ante data missing, 
             # pero WFO sin data no sirve.
             # Asumimos que full_data cubre todo el a√±o.
             pass
        
        # Warmup Data
        # Se toma relativo al primer candle del test set (para asegurar continuidad inmediata)
        # O relativo al primer candle del trai set?
        # Spec 4.1: "warmup_bars = 240 prior to the Test window start (or Train start?)"
        # Correcci√≥n en el log: "warmup data validada vs test start".
        # PERO, un sistema real entrena con indicadores ya calientes. 
        # Asi que el Train Set necesita warmup. 
        # Y el Test Set necesita warmup (que es el final del Train Set).
        # El objeto `warmup_data` en el dataclass Window suele ser "datos anteriores al Train" 
        # para que el primer candle de Train ya tenga RSi, etc.
        # SIN EMBARGO, el test `test_warmup_exactly_240_bars` verifica `win.warmup_data`.
        # Si la red neuronal entrena con features, necesita features desde la vela 0 de Train.
        # Por ende, warmup debe ser PRE-TRAIN.
        #
        # Re-reading Spec Note in `specification.md`:
        # "Warmup... prior to the Test window start (or Train start? ... Correction ... win.warmup_data checks vs Test Start)"
        # Espera, si el test checkea `win.warmup_data[-1].timestamp < win.test_data[0].timestamp`, 
        # eso es trivial si warmup es pre-train.
        #
        # Vamos a asumir Warmup es PRE-TRAIN, porque es lo necesario para entrenar.
        # Si fuera Pre-Test, ser√≠a train_data.
        #
        # Wait, el test: `assert win.warmup_data[-1].timestamp < win.test_data[0].timestamp`
        # Eso se cumple tanto si es Pre-Train como si es Pre-Test (pero overlapping Train).
        #
        # El standard de backtesting es:
        # Full Stream: [Warmup][Train][Test] -> No, Train incluye features.
        # Entonces: [Warmup para Train][Train] ... luego Test usa final de Train como warmup.
        # El Window object expl√≠citamente tiene `warmup_data`.
        # Si usamos `_get_warmup_candles` con `train_data[0].timestamp`, obtenemos 240 velas antes de Train.
        # Esto permite calcular indicadores para el primer punto de Train. OK.
        
        if train_data:
            train_start_ts = train_data[0].timestamp
            warmup_data = _get_warmup_candles(full_data, train_start_ts, config.warmup_bars)
        else:
            warmup_data = []

        # Crear Ventana
        win = Window(
            window_id=window_id,
            label=label,
            train_start_month=train_start_month,
            train_end_month=train_end_month,
            test_start_month=test_start_month,
            test_end_month=test_end_month,
            train_data=train_data,
            test_data=test_data,
            warmup_data=warmup_data
        )
        windows.append(win)
        
    return windows


def _slice_candles_by_month(
    candles: List[Candle],
    year: int,
    start_month: int,
    end_month: int
) -> List[Candle]:
    """
    Helper: extrae candles de meses espec√≠ficos.
    """
    if not candles:
        return []
        
    # Calcular timestamps l√≠mite
    # Start: d√≠a 1 del start_month a las 00:00:00
    # End: √∫ltimo d√≠a del end_month a las 23:59:59.999...
    
    start_dt = datetime(year, start_month, 1, 0, 0, 0, tzinfo=timezone.utc)
    start_ts = int(start_dt.timestamp() * 1000)
    
    last_day = monthrange(year, end_month)[1]
    end_dt = datetime(year, end_month, last_day, 23, 59, 59, 999999, tzinfo=timezone.utc)
    end_ts = int(end_dt.timestamp() * 1000)
    
    # Filtrado eficiente (asumiendo ordenados, pero O(N) es aceptable para configs t√≠picas)
    # Optimizaci√≥n: binary search si fuera muy grande, pero linear scan ok aqu√≠.
    
    result = []
    for c in candles:
        if c.timestamp >= start_ts and c.timestamp <= end_ts:
            result.append(c)
        elif c.timestamp > end_ts:
            # Si asumimos orden, podemos romper early
            break
            
    return result


def _get_warmup_candles(
    candles: List[Candle],
    reference_ts: int,
    warmup_bars: int
) -> List[Candle]:
    """
    Helper: extrae √∫ltimas N velas antes de reference_ts (exclusivo).
    """
    # Recolectar candidatos
    candidates = []
    for c in candles:
        if c.timestamp < reference_ts:
            candidates.append(c)
        else:
            # Asumiendo orden, alcanzamos el target
            break
            
    if len(candidates) < warmup_bars:
        # No hay suficiente data hist√≥rica para el warmup
        # Dependiendo de la severidad, retornamos lo que hay o vac√≠o o error.
        # Para consistencia estricta:
        # raise ValueError(f"Insufficient warmup data: required {warmup_bars}, got {len(candidates)}")
        # Pero mejor retornamos candidates y dejamos que el validador decida,
        # O seguimos el test que espera len == 240.
        return candidates # El test fallar√° si len != 240, que es correcto.
        
    return candidates[-warmup_bars:]


================================================================================
# FILE: src/portfolio/correlation.py
================================================================================

import pandas as pd
import numpy as np
from typing import List
from src.database.models import Trade

def build_equity_curve(trades: List[Trade], freq: str = '1D') -> pd.DataFrame:
    """
    Convert a list of trades into a time-series equity curve.
    
    Args:
        trades: List of Trade objects.
        freq: Resampling frequency (default '1D').
        
    Returns:
        DataFrame with index 'timestamp' and column 'equity'.
    """
    if not trades:
        return pd.DataFrame(columns=['equity'])
        
    data = []
    for t in trades:
        data.append({
            'timestamp': pd.to_datetime(t.timestamp),
            'pnl': float(t.profit_loss)
        })
    
    df = pd.DataFrame(data)
    # Resample to align grid (e.g., daily) and sum PnL for that period
    df = df.set_index('timestamp').resample(freq).sum().fillna(0)
    # Cumulative sum to get equity curve
    df['equity'] = df['pnl'].cumsum()
    return df

def calculate_correlation(trades_a: List[Trade], trades_b: List[Trade]) -> float:
    """
    Calculate Pearson correlation between the equity curves of two trade lists.
    
    Args:
        trades_a: First list of trades.
        trades_b: Second list of trades.
        
    Returns:
        float: Correlation coefficient between -1 and 1. Returns 0.0 if not enough data.
    """
    if not trades_a or not trades_b:
        return 0.0
        
    equity_a = build_equity_curve(trades_a)
    equity_b = build_equity_curve(trades_b)
    
    if equity_a.empty or equity_b.empty:
        return 0.0
    
    # Merge on timestamp to align the series
    # Use outer join to keep all dates, then forward fill equity
    # (if no trade happened on a day, equity remains same as previous day)
    merged = pd.merge(
        equity_a['equity'], 
        equity_b['equity'], 
        left_index=True, 
        right_index=True, 
        how='outer', 
        suffixes=('_a', '_b')
    )
    
    # Forward fill to propagate equity values
    merged = merged.fillna(method='ffill')
    
    # If starting points differ, fill initial NaNs with 0 (assuming starting equity 0)
    merged = merged.fillna(0)
    
    if len(merged) < 2:
        return 0.0
        
    # Calculate Pearson correlation of the equity levels
    return float(merged['equity_a'].corr(merged['equity_b']))


================================================================================
# FILE: src/simulation/backtest.py
================================================================================

from dataclasses import dataclass
from decimal import Decimal
from typing import List, Optional
from core.market import MarketState
from core.timeframe import Timeframe
from core.candle import Candle
from execution.broker import Broker, OrderRequest, OrderResult, Position, OrderType, OrderSide
from execution.risk import RiskManager, RiskConfig
from execution.executor import TradeExecutor
from strategy.engine import TJRStrategy

@dataclass(frozen=True)
class BacktestReport:
    initial_balance: Decimal
    final_balance: Decimal
    net_profit: Decimal
    total_trades: int
    winning_trades: int
    losing_trades: int
    win_rate: float
    gross_profit: Decimal
    gross_loss: Decimal
    max_drawdown: Decimal

class Backtester:
    @staticmethod
    def calculate_report(initial_balance: Decimal, equity_curve: List[Decimal], trade_history: List[dict]) -> BacktestReport:
        final_balance = equity_curve[-1]
        net_profit = final_balance - initial_balance
        
        total_trades = len(trade_history)
        winning_trades = 0
        losing_trades = 0
        gross_profit = Decimal("0")
        gross_loss = Decimal("0")
        
        for trade in trade_history:
            pnl = trade.get('pnl', Decimal("0"))
            if pnl > 0:
                winning_trades += 1
                gross_profit += pnl
            else:
                losing_trades += 1
                gross_loss += abs(pnl)
                
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0.0
        
        # Max Drawdown calculation
        max_dd = Decimal("0")
        peak = initial_balance
        for val in equity_curve:
            if val > peak:
                peak = val
            dd = peak - val
            if dd > max_dd:
                max_dd = dd
                
        return BacktestReport(
            initial_balance=initial_balance,
            final_balance=final_balance,
            net_profit=net_profit,
            total_trades=total_trades,
            winning_trades=winning_trades,
            losing_trades=losing_trades,
            win_rate=win_rate,
            gross_profit=gross_profit,
            gross_loss=gross_loss,
            max_drawdown=max_dd
        )

    def run(self, candles: List[Candle], broker: Broker, strategy: TJRStrategy, executor: TradeExecutor, timeframe: Timeframe) -> BacktestReport:
        initial_balance = broker.get_balance()
        market = MarketState.empty("BTCUSDT") # Symbol generic for now
        total_trades = 0
        
        # Note: We need a way to track equity over time in the broker or here
        # Let's assume the broker has an equity_curve list (like our InMemoryBroker)
        
        for candle in candles:
            market = market.update(candle)
            
            # 1. Update positions (Check SL/TP)
            # This requires the broker to have an update_positions method
            if hasattr(broker, 'update_positions'):
                broker.update_positions(candle.close)
            
            # 2. Analyze & Execute
            # Only trade if no open positions
            if not broker.get_positions():
                signal = strategy.analyze(market, timeframe)
                if signal:
                    result = executor.execute_trade(signal)
                    if result and result.status == "FILLED":
                        total_trades += 1
        
        # Re-access equity curve from broker if possible
        equity_curve = getattr(broker, 'equity_curve', [broker.get_balance()])
        trade_history = getattr(broker, 'trade_history', [])
        
        return self.calculate_report(initial_balance, equity_curve, trade_history)


================================================================================
# FILE: src/simulation/broker.py
================================================================================

from decimal import Decimal
from typing import List, Optional, Any
from src.execution.broker import Broker, OrderRequest, OrderResult, Position, OrderType, OrderSide

class InMemoryBroker(Broker):
    """
    Simulation Broker with realistic 0.1% Fees.
    """
    def __init__(self, balance: Decimal = Decimal("10000"), fee_rate: Decimal = Decimal("0.001")):
        self._balance = balance
        self._fee_rate = fee_rate
        self.orders: List[OrderRequest] = []
        self.positions: List[Position] = []
        self.equity_curve: List[Decimal] = [self._balance]
        self.total_fees_paid = Decimal("0")
        self.trade_history = []
        self.closed_positions = []
        
    def get_balance(self) -> Decimal:
        return self._balance

    def place_order(self, order: OrderRequest) -> OrderResult:
        if self._balance <= 0:
            return OrderResult(order_id="REJECTED", status="FAILED", filled_price=Decimal("0"), filled_quantity=Decimal("0"))
            
        # Simulate Entry Fee
        entry_val = order.quantity * order.price
        fee = entry_val * self._fee_rate
        
        if self._balance < fee:
            # Cannot even pay fee
            return OrderResult(order_id="REJECTED_FEE", status="FAILED", filled_price=Decimal("0"), filled_quantity=Decimal("0"))
            
        self._balance -= fee
        self.total_fees_paid += fee
        
        self.orders.append(order)
        
        pos = Position(
            symbol=order.symbol,
            side=order.side,
            quantity=order.quantity,
            entry_price=order.price,
            unrealized_pnl=Decimal("0"),
            stop_loss=order.stop_loss,
            take_profit=order.take_profit,
            metadata=order.metadata
        )
        self.positions.append(pos)
        
        # Track equity after fee
        self.equity_curve.append(self._balance)
        
        return OrderResult(
            order_id=f"id_{len(self.orders)}",
            status="FILLED",
            filled_price=order.price,
            filled_quantity=order.quantity
        )

    def get_positions(self) -> List[Position]:
        return self.positions
    
    def get_closed_positions(self) -> List[Any]:
        """Returns all positions that have been closed."""
        return self.closed_positions

    def cancel_order(self, order_id: str) -> bool:
        return True

    def update_positions(self, current_price: Decimal):
        """Simulate TP/SL logic with exit fees."""
        remaining = []
        for pos in self.positions:
            closed = False
            exit_price = Decimal("0")
            
            if pos.side == OrderSide.BUY:
                if pos.stop_loss and current_price <= pos.stop_loss:
                    exit_price = pos.stop_loss
                    closed = True
                elif pos.take_profit and current_price >= pos.take_profit:
                    exit_price = pos.take_profit
                    closed = True
            elif pos.side == OrderSide.SELL:
                if pos.stop_loss and current_price >= pos.stop_loss:
                    exit_price = pos.stop_loss
                    closed = True
                elif pos.take_profit and current_price <= pos.take_profit:
                    exit_price = pos.take_profit
                    closed = True
            
            if closed:
                # 1. Calculate Gross PnL
                if pos.side == OrderSide.BUY:
                    gross_pnl = (exit_price - pos.entry_price) * pos.quantity
                else:
                    gross_pnl = (pos.entry_price - exit_price) * pos.quantity
                
                # 2. Calculate Exit Fee
                exit_val = pos.quantity * exit_price
                exit_fee = exit_val * self._fee_rate
                
                # 3. Update Balance
                net_pnl = gross_pnl - exit_fee
                self._balance += net_pnl
                self.total_fees_paid += exit_fee
                self.equity_curve.append(self._balance)
                self.trade_history.append({"pnl": net_pnl, "fee": exit_fee})
                
                # For V3 Data Extraction
                pos_closed = pos.__dict__.copy()
                pos_closed.update({
                    'exit_price': exit_price,
                    'pnl': net_pnl,
                    'closed_at_price': current_price
                })
                # Using a simple namespace/dict wrapper to act like an object in Worker
                from types import SimpleNamespace
                self.closed_positions.append(SimpleNamespace(**pos_closed))
            else:
                remaining.append(pos)
        self.positions = remaining

    def get_current_drawdown_pct(self) -> Decimal:
        """Calculate current drawdown from equity peak."""
        if not self.equity_curve:
            return Decimal("0.0")
        
        # Determine peak equity so far
        # Note: self.equity_curve can grow large, optimization: track peak in _balance update
        # For now, max() is O(N) but safe.
        peak = max(self.equity_curve)
        current = self._balance
        
        if peak <= 0:
            return Decimal("0.0")
            
        return (peak - current) / peak

    def get_open_risk(self) -> Decimal:
        """Calculate total risk of all open positions."""
        total_risk = Decimal("0")
        for pos in self.positions:
            if pos.stop_loss and pos.entry_price:
                risk_per_share = abs(pos.entry_price - pos.stop_loss)
                total_risk += risk_per_share * pos.quantity
            else:
                # Fallback: Full value at risk if no SL
                total_risk += pos.entry_price * pos.quantity
        return total_risk


================================================================================
# FILE: src/simulation/generator.py
================================================================================

from decimal import Decimal
from typing import List, Generator
import random
from core.candle import Candle
from core.timeframe import Timeframe

class MarketGenerator:
    """
    Generates synthetic OHLC data with structural patterns.
    """
    def __init__(self, start_price: int = 50000, start_ts: int = 1000):
        self.current_price = Decimal(str(start_price))
        self.ts = start_ts
        self.timeframe = Timeframe.M5
        
    def _create_candle(self, open_p: Decimal, close_p: Decimal) -> Candle:
        high_p = max(open_p, close_p) + (Decimal("10") * Decimal(random.random()))
        low_p = min(open_p, close_p) - (Decimal("10") * Decimal(random.random()))
        # Ensure minimal volume
        vol = Decimal("100") + Decimal(random.randint(0, 500))
        
        c = Candle(
            timestamp=self.ts,
            open=open_p,
            close=close_p,
            high=high_p,
            low=low_p,
            volume=vol,
            timeframe=self.timeframe
        )
        self.ts += (5 * 60) # 5 min increments
        return c

    def generate_random_walk(self, n: int) -> List[Candle]:
        candles = []
        for _ in range(n):
            open_p = self.current_price
            # Volatility around 0.1%
            change = open_p * Decimal("0.001") * Decimal(2 * (random.random() - 0.5))
            close_p = open_p + change
            
            c = self._create_candle(open_p, close_p)
            candles.append(c)
            self.current_price = close_p
            
        return candles

    def generate_bullish_cycle(self) -> List[Candle]:
        """
        Generates a predefined TJR Bullish Cycle:
        1. Consolidation (Range)
        2. Sweep (Dump below range)
        3. Reversal (Strong Green)
        4. Expansion (Trend Up)
        """
        candles = []
        # 1. Consensus / Range
        for _ in range(5):
             candles.append(self._create_candle(self.current_price, self.current_price + Decimal(random.randint(-10, 10))))
             self.current_price = candles[-1].close

        # 2. Sweep Low (Dump 100 points)
        dump_open = self.current_price
        dump_close = dump_open - Decimal("150")
        candles.append(self._create_candle(dump_open, dump_close))
        self.current_price = dump_close
        
        # 3. Reversal (OB Creation)
        # Strong Buy back up
        pump_open = self.current_price
        pump_close = pump_open + Decimal("200") # Break structure?
        candles.append(self._create_candle(pump_open, pump_close))
        self.current_price = pump_close
        
        # 4. Mitigation / Retest
        # Slow drift down
        for _ in range(3):
            close = self.current_price - Decimal("20")
            candles.append(self._create_candle(self.current_price, close))
            self.current_price = close
            
        # 5. Expansion
        for _ in range(10):
            close = self.current_price + Decimal("50")
            candles.append(self._create_candle(self.current_price, close))
            self.current_price = close
            
        return candles

    def generate_cycle_stream(self, cycles: int = 1) -> Generator[Candle, None, None]:
        for _ in range(cycles):
             # Alternating cycles or just Bullish for MVP
             batch = self.generate_bullish_cycle()
             for c in batch:
                 yield c
                 


================================================================================
# FILE: src/strategy/engine.py
================================================================================

from typing import Optional, List
from decimal import Decimal
from src.core.market import MarketState
from src.core.timeframe import Timeframe
from src.execution.executor import TradeSignal
from src.execution.broker import OrderSide
from .ob import detect_ob, OrderBlock, OBType

class TJRStrategy:
    """
    Orchestrator for TJR Price Action Strategy.
    Scans for Setups (OBs formed by Sweep+BOS) and triggers on Retest.
    """
    def __init__(
        self, 
        fixed_stop_loss: Optional[Decimal] = None, 
        take_profit_multiplier: Decimal = Decimal("2.0"),
        stop_loss_atr_multiplier: Optional[Decimal] = None
    ):
        """
        :param fixed_stop_loss: If provided, uses this fixed USD distance for SL instead of structural low.
        :param take_profit_multiplier: R-Multiple for TP (default 2.0).
        :param stop_loss_atr_multiplier: If provided, uses ATR * this multiplier for SL distance.
        """
        self.fixed_stop_loss = fixed_stop_loss
        self.take_profit_multiplier = take_profit_multiplier
        self.stop_loss_atr_multiplier = stop_loss_atr_multiplier

    def _calculate_atr(self, series, period: int = 14) -> Optional[Decimal]:
        """Calculate Average True Range for SL sizing."""
        if len(series) < period + 1:
            return None
        
        tr_values = []
        for i in range(-period, 0):
            candle = series[i]
            prev_candle = series[i - 1]
            high_low = candle.high - candle.low
            high_prev_close = abs(candle.high - prev_candle.close)
            low_prev_close = abs(candle.low - prev_candle.close)
            tr = max(high_low, high_prev_close, low_prev_close)
            tr_values.append(tr)
        
        return sum(tr_values) / len(tr_values)

    def analyze(self, market: MarketState, timeframe: Timeframe = Timeframe.M5) -> Optional[TradeSignal]:
        series = market.get_series(timeframe)
        if len(series) < 5:
            return None
            
        current_candle = series.current
        current_idx = len(series) - 1
        
        # 1. Scan for recent Order Blocks (e.g., look back 50 candles)
        valid_setup = None
        
        # Optimize: Scan backwards for OBs
        for i in range(current_idx - 1, max(-1, current_idx - 50), -1):
            ob = detect_ob(series, i)
            if ob:
                if ob.type == OBType.BULLISH:
                    if current_candle.low <= ob.top and current_candle.close >= ob.bottom:
                         if ob.index < current_idx:
                             valid_setup = ob
                             break
                
                elif ob.type == OBType.BEARISH:
                    if current_candle.high >= ob.bottom and current_candle.close <= ob.top:
                        if ob.index < current_idx:
                             valid_setup = ob
                             break
                             
        if not valid_setup:
            return None
            
        # 3. Generate Signal
        # Calculate ATR if needed for ATR-based SL
        atr = None
        if self.stop_loss_atr_multiplier:
            atr = self._calculate_atr(series)
        
        if valid_setup.type == OBType.BULLISH:
            entry = current_candle.close
            
            # SL Logic: ATR > Fixed > Structural
            if self.stop_loss_atr_multiplier and atr:
                sl = entry - (atr * self.stop_loss_atr_multiplier)
            elif self.fixed_stop_loss and self.fixed_stop_loss > 0:
                sl = entry - self.fixed_stop_loss
            else:
                sl = valid_setup.bottom
                
            risk = entry - sl
            if risk <= 0: return None
            
            tp = entry + (risk * self.take_profit_multiplier)
            
            return TradeSignal(
                symbol="BTCUSDT",
                side=OrderSide.BUY,
                entry_price=entry,
                stop_loss=sl,
                take_profit=tp
            )
            
        elif valid_setup.type == OBType.BEARISH:
            entry = current_candle.close
            
            # SL Logic: ATR > Fixed > Structural
            if self.stop_loss_atr_multiplier and atr:
                sl = entry + (atr * self.stop_loss_atr_multiplier)
            elif self.fixed_stop_loss and self.fixed_stop_loss > 0:
                sl = entry + self.fixed_stop_loss
            else:
                sl = valid_setup.top
                
            risk = sl - entry
            if risk <= 0: return None
            
            tp = entry - (risk * self.take_profit_multiplier)
            
            return TradeSignal(
                symbol="BTCUSDT",
                side=OrderSide.SELL,
                entry_price=entry,
                stop_loss=sl,
                take_profit=tp
            )
            
        return None


================================================================================
# FILE: src/strategy/fractals.py
================================================================================

from src.core.series import MarketSeries

def is_valid_high(series: MarketSeries, index: int) -> bool:
    """
    TJR Definition for Valid Swing High:
    A GREEN candle followed by a RED candle.
    """
    if index < 0 or index + 1 >= len(series):
        return False
    
    current = series.get(index)
    next_candle = series.get(index + 1)
    
    # Green: Close > Open
    is_green = current.close > current.open
    # Red: Close < Open
    next_is_red = next_candle.close < next_candle.open
    
    return is_green and next_is_red

def is_valid_low(series: MarketSeries, index: int) -> bool:
    """
    TJR Definition for Valid Swing Low:
    A RED candle followed by a GREEN candle.
    """
    if index < 0 or index + 1 >= len(series):
        return False
        
    current = series.get(index)
    next_candle = series.get(index + 1)
    
    # Red: Close < Open
    is_red = current.close < current.open
    # Green: Close > Open
    next_is_green = next_candle.close > next_candle.open
    
    return is_red and next_is_green


================================================================================
# FILE: src/strategy/fvg.py
================================================================================

from dataclasses import dataclass
from enum import Enum
from typing import Optional
from decimal import Decimal
from core.series import MarketSeries

class FVGType(Enum):
    BULLISH = "BULLISH"
    BEARISH = "BEARISH"

@dataclass(frozen=True)
class FVG:
    type: FVGType
    top: Decimal
    bottom: Decimal
    index: int # Index of the 2nd candle (the middle of the gap)

def detect_fvg(series: MarketSeries, index: int) -> Optional[FVG]:
    """
    Detects if there is a Fair Value Gap triggered by the candle at `index`.
    A FVG is formed by the relationship between candles i-2, i-1, i.
    Uses TJR definition:
    Bullish: High(i-2) < Low(i). Gap is [High(i-2), Low(i)]
    Bearish: Low(i-2) > High(i). Gap is [High(i), Low(i-2)] - Wait, standard is [Low(i-2), High(i)]. 
    Let's check TJR doc:
    Msg: "Espacio donde las mechas de vela 1 y vela 3 NO se tocan"
    Length: 3 candles.
    We check at index `index` (which is candle 3).
    """
    if index < 2:
        return None
        
    c1 = series.get(index - 2)
    # c2 = series.get(index - 1) # Middle candle usually big
    c3 = series.get(index)
    
    # Bullish FVG
    # C1 High < C3 Low
    if c1.high < c3.low:
        return FVG(
            type=FVGType.BULLISH,
            top=c3.low,
            bottom=c1.high,
            index=index - 1
        )
        
    # Bearish FVG
    # C1 Low > C3 High
    if c1.low > c3.high:
        return FVG(
            type=FVGType.BEARISH,
            top=c1.low,
            bottom=c3.high,
            index=index - 1
        )
        
    return None


================================================================================
# FILE: src/strategy/ob.py
================================================================================

from dataclasses import dataclass
from enum import Enum
from typing import Optional
from decimal import Decimal
from src.core.series import MarketSeries
from .structure import detect_bos, StructureType
from .fractals import is_valid_low, is_valid_high

class OBType(Enum):
    BULLISH = "BULLISH"
    BEARISH = "BEARISH"

@dataclass(frozen=True)
class OrderBlock:
    type: OBType
    top: Decimal
    bottom: Decimal
    index: int # Index of the OB candle

def detect_ob(series: MarketSeries, index: int) -> Optional[OrderBlock]:
    """
    TJR Order Block Detection:
    1. Scan for BOS at 'index'.
    2. If BOS detected, trace back the move to find origin.
    3. Verify Origin scanned/swept a previous Swing Point.
    4. If swept, the last contrarian candle is the OB.
    """
    
    # 1. Check BOS
    bos = detect_bos(series, index)
    if not bos:
        return None
        
    current_bos_idx = index
    
    if bos.type == StructureType.BOS_BULLISH:
        # Looking for Bullish OB
        # Logic:
        # a. Find the lowest low between broken_index (Old High) and current_bos_idx?
        #    Actually we need the origin of the move that broke the high.
        #    Usually the lowest point in recent history before the break.
        
        # Scan back for the lowest point since the previous swing high? 
        # Or justscan back for the swing low that initiated this.
        
        # Simple algorithm for MVP:
        # Find the absolute lowest candle index between [index-20, index].
        # Let's say range is 50 to match BOS scanner.
        scan_stop = max(-1, index - 50)
        lowest_idx = -1
        lowest_val = Decimal("1000000000")
        
        for i in range(index, scan_stop, -1):
            c = series.get(i)
            if c.low < lowest_val:
                lowest_val = c.low
                lowest_idx = i
                
        if lowest_idx == -1:
            return None
            
        # The OB is typically the last DOWN candle (Red) at or near this bottom
        # specifically the one responsible for the sweep.
        # Let's look at the bottom candle itself.
        bott_candle = series.get(lowest_idx)
        
        # 3. Verify Sweep
        # Did this bottom sweep a previous low?
        # We need to find a VALID LOW before 'lowest_idx' that is HIGHER than 'lowest_val'.
        # Meaning: Previous Low > Current Low.
        
        swept = False
        for k in range(lowest_idx - 1, scan_stop, -1):
            if is_valid_low(series, k):
                 prev_low_candle = series.get(k)
                 if prev_low_candle.low > lowest_val: # We went lower than strict previous low
                     swept = True
                     break
        
        if not swept:
            return None
            
        # 4. Identify OB Candle
        # TJR: "Ultima vela ROJA antes del movimiento"
        # Search backwards from lowest_idx (inclusive) or forwards?
        # Usually it's the candle AT the bottom or just before the explosive move.
        # If the bottom candle is RED, it is the OB.
        # If the bottom candle is GREEN (hammer?), maybe the red before it.
        # Simplified: Use the candle at lowest_idx if Red, else scan back 1-2 candles for Red.
        
        ob_idx = lowest_idx
        c_ob = series.get(ob_idx)
        if c_ob.close > c_ob.open: # Green
             # Try prev
             if ob_idx > 0:
                 c_prev = series.get(ob_idx - 1)
                 if c_prev.close < c_prev.open:
                     ob_idx = ob_idx - 1
                     c_ob = c_prev
        
        # Construct OB
        return OrderBlock(
            type=OBType.BULLISH,
            top=c_ob.high,
            bottom=c_ob.low,
            index=ob_idx
        )

    elif bos.type == StructureType.BOS_BEARISH:
        # Looking for Bearish OB
        scan_stop = max(-1, index - 50)
        highest_idx = -1
        highest_val = Decimal("-1")
        
        for i in range(index, scan_stop, -1):
            c = series.get(i)
            if c.high > highest_val:
                highest_val = c.high
                highest_idx = i
                
        if highest_idx == -1:
            return None
            
        # Check Sweep
        swept = False
        for k in range(highest_idx - 1, scan_stop, -1):
             if is_valid_high(series, k):
                 prev_high_candle = series.get(k)
                 if prev_high_candle.high < highest_val: # We went higher than previous high
                     swept = True
                     break
        
        if not swept:
            return None
            
        # OB Candle (Green)
        ob_idx = highest_idx
        c_ob = series.get(ob_idx)
        if c_ob.close < c_ob.open: # Red
             if ob_idx > 0:
                 c_prev = series.get(ob_idx - 1)
                 if c_prev.close > c_prev.open:
                     ob_idx = ob_idx - 1
                     c_ob = c_prev
                     
        return OrderBlock(
            type=OBType.BEARISH,
            top=c_ob.high,
            bottom=c_ob.low,
            index=ob_idx
        )

    return None


================================================================================
# FILE: src/strategy/structure.py
================================================================================

from dataclasses import dataclass
from enum import Enum
from typing import Optional
from decimal import Decimal
from src.core.series import MarketSeries
from .fractals import is_valid_high, is_valid_low

class StructureType(Enum):
    BOS_BULLISH = "BOS_BULLISH"
    BOS_BEARISH = "BOS_BEARISH"

@dataclass(frozen=True)
class StructureEvent:
    type: StructureType
    price: Decimal # The price level that was broken (High/Low)
    broken_index: int # Index of the candle that formed the High/Low
    breakout_index: int # Index of the candle causing the break

def detect_bos(series: MarketSeries, index: int) -> Optional[StructureEvent]:
    """
    Scans for the most recent valid fractal and checks if the current candle
    breaks it with a BODY CLOSE.
    
    LIMITATION: For MVP, scans back up to 50 candles.
    """
    if index < 2:
        return None
        
    current = series.get(index)
    
    # Check for Bullish BOS (Breaking a recent High)
    # Scan backwards for the most recent valid high
    last_high_idx = -1
    for i in range(index - 1, max(-1, index - 50), -1):
        if is_valid_high(series, i):
            last_high_idx = i
            break
            
    if last_high_idx != -1:
        high_candle = series.get(last_high_idx)
        # Check if we CLOSED above the high
        if current.close > high_candle.high:
            # Verify that intermediate candles didn't already break it?
            # For simplicity/MVP, we just check if THIS candle broke it.
            # In a real scanner, you'd track state consistently.
            # Assuming this is called sequentially or we just want signal at this bar.
            return StructureEvent(
                type=StructureType.BOS_BULLISH,
                price=high_candle.high,
                broken_index=last_high_idx,
                breakout_index=index
            )

    # Check for Bearish BOS (Breaking a recent Low)
    last_low_idx = -1
    for i in range(index - 1, max(-1, index - 50), -1):
        if is_valid_low(series, i):
            last_low_idx = i
            break
            
    if last_low_idx != -1:
        low_candle = series.get(last_low_idx)
        # Check if we CLOSED below the low
        if current.close < low_candle.low:
            return StructureEvent(
                type=StructureType.BOS_BEARISH,
                price=low_candle.low,
                broken_index=last_low_idx,
                breakout_index=index
            )
            
    return None


================================================================================
# FILE: src/utils/data_loader.py
================================================================================

import csv
from decimal import Decimal
from typing import List
from src.core.candle import Candle
from src.core.timeframe import Timeframe

def parse_binance_line(line: str, timeframe: Timeframe) -> Candle:
    """
    Parses a single line of Binance CSV kline data.
    Format:
    0: Open time (ms)
    1: Open
    2: High
    3: Low
    4: Close
    5: Volume
    ...
    """
    parts = line.strip().split(",")
    if len(parts) < 6:
        raise ValueError(f"Invalid Binance CSV line (insufficient columns): {line}")
        
    try:
        timestamp = int(parts[0])
        open_p = Decimal(parts[1])
        high_p = Decimal(parts[2])
        low_p = Decimal(parts[3])
        close_p = Decimal(parts[4])
        volume = Decimal(parts[5])
        
        return Candle(
            timestamp=timestamp,
            open=open_p,
            high=high_p,
            low=low_p,
            close=close_p,
            volume=volume,
            timeframe=timeframe,
            complete=True
        )
    except (ValueError, TypeError) as e:
        raise ValueError(f"Error parsing Binance line: {e} | Line: {line}")

def load_binance_csv(file_path: str, timeframe: Timeframe) -> List[Candle]:
    """
    Loads a full Binance CSV file and returns a list of Candles.
    """
    candles = []
    with open(file_path, "r") as f:
        # Binance CSVs sometimes don't have headers.
        # We assume no headers for these monthly kline zips.
        for line in f:
            if not line.strip():
                continue
            candles.append(parse_binance_line(line, timeframe))
            
    return candles


================================================================================
# FILE: verify_results_integrity.py
================================================================================

import sys
from src.database import get_db_session
from src.database.models import Strategy, Pattern, Trade
from sqlalchemy import func

def verify_results():
    print("--- üîç RESULTS VERIFICATION AUDIT ---")
    
    with get_db_session() as db:
        # 1. Audit Approved Strategies
        strategies = db.query(Strategy).filter(Strategy.status == 'APPROVED').all()
        print(f"\n[1] APPROVED STRATEGIES: {len(strategies)}")
        if not strategies:
            print("‚ùå No approved strategies found (despite dashboard saying 8?)")
        
        for s in strategies:
            print(f"   ‚ñ∫ {s.name}")
            print(f"     PF: {s.profit_factor} | WR: {s.win_rate}%")
            print(f"     Params: {s.parameters}")
            print(f"     Filters: {s.filters}")
            
            # Sanity Check: Is PF realistic?
            if s.profit_factor is not None and float(s.profit_factor) > 10.0:
                print("     ‚ö†Ô∏è WARNING: Suspiciously high Profit Factor (Look-ahead bias?)")
            
        # 2. Audit Patterns
        patterns = db.query(Pattern).filter(Pattern.is_active == True).all()
        print(f"\n[2] DETECTED FAILURE PATTERNS: {len(patterns)}")
        for p in patterns:
            print(f"   ‚ñ∫ Type: {p.pattern_type}")
            print(f"     WR in context: {p.win_rate}% (Samples: {p.sample_size})")
            print(f"     Conditions: {p.conditions}")

        # 3. Audit Trade Integrity (Sample)
        trades_count = db.query(func.count(Trade.id)).scalar()
        print(f"\n[3] TOTAL TRADES RECORDED: {trades_count}")
        
        # Check for Look-ahead bias in a sample trade
        # Entry time should be >= Candle timestamp (which usually denotes Open time or Close time? need to verify)
        # Binance klines uses Open Time. So Close Time is Timestamp + Timeframe_ms - 1.
        # If we trade at Close of candle T, our trade timestamp should be T + 4h.
        
        sample_trade = db.query(Trade).order_by(Trade.timestamp.desc()).first()
        if sample_trade:
            print(f"\n   ‚ñ∫ LATEST TRADE AUDIT:")
            print(f"     Pair: {sample_trade.pair}")
            print(f"     Timestamp (Signal): {sample_trade.timestamp}")
            print(f"     Entry: {sample_trade.entry_price}")
            print(f"     Result: {sample_trade.result}")
            print(f"     Market State Keys: {list(sample_trade.market_state.keys()) if sample_trade.market_state else 'None'}")
            
    print("\n--- ‚úÖ AUDIT COMPLETE ---")

if __name__ == "__main__":
    verify_results()
